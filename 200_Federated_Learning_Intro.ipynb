{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.\n",
    "<br>\n",
    "1. Need to figure out what the weights are\n",
    "2. Need to look into asynchronous FL\n",
    "3. Create a global decoder from the last update... can I test it? I don't think so...\n",
    "4. I still think it could be beneficial to create a random external model that we could easily do FL on...\n",
    "5. Does scipy.optimize.minimize() run 1 iter or all necessary? Can it be replaced with SGD?  How are BFGS and SGD related?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py as h5\n",
    "#import aopy \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#from numpy.matlib import repmat\n",
    "#from matplotlib import pyplot as plt\n",
    "#from scipy.signal import detrend, firwin, freqz, lfilter\n",
    "#from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from scipy.optimize import minimize, least_squares\n",
    "import copy\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fda900",
   "metadata": {},
   "source": [
    "## Load Our Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71bffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_labels_df1 = pd.read_csv(\"Data\\emg_full_labels1.csv\")\n",
    "emg_labels_df2 = pd.read_csv(\"Data\\emg_full_labels2.csv\")\n",
    "emg_labels_df = pd.concat((emg_labels_df1, emg_labels_df2))\n",
    "\n",
    "try:\n",
    "    emg_labels_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    # Masterful code here\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "\n",
    "print(emg_labels_df.shape)\n",
    "display(emg_labels_df.head())\n",
    "\n",
    "labels_df = pd.DataFrame(emg_labels_df['Subject'].map(key_to_num))\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165598f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "emg_data_df1 = pd.read_csv(\"Data\\emg_full_data1.csv\")\n",
    "emg_data_df2 = pd.read_csv(\"Data\\emg_full_data2.csv\")\n",
    "emg_data_df = pd.concat((emg_data_df1, emg_data_df2))\n",
    "\n",
    "try:\n",
    "    emg_data_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "\n",
    "emg_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#envelope_df50 = pd.read_csv(\"Data\\envelope_df50.csv\")\n",
    "envelope_df100 = pd.read_csv(\"Data\\envelope_df100.csv\")\n",
    "#envelope_df150 = pd.read_csv(\"Data\\envelope_df150.csv\")\n",
    "#envelope_df200 = pd.read_csv(\"Data\\envelope_df200.csv\")\n",
    "#envelope_df250 = pd.read_csv(\"Data\\envelope_df250.csv\")\n",
    "#envelope_df300 = pd.read_csv(\"Data\\envelope_df300.csv\")\n",
    "\n",
    "#raw_envs = [envelope_df50, envelope_df100, envelope_df150, envelope_df200, envelope_df250, envelope_df300]\n",
    "#all_envs = [env.drop('Unnamed: 0', axis=1) for env in raw_envs]\n",
    "try:\n",
    "    envelope_df100.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "\n",
    "print(envelope_df100.shape)\n",
    "envelope_df100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e9ae3",
   "metadata": {},
   "source": [
    "## Run One Iteration On Above Data and Check Decoders Are the Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae05a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3db594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6edd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772b8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2e434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d28122",
   "metadata": {},
   "source": [
    "# WE ARE NOT RECREATING THE SIMULATIONS, JUST TAKE THE COST FUNCTIONS AND MINIMIZATION CODE, THE SIMS HERE ARE FOR A DIFFERENT TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e38bd",
   "metadata": {},
   "source": [
    "## Code From Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac42bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gradient of cost:\n",
    "# d(c_L2(D))/d(D) = 2*(DF + HV - V+)*F.T + 2*alphaD*D\n",
    "def gradient_cost_l2(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder # TODO: we now have a timeseries component - consult Sam\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64\n",
    "    Nt = learning_batch\n",
    "\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd, Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "\n",
    "    return ((2 * (D@F + H@Vminus - Vplus) @ F.T / (Nd*Nt) \n",
    "        + 2 * alphaD * D / (Nd*Ne)).flatten())\n",
    "\n",
    "  # set up gradient of cost:\n",
    "# d(c_L2(D))/d(D) = 2*(DF + HV - V+)*F.T + 2*alphaD*D\n",
    "def gradient_cost_l2_discrete(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder # TODO: we now have a timeseries component - consult Sam\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64\n",
    "    Nt = learning_batch\n",
    "\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd, Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "    v_unbounded = D@F\n",
    "    theta = np.arctan2(v_unbounded[1],v_unbounded[0])\n",
    "    v_actual = np.asarray([10*np.cos(theta),10*np.sin(theta)])\n",
    "    return ((2 * (v_actual + H@Vminus - Vplus) @ F.T / (Nd*Nt) \n",
    "        + 2 * alphaD * D / (Nd*Ne)).flatten())\n",
    "        \n",
    "# set up the cost function: \n",
    "# c_L2 = (||DF + HV - V+||_2)^2 + alphaD*(||D||_2)^2 + alphaF*(||F||_2)^2\n",
    "def cost_l2_discrete(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64 # default = 64\n",
    "    Nt = learning_batch\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "    v_unbounded = D@F\n",
    "    theta = np.arctan2(v_unbounded[1],v_unbounded[0])\n",
    "    v_actual = np.asarray([10*np.cos(theta),10*np.sin(theta)])\n",
    "    e = ( np.sum( (v_actual + H@Vminus - Vplus)**2 ) / (Nd*Nt) \n",
    "            + alphaD * np.sum( D**2 ) / (Nd*Ne)\n",
    "            + alphaF * np.sum( F**2 ) / (Ne*Nt) )\n",
    "    return e\n",
    "\n",
    "# set up the cost function: \n",
    "# c_L2 = (||DF + HV - V+||_2)^2 + alphaD*(||D||_2)^2 + alphaF*(||F||_2)^2\n",
    "def cost_l2(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64 # default = 64\n",
    "    Nt = learning_batch\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "\n",
    "    e = ( np.sum( (D @ F + H@Vminus - Vplus)**2 ) / (Nd*Nt) \n",
    "            + alphaD * np.sum( D**2 ) / (Nd*Ne)\n",
    "            + alphaF * np.sum( F**2 ) / (Ne*Nt) )\n",
    "    return e\n",
    "\n",
    "def estimate_decoder(F, H, V):\n",
    "    return (V[:,1:]-H@V[:,:-1])@np.linalg.pinv(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb0d52",
   "metadata": {},
   "source": [
    "# Modifying Simulations Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32913387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 2 new parameters\n",
    "def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2,display_info=False,num_iters=False):\n",
    "    p_classify = []\n",
    "    accuracy_temp = []\n",
    "    num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "\n",
    "    # RANDOMIZE DATASET\n",
    "    randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "    filtered_signals_randomized = filtered_signals[randomized_integers]\n",
    "    cued_target_position_randomized = cued_target_position[randomized_integers]\n",
    "    # batches the trials into each of the update batch\n",
    "    for ix in range(num_updates):\n",
    "        s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "        p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "        v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "        # CLASSIFY CURRENT DECODER ACCURACY\n",
    "        v_actual = D[-1]@s\n",
    "        for trial in range(learning_batch):\n",
    "            v_trial = v_actual[:,int(trial*60):int((trial+1)*60)] # velocities for each trials (2,60)\n",
    "            p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "            p_classify.append(classify(p_final))\n",
    "        \n",
    "        # UPDATE DECODER\n",
    "        u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "        q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "        # emg_windows against intended_targets (trial specific cued target)\n",
    "        F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "        V = copy.deepcopy(q)\n",
    "\n",
    "        # initial decoder estimate for gradient descent\n",
    "        D0 = np.random.rand(2,64)\n",
    "\n",
    "        # set alphas\n",
    "        H = np.zeros((2,2))\n",
    "        # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "        if num_iters is False:\n",
    "            out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "        else:\n",
    "            out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "        \n",
    "        # reshape to decoder parameters\n",
    "        W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "        # DO SMOOTHBATCH\n",
    "        W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "        D.append(W_new)\n",
    "\n",
    "        # COMPUTE CLASSIFICATION ACURACY \n",
    "        p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "        accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "    p_classify = np.asarray(p_classify)\n",
    "    return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_batch = 8\n",
    "alpha = .95 # higher alpha means more old decoder (slower update)\n",
    "alphaF=1e-1\n",
    "alphaD = 1e-1\n",
    "# create random decoder \n",
    "D_0 = np.random.rand(2,64)\n",
    "D = []\n",
    "D_constant = []\n",
    "D_bounded = []\n",
    "D_constant_bounded = []\n",
    "D.append(D_0)\n",
    "D_constant.append(D_0)\n",
    "D_bounded.append(D_0)\n",
    "D_constant_bounded.append(D_0)\n",
    "accuracy = []\n",
    "accuracy_constant = []\n",
    "accuracy_bounded = []\n",
    "accuracy_constant_bounded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de312ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code for running simulations...\n",
    "# for ix in range(10000):\n",
    "    #accuracy_constant_,D_constant,p_constrained_constant = simulation_constant_intent(D_constant,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant.extend(accuracy_constant_)\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n",
    "    #accuracy.extend(accuracy_)\n",
    "    #accuracy_bounded_,D_bounded,p_bounded = simulation_bounded_pos(D_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)  \n",
    "    #accuracy_bounded.extend(accuracy_bounded_)\n",
    "    #accuracy_constant_bounded_,D_constant_bounded,p_constant_bounded = simulation_constant_intent_bounded(D_constant_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant_bounded.extend(accuracy_constant_bounded_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094df67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified code for running simulations...\n",
    "# Why loop at all right now...\n",
    "#for ix in range(10):\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "455c7b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_signals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16136\\695885376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mp_classify\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0maccuracy_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mnum_updates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_signals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlearning_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# how many times can we update decoder based on learning batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# RANDOMIZE DATASET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_signals' is not defined"
     ]
    }
   ],
   "source": [
    "# Added 2 new parameters\n",
    "#def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2,display_info=False,num_iters=False):\n",
    "#D  # Already defined\n",
    "#learning_batch  # Already defined\n",
    "#alpha  # Already defined\n",
    "#alphaF=1e-2  #defined as something else earlier...\n",
    "#alphaD=1e-2  #defined as something else earlier...\n",
    "display_info=True\n",
    "num_iters=False\n",
    "\n",
    "######################################################################################\n",
    "# Where do filter_signals and cued_target_position come from lol\n",
    "\n",
    "\n",
    "p_classify = []\n",
    "accuracy_temp = []\n",
    "num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "\n",
    "# RANDOMIZE DATASET\n",
    "randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "filtered_signals_randomized = filtered_signals[randomized_integers]\n",
    "cued_target_position_randomized = cued_target_position[randomized_integers]\n",
    "# batches the trials into each of the update batch\n",
    "for ix in range(num_updates):\n",
    "    s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "    # CLASSIFY CURRENT DECODER ACCURACY\n",
    "    v_actual = D[-1]@s\n",
    "    for trial in range(learning_batch):\n",
    "        v_trial = v_actual[:,int(trial*60):int((trial+1)*60)] # velocities for each trials (2,60)\n",
    "        p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "        p_classify.append(classify(p_final))\n",
    "\n",
    "    # UPDATE DECODER\n",
    "    u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "    q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "    # emg_windows against intended_targets (trial specific cued target)\n",
    "    F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "    V = copy.deepcopy(q)\n",
    "\n",
    "    # initial decoder estimate for gradient descent\n",
    "    D0 = np.random.rand(2,64)\n",
    "\n",
    "    # set alphas\n",
    "    H = np.zeros((2,2))\n",
    "    # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "    if num_iters is False:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "\n",
    "    # reshape to decoder parameters\n",
    "    W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "    # DO SMOOTHBATCH\n",
    "    W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "    D.append(W_new)\n",
    "\n",
    "    # COMPUTE CLASSIFICATION ACURACY \n",
    "    p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "    accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "p_classify = np.asarray(p_classify)\n",
    "#return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a738a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
