{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.\n",
    "<br>\n",
    "1. Need to figure out what the weights are\n",
    "2. Need to look into asynchronous FL\n",
    "3. Create a global decoder from the last update... can I test it? I don't think so...\n",
    "4. I still think it could be beneficial to create a random external model that we could easily do FL on...\n",
    "5. Does scipy.optimize.minimize() run 1 iter or all necessary? Can it be replaced with SGD?  How are BFGS and SGD related?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e38bd",
   "metadata": {},
   "source": [
    "## Code From Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac42bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gradient of cost:\n",
    "# d(c_L2(D))/d(D) = 2*(DF + HV - V+)*F.T + 2*alphaD*D\n",
    "def gradient_cost_l2(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder # TODO: we now have a timeseries component - consult Sam\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64\n",
    "    Nt = learning_batch\n",
    "\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd, Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "\n",
    "    return ((2 * (D@F + H@Vminus - Vplus) @ F.T / (Nd*Nt) \n",
    "        + 2 * alphaD * D / (Nd*Ne)).flatten())\n",
    "\n",
    "  # set up gradient of cost:\n",
    "# d(c_L2(D))/d(D) = 2*(DF + HV - V+)*F.T + 2*alphaD*D\n",
    "def gradient_cost_l2_discrete(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder # TODO: we now have a timeseries component - consult Sam\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64\n",
    "    Nt = learning_batch\n",
    "\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd, Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "    v_unbounded = D@F\n",
    "    theta = np.arctan2(v_unbounded[1],v_unbounded[0])\n",
    "    v_actual = np.asarray([10*np.cos(theta),10*np.sin(theta)])\n",
    "    return ((2 * (v_actual + H@Vminus - Vplus) @ F.T / (Nd*Nt) \n",
    "        + 2 * alphaD * D / (Nd*Ne)).flatten())\n",
    "        \n",
    "# set up the cost function: \n",
    "# c_L2 = (||DF + HV - V+||_2)^2 + alphaD*(||D||_2)^2 + alphaF*(||F||_2)^2\n",
    "def cost_l2_discrete(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64 # default = 64\n",
    "    Nt = learning_batch\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "    v_unbounded = D@F\n",
    "    theta = np.arctan2(v_unbounded[1],v_unbounded[0])\n",
    "    v_actual = np.asarray([10*np.cos(theta),10*np.sin(theta)])\n",
    "    e = ( np.sum( (v_actual + H@Vminus - Vplus)**2 ) / (Nd*Nt) \n",
    "            + alphaD * np.sum( D**2 ) / (Nd*Ne)\n",
    "            + alphaF * np.sum( F**2 ) / (Ne*Nt) )\n",
    "    return e\n",
    "\n",
    "# set up the cost function: \n",
    "# c_L2 = (||DF + HV - V+||_2)^2 + alphaD*(||D||_2)^2 + alphaF*(||F||_2)^2\n",
    "def cost_l2(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64 # default = 64\n",
    "    Nt = learning_batch\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "\n",
    "    e = ( np.sum( (D @ F + H@Vminus - Vplus)**2 ) / (Nd*Nt) \n",
    "            + alphaD * np.sum( D**2 ) / (Nd*Ne)\n",
    "            + alphaF * np.sum( F**2 ) / (Ne*Nt) )\n",
    "    return e\n",
    "\n",
    "def estimate_decoder(F, H, V):\n",
    "    return (V[:,1:]-H@V[:,:-1])@np.linalg.pinv(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa8666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2):\n",
    "    p_classify = []\n",
    "    accuracy_temp = []\n",
    "    num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "\n",
    "    # RANDOMIZE DATASET\n",
    "    randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "    filtered_signals_randomized = filtered_signals[randomized_integers]\n",
    "    cued_target_position_randomized = cued_target_position[randomized_integers]\n",
    "    # batches the trials into each of the update batch\n",
    "    for ix in range(num_updates):\n",
    "        s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "        p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "        v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "        # CLASSIFY CURRENT DECODER ACCURACY\n",
    "        v_actual = D[-1]@s\n",
    "        for trial in range(learning_batch):\n",
    "            v_trial = v_actual[:,int(trial*60):int((trial+1)*60)] # velocities for each trials (2,60)\n",
    "            p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "            p_classify.append(classify(p_final))\n",
    "        \n",
    "        # UPDATE DECODER\n",
    "        u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "        q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "        # emg_windows against intended_targets (trial specific cued target)\n",
    "        F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "        V = copy.deepcopy(q)\n",
    "\n",
    "        # initial decoder estimate for gradient descent\n",
    "        D0 = np.random.rand(2,64)\n",
    "\n",
    "        # set alphas\n",
    "        H = np.zeros((2,2))\n",
    "        # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': False})\n",
    "\n",
    "        # reshape to decoder parameters\n",
    "        W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "        # DO SMOOTHBATCH\n",
    "        W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "        D.append(W_new)\n",
    "\n",
    "        # COMPUTE CLASSIFICATION ACURACY \n",
    "        p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "        accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "    p_classify = np.asarray(p_classify)\n",
    "    return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d7c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3eb0d52",
   "metadata": {},
   "source": [
    "# Modifying Simulations Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32913387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 2 new parameters\n",
    "def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2,display_info=False,num_iters=False):\n",
    "    p_classify = []\n",
    "    accuracy_temp = []\n",
    "    num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "\n",
    "    # RANDOMIZE DATASET\n",
    "    randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "    filtered_signals_randomized = filtered_signals[randomized_integers]\n",
    "    cued_target_position_randomized = cued_target_position[randomized_integers]\n",
    "    # batches the trials into each of the update batch\n",
    "    for ix in range(num_updates):\n",
    "        s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "        p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "        v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "        # CLASSIFY CURRENT DECODER ACCURACY\n",
    "        v_actual = D[-1]@s\n",
    "        for trial in range(learning_batch):\n",
    "            v_trial = v_actual[:,int(trial*60):int((trial+1)*60)] # velocities for each trials (2,60)\n",
    "            p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "            p_classify.append(classify(p_final))\n",
    "        \n",
    "        # UPDATE DECODER\n",
    "        u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "        q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "        # emg_windows against intended_targets (trial specific cued target)\n",
    "        F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "        V = copy.deepcopy(q)\n",
    "\n",
    "        # initial decoder estimate for gradient descent\n",
    "        D0 = np.random.rand(2,64)\n",
    "\n",
    "        # set alphas\n",
    "        H = np.zeros((2,2))\n",
    "        # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "        if num_iters is False:\n",
    "            out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "        else:\n",
    "            out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "        \n",
    "        # reshape to decoder parameters\n",
    "        W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "        # DO SMOOTHBATCH\n",
    "        W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "        D.append(W_new)\n",
    "\n",
    "        # COMPUTE CLASSIFICATION ACURACY \n",
    "        p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "        accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "    p_classify = np.asarray(p_classify)\n",
    "    return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06741b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea608be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This requires the aopy package...\n",
    "'''\n",
    "weiner_task_data, weiner_config = aopy.data.load_bmi3d_hdf_table('',filename, 'weiner')\n",
    "f = weiner_task_data\n",
    "print(f.dtype.names) # get the names of the fields\n",
    "\n",
    "timestamp = f['timestamp']\n",
    "decoded_cursor_position = f['decoded_cursor_position']\n",
    "decoded_velocity = f['decoded_velocity']\n",
    "weiner_filter_w = f['weiner_filter_w']\n",
    "weiner_filter_h = f['weiner_filter_h']\n",
    "alpha = f['alpha']\n",
    "raw_emg = f['raw_emg']\n",
    "filtered_emg = f['filtered_emg']\n",
    "reference = f['reference']\n",
    "cued_target_position = f['cued_target_position']\n",
    "\n",
    "# change target positions to angle\n",
    "cued_angles = np.arctan2(cued_target_position[:,1], cued_target_position[:,0])\n",
    "cued_angles_classes = (cued_angles*2/np.pi+2.001).astype(int) # need to add that 0.001 so all the numbers can round down during int conversion\n",
    "'''\n",
    "\n",
    "# Suppress output\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ba27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the data I have is already the filtered EMG data?\n",
    "\n",
    "#win_downsample = int(np.floor(raw_emg.shape[2]/2048*60))\n",
    "#filtered_signals = np.zeros((50, 64,  win_downsample))\n",
    "#window = np.floor(2048/60)\n",
    "#for trial in range(1,raw_emg.shape[0]):\n",
    "#    for ix in range(win_downsample - 1):\n",
    "#        # window the signal\n",
    "#        window_signal = raw_emg[trial,:,int(ix*window):int((ix+1)*window)]\n",
    "#        # print(window_signal[0,0])\n",
    "#        # filter the windowed signal\n",
    "#        filtered_signal = filter_signal(window_signal)\n",
    "#        filtered_signals[trial,:,ix] = np.squeeze(filtered_signal)\n",
    "#        # print(filtered_signal)\n",
    "\n",
    "#Therefore, import my pandas data\n",
    "t0 = time.time()\n",
    "\n",
    "emg_data_df1 = pd.read_csv(\"Data\\emg_full_data1.csv\")\n",
    "emg_labels_df1 = pd.read_csv(\"Data\\emg_full_labels1.csv\")\n",
    "emg_data_df2 = pd.read_csv(\"Data\\emg_full_data2.csv\")\n",
    "emg_labels_df2 = pd.read_csv(\"Data\\emg_full_labels2.csv\")\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "\n",
    "display(emg_data_df1.head())\n",
    "display(emg_labels_df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e983b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets = 4\n",
    "TARGET_LOCATION_RADIUS = 10\n",
    "thetas = 2*np.pi/num_targets*np.arange(0,num_targets) # convert number of targets to angles\n",
    "target_positions = TARGET_LOCATION_RADIUS*np.asarray([np.cos(thetas),np.sin(thetas)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_batch = 8\n",
    "alpha = .95 # higher alpha means more old decoder (slower update)\n",
    "alphaF=1e-1\n",
    "alphaD = 1e-1\n",
    "# create random decoder \n",
    "D_0 = np.random.rand(2,64)\n",
    "D = []\n",
    "D_constant = []\n",
    "D_bounded = []\n",
    "D_constant_bounded = []\n",
    "D.append(D_0)\n",
    "D_constant.append(D_0)\n",
    "D_bounded.append(D_0)\n",
    "D_constant_bounded.append(D_0)\n",
    "accuracy = []\n",
    "accuracy_constant = []\n",
    "accuracy_bounded = []\n",
    "accuracy_constant_bounded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de312ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code for running simulations...\n",
    "# for ix in range(10000):\n",
    "    #accuracy_constant_,D_constant,p_constrained_constant = simulation_constant_intent(D_constant,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant.extend(accuracy_constant_)\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n",
    "    #accuracy.extend(accuracy_)\n",
    "    #accuracy_bounded_,D_bounded,p_bounded = simulation_bounded_pos(D_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)  \n",
    "    #accuracy_bounded.extend(accuracy_bounded_)\n",
    "    #accuracy_constant_bounded_,D_constant_bounded,p_constant_bounded = simulation_constant_intent_bounded(D_constant_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant_bounded.extend(accuracy_constant_bounded_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094df67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified code for running simulations...\n",
    "# Why loop at all right now...\n",
    "#for ix in range(10):\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "455c7b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_signals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16136\\695885376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mp_classify\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0maccuracy_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mnum_updates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_signals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlearning_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# how many times can we update decoder based on learning batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# RANDOMIZE DATASET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_signals' is not defined"
     ]
    }
   ],
   "source": [
    "# Added 2 new parameters\n",
    "#def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2,display_info=False,num_iters=False):\n",
    "#D  # Already defined\n",
    "#learning_batch  # Already defined\n",
    "#alpha  # Already defined\n",
    "#alphaF=1e-2  #defined as something else earlier...\n",
    "#alphaD=1e-2  #defined as something else earlier...\n",
    "display_info=True\n",
    "num_iters=False\n",
    "\n",
    "######################################################################################\n",
    "# Where do filter_signals and cued_target_position come from lol\n",
    "\n",
    "\n",
    "p_classify = []\n",
    "accuracy_temp = []\n",
    "num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "\n",
    "# RANDOMIZE DATASET\n",
    "randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "filtered_signals_randomized = filtered_signals[randomized_integers]\n",
    "cued_target_position_randomized = cued_target_position[randomized_integers]\n",
    "# batches the trials into each of the update batch\n",
    "for ix in range(num_updates):\n",
    "    s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "    # CLASSIFY CURRENT DECODER ACCURACY\n",
    "    v_actual = D[-1]@s\n",
    "    for trial in range(learning_batch):\n",
    "        v_trial = v_actual[:,int(trial*60):int((trial+1)*60)] # velocities for each trials (2,60)\n",
    "        p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "        p_classify.append(classify(p_final))\n",
    "\n",
    "    # UPDATE DECODER\n",
    "    u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "    q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "    # emg_windows against intended_targets (trial specific cued target)\n",
    "    F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "    V = copy.deepcopy(q)\n",
    "\n",
    "    # initial decoder estimate for gradient descent\n",
    "    D0 = np.random.rand(2,64)\n",
    "\n",
    "    # set alphas\n",
    "    H = np.zeros((2,2))\n",
    "    # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "    if num_iters is False:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "\n",
    "    # reshape to decoder parameters\n",
    "    W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "    # DO SMOOTHBATCH\n",
    "    W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "    D.append(W_new)\n",
    "\n",
    "    # COMPUTE CLASSIFICATION ACURACY \n",
    "    p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "    accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "p_classify = np.asarray(p_classify)\n",
    "#return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a738a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
