{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.  Start by modifying the Simulations code, worry about (a)synchronicity later.\n",
    "<br>\n",
    "1. The dec matrix is the weights to pass back an forth (I think), although it comes out of SmoothBatch first\n",
    "1. We are assuming we can test on the second half (updates 10-19ish) since learning should be complete by then!\n",
    "1. Scipy.optimize.minimize() runs many iters to fully minimize its cost function.  You can change it to run as many iters as you'd like, although AFAIK you won't know how many it takes to converge.  But this is still a good set up for FL.\n",
    "1. Hmm minimize() is doing BFGS rn and not SGD... not sure if that matters really.  Could probably implement SGD on my own or find it.  BFGS is 2nd order but we don't have a lot of parameters, I don't think.  Plus we can (already have?) solved analytically for the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#from numpy.matlib import repmat\n",
    "#from matplotlib import pyplot as plt\n",
    "#from scipy.signal import detrend, firwin, freqz, lfilter\n",
    "#from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from scipy.optimize import minimize, least_squares\n",
    "import copy\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *\n",
    "from simulations import *\n",
    "import time\n",
    "# Do the below if you're in the pytch environment\n",
    "#import pickle5 as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fda900",
   "metadata": {},
   "source": [
    "## Load Our Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165598f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "emg_data_df1 = pd.read_csv(\"Data\\emg_full_data1.csv\")\n",
    "emg_data_df2 = pd.read_csv(\"Data\\emg_full_data2.csv\")\n",
    "emg_data_df = pd.concat((emg_data_df1, emg_data_df2))\n",
    "try:\n",
    "    emg_data_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "print(emg_data_df.shape)\n",
    "emg_data_df.head()\n",
    "'''\n",
    "# Just use the emg data directly from the pickle file for now\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845b1837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "#envelope_df50 = pd.read_csv(\"Data\\envelope_df50.csv\")\n",
    "envelope_df100 = pd.read_csv(\"Data\\envelope_df100.csv\")\n",
    "#envelope_df150 = pd.read_csv(\"Data\\envelope_df150.csv\")\n",
    "#envelope_df200 = pd.read_csv(\"Data\\envelope_df200.csv\")\n",
    "#envelope_df250 = pd.read_csv(\"Data\\envelope_df250.csv\")\n",
    "#envelope_df300 = pd.read_csv(\"Data\\envelope_df300.csv\")\n",
    "#raw_envs = [envelope_df50, envelope_df100, envelope_df150, envelope_df200, envelope_df250, envelope_df300]\n",
    "#all_envs = [env.drop('Unnamed: 0', axis=1) for env in raw_envs]\n",
    "try:\n",
    "    envelope_df100.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "print(envelope_df100.shape)\n",
    "envelope_df100.head()\n",
    "'''\n",
    "# Just use the emg data directly from the pickle file for now\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe511db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.349754095077515\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "with open('Data\\continuous_full_data_block1.pickle', 'rb') as handle:\n",
    "    #refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1, Hs_block1, alphas_block1, pDs_block1, times_block1, conditions_block1 = pickle.load(handle)\n",
    "    refs_block1, _, _, _, emgs_block1, Ws_block1, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "#with open('Data\\continuous_full_data_block2.pickle', 'rb') as handle:\n",
    "    #refs_block2, poss_block2, dec_vels_block2, int_vel_block2, emgs_block2, Ws_block2, Hs_block2, alphas_block2, pDs_block2, times_block2, conditions_block2 = pickle.load(handle)\n",
    "    #refs_block2, _, _, _, emgs_block2, Ws_block2, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c5ad93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 2, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 conditions, 20770 data points (only 19 unique sets!), xy, channels\n",
    "Ws_block1[keys[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9af127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614,\n",
       "       10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432,\n",
       "       20769])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7812773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder: (2, 64)\n",
      "\n",
      "Total difference between dec0 and dec1: 0.0\n",
      "E.g., as previously shown, the first two decs are the same\n",
      "\n",
      "Total difference between dec0 and dec2: 3.1981579823181594\n"
     ]
    }
   ],
   "source": [
    "dec_cond0_user1_update0 = Ws_block1[keys[0]][0,0,:,:]\n",
    "dec_cond0_user1_update1 = Ws_block1[keys[0]][0,update_ix[1],:,:]\n",
    "dec_cond0_user1_update2 = Ws_block1[keys[0]][0,update_ix[2],:,:]\n",
    "\n",
    "print(f\"Shape of decoder: {dec_cond0_user1_update0.shape}\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec1: {(dec_cond0_user1_update0 - dec_cond0_user1_update1).sum()}\")\n",
    "print(\"E.g., as previously shown, the first two decs are the same\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec2: {(dec_cond0_user1_update0 - dec_cond0_user1_update2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd624fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 20770, 64)\n",
      "(20770, 64)\n"
     ]
    }
   ],
   "source": [
    "#emg_cond0_user1_update0 = emg_data_df.iloc[:64,:].shape\n",
    "\n",
    "# (Condition, datapoints, channels)\n",
    "print(emgs_block1[keys[0]][:,:,:].shape)\n",
    "\n",
    "# Condition 0 of subject 1 (\"0\")\n",
    "print(emgs_block1[keys[0]][0,:,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e9ae3",
   "metadata": {},
   "source": [
    "## Run One Iteration On Above Data and Check Decoders Are the Same\n",
    "1. Modifying Simulations Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d034aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20770, 64)\n",
      "(20770, 2)\n"
     ]
    }
   ],
   "source": [
    "# Just 1 person\n",
    "filtered_signals = emgs_block1[keys[0]][0,:,:]\n",
    "# Read in the reference positions from the pickle file\n",
    "cued_target_position = refs_block1[keys[0]][0,:,:]\n",
    "\n",
    "print(filtered_signals.shape)\n",
    "print(cued_target_position.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc88fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously created random decoder, but we are trying to rerun\n",
    "#D_0 = np.random.rand(2,64)\n",
    "D_0 = dec_cond0_user1_update0\n",
    "\n",
    "#learning_batch = 8\n",
    "learning_batch = update_ix[1]  # I think this is supposed to be the number of datapoints per update?... why was it only 8 before then? Still don't know where they were getting 60 data points from\n",
    "#num_dps_per_update = update_ix[1]  #1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .95 # higher alpha means more old decoder (slower update)\n",
    "alphaF=1e-1\n",
    "alphaD = 1e-1\n",
    "\n",
    "D = []\n",
    "D_constant = []\n",
    "D_bounded = []\n",
    "D_constant_bounded = []\n",
    "D.append(D_0)\n",
    "D_constant.append(D_0)\n",
    "D_bounded.append(D_0)\n",
    "D_constant_bounded.append(D_0)\n",
    "accuracy = []\n",
    "accuracy_constant = []\n",
    "accuracy_bounded = []\n",
    "accuracy_constant_bounded = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808d4ac",
   "metadata": {},
   "source": [
    "Assuming I don't need to take all these different funcs into account... should probably ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de312ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code for running simulations...\n",
    "# for ix in range(10000):\n",
    "    #accuracy_constant_,D_constant,p_constrained_constant = simulation_constant_intent(D_constant,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant.extend(accuracy_constant_)\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n",
    "    #accuracy.extend(accuracy_)\n",
    "    #accuracy_bounded_,D_bounded,p_bounded = simulation_bounded_pos(D_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)  \n",
    "    #accuracy_bounded.extend(accuracy_bounded_)\n",
    "    #accuracy_constant_bounded_,D_constant_bounded,p_constant_bounded = simulation_constant_intent_bounded(D_constant_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant_bounded.extend(accuracy_constant_bounded_)\n",
    "    \n",
    "# Modified code for running simulations...\n",
    "# Why loop at all right now...\n",
    "#for ix in range(10):\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8494cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 2 new parameters\n",
    "#def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2,display_info=False,num_iters=False):\n",
    "#D  # Already defined\n",
    "#learning_batch  # Already defined\n",
    "#alpha  # Already defined\n",
    "#alphaF=1e-2  #defined as something else earlier...\n",
    "#alphaD=1e-2  #defined as something else earlier...\n",
    "display_info=True\n",
    "num_iters=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0102e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cond_number = 0 .... I don't need this because I already passed in the 0th condition to filtered signals... idk if I should keep it that way... I guess let's get it to run first.......\n",
    "# What was the original shape and meaning of filtered_signals? It must have been (datapoints, ?, ?), one of those is channels idk about the other one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18942579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a761d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389ee7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab43d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2f7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "130563dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_27568\\4248677151.py:54: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 21816.557630\n",
      "         Iterations: 148\n",
      "         Function evaluations: 199\n",
      "         Gradient evaluations: 199\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 21570.103379\n",
      "         Iterations: 149\n",
      "         Function evaluations: 193\n",
      "         Gradient evaluations: 193\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 26808.097431\n",
      "         Iterations: 148\n",
      "         Function evaluations: 189\n",
      "         Gradient evaluations: 189\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 29048.277741\n",
      "         Iterations: 149\n",
      "         Function evaluations: 203\n",
      "         Gradient evaluations: 203\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 24568.018656\n",
      "         Iterations: 150\n",
      "         Function evaluations: 188\n",
      "         Gradient evaluations: 188\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 20350.854089\n",
      "         Iterations: 148\n",
      "         Function evaluations: 199\n",
      "         Gradient evaluations: 199\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 28390.773113\n",
      "         Iterations: 149\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 192\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 22688.548979\n",
      "         Iterations: 148\n",
      "         Function evaluations: 195\n",
      "         Gradient evaluations: 195\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 32706.833833\n",
      "         Iterations: 149\n",
      "         Function evaluations: 194\n",
      "         Gradient evaluations: 194\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 29471.857177\n",
      "         Iterations: 148\n",
      "         Function evaluations: 193\n",
      "         Gradient evaluations: 193\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 18080.415193\n",
      "         Iterations: 149\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 192\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 28248.595046\n",
      "         Iterations: 149\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 192\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 28061.279711\n",
      "         Iterations: 148\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 192\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 19039.545733\n",
      "         Iterations: 149\n",
      "         Function evaluations: 190\n",
      "         Gradient evaluations: 190\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 24633.090553\n",
      "         Iterations: 150\n",
      "         Function evaluations: 193\n",
      "         Gradient evaluations: 193\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 17026.127462\n",
      "         Iterations: 148\n",
      "         Function evaluations: 195\n",
      "         Gradient evaluations: 195\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 17911.735791\n",
      "         Iterations: 149\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 192\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1009.929899\n",
      "         Iterations: 142\n",
      "         Function evaluations: 186\n",
      "         Gradient evaluations: 186\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n"
     ]
    }
   ],
   "source": [
    "# Added 2 new parameters\n",
    "#def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2,display_info=False,num_iters=False):\n",
    "p_classify = []\n",
    "accuracy_temp = []\n",
    "\n",
    "#num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "num_updates = 19  # This is 19 for us\n",
    "\n",
    "# Nah idk what this is doing so I'm gonna remove it\n",
    "# RANDOMIZE DATASET\n",
    "#randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "#filtered_signals_randomized = filtered_signals[randomized_integers]\n",
    "#cued_target_position_randomized = cued_target_position[randomized_integers]\n",
    "\n",
    "# batches the trials into each of the update batch\n",
    "for ix in range(num_updates):\n",
    "    #s = np.hstack([x for x in filtered_signals[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    #p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    #v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "    \n",
    "    # What operation do I need to do to get this to be 64x1200?\n",
    "    #s = np.hstack([x for x in np.transpose(filtered_signals[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:])])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    s = np.transpose(filtered_signals[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    p_intended = np.transpose(cued_target_position[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "    # CLASSIFY CURRENT DECODER ACCURACY\n",
    "    #v_actual = D[-1]@s\n",
    "    #for trial in range(learning_batch):\n",
    "    #    # The 60s are messing things up...\n",
    "    #    # Actually the 60s don't really affect things since they get summed out anyways, its just a window size more or less\n",
    "    #    v_trial = v_actual[:,int(trial*60):int((trial+1)*60)] # velocities for each trials (2,60)\n",
    "    #    p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "    #    # What is this supposed to be? Target_positions was global, p_final is (2,) so I guess just a subset of target_positions?\n",
    "    #    # They didn't do that I don't think tho lol\n",
    "    #    p_classify.append(classify(p_final, p_intended)) \n",
    "\n",
    "    # UPDATE DECODER\n",
    "    u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "    q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "    # emg_windows against intended_targets (trial specific cued target)\n",
    "    F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "    V = copy.deepcopy(q)\n",
    "\n",
    "    # Ask why we have this (D0) and also D_0...\n",
    "    # initial decoder estimate for gradient descent\n",
    "    D0 = np.random.rand(2,64)\n",
    "\n",
    "    # set alphas\n",
    "    H = np.zeros((2,2))\n",
    "    # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "    if num_iters is False:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "\n",
    "    # reshape to decoder parameters\n",
    "    W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "    # DO SMOOTHBATCH\n",
    "    W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "    D.append(W_new)\n",
    "\n",
    "    # COMPUTE CLASSIFICATION ACURACY \n",
    "    # No randomized... maybe it's just cued_target_positions?\n",
    "    #p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "    #p_target = cued_target_position\n",
    "    #accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "p_classify = np.asarray(p_classify)\n",
    "#return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd88e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adfd85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd414b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec91cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a89afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd5a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7cf2ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 60)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_trial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40a14082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca1f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc649b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "76800/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcb51968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(76800-72000)/1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba69d263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b325724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8753a3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1200)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_intended.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0eaa68f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21c3be23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1200)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_constrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8ac86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4001f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a90437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2475e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abdfbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ded716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a669e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5337a55",
   "metadata": {},
   "source": [
    "Don't remember what the stikt here is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "455c7b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 21935.005158\n",
      "         Iterations: 140\n",
      "         Function evaluations: 188\n",
      "         Gradient evaluations: 188\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 0 is different from 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2376\\341504634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Band-aid solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mp_intended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_intended\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m20770\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mv_intended\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp_constrained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_new_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp_intended\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# CLASSIFY CURRENT DECODER ACCURACY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Research\\personalization-privacy-risk\\simulations.py\u001b[0m in \u001b[0;36moutput_new_decoder\u001b[1;34m(s, D, p_intended)\u001b[0m\n\u001b[0;32m    187\u001b[0m     '''\n\u001b[0;32m    188\u001b[0m     \u001b[1;31m# take first trial, random decoder, and do target classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0ms\u001b[0m \u001b[1;31m# actual decoded velocity (2,60)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# integrate decoded velocities into positions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 0 is different from 64)"
     ]
    }
   ],
   "source": [
    "p_classify = []\n",
    "accuracy_temp = []\n",
    "\n",
    "#num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "num_updates = 19  # Let this equal the number of actual updates\n",
    "\n",
    "#############################################################################################\n",
    "# RANDOMIZE DATASET\n",
    "# Idk what's going on here\n",
    "randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "filtered_signals_randomized = filtered_signals  # filtered_signals[randomized_integers]\n",
    "cued_target_position_randomized = cued_target_position  # cued_target_position[randomized_integers]\n",
    "#############################################################################################\n",
    "\n",
    "# batches the trials into each of the update batch\n",
    "for ix in range(num_updates):\n",
    "    #s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    # stack s (64 x (60 timepoints x learning batch size))\n",
    "    #s = filtered_signals_randomized[:, int(ix*learning_batch+1):int((ix+1)*learning_batch+1)]\n",
    "    # Idk why they had all these +1s...\n",
    "    s = np.transpose(filtered_signals_randomized[:, int(ix*learning_batch):int((ix+1)*learning_batch+1)])\n",
    "    #p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    p_intended = np.hstack([np.tile(x[:,np.newaxis],learning_batch) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    # Band-aid solution\n",
    "    p_intended = p_intended[:, :20770]\n",
    "    v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "    # CLASSIFY CURRENT DECODER ACCURACY\n",
    "    v_actual = D[-1]@s\n",
    "    for trial in range(learning_batch):\n",
    "        v_trial = v_actual[:,int(trial*num_dps_per_update):int((trial+1)*num_dps_per_update)] # velocities for each trials (2,60)\n",
    "        p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "        p_classify.append(classify(p_final, cued_target_position))\n",
    "\n",
    "    # UPDATE DECODER\n",
    "    u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "    q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "    # emg_windows against intended_targets (trial specific cued target)\n",
    "    F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "    V = copy.deepcopy(q)\n",
    "\n",
    "    # initial decoder estimate for gradient descent\n",
    "    D0 = np.random.rand(2,64)\n",
    "    # Why is this different from D_0?\n",
    "\n",
    "    # set alphas\n",
    "    H = np.zeros((2,2))\n",
    "    # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "    if num_iters is False:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "\n",
    "    # reshape to decoder parameters\n",
    "    W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "    # DO SMOOTHBATCH\n",
    "    W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "    D.append(W_new)\n",
    "\n",
    "    # COMPUTE CLASSIFICATION ACURACY \n",
    "    # This is definitely wrong, not sure what it should be changed to...\n",
    "    p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "    accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "p_classify = np.asarray(p_classify)\n",
    "#return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efe08541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ea206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b966e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc952085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7912bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df5e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e8a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d07dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85855a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0486a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44b32675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c2d6dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b0c6e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 20770)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51dbe44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "223d02f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_signals_randomized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac9a47aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 20770)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(filtered_signals_randomized[:, int(ix*learning_batch):int((ix+1)*learning_batch+1)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2ff1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(ix*learning_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cebafc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((ix+1)*learning_batch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b95e7ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_signals_randomized[:, 1200:2401].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e840880e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f94b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd62c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff2efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42dd2144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd320c",
   "metadata": {},
   "source": [
    "This one also breaks, usually it runs through once (ix gets to 1) but then breaks the second time around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6203bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20770)\n",
      "(20770, 2)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1106.912511\n",
      "         Iterations: 148\n",
      "         Function evaluations: 199\n",
      "         Gradient evaluations: 199\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 4708.949398\n",
      "         Iterations: 149\n",
      "         Function evaluations: 204\n",
      "         Gradient evaluations: 204\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1213.727482\n",
      "         Iterations: 149\n",
      "         Function evaluations: 191\n",
      "         Gradient evaluations: 191\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1619.567090\n",
      "         Iterations: 150\n",
      "         Function evaluations: 201\n",
      "         Gradient evaluations: 201\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 752.830458\n",
      "         Iterations: 151\n",
      "         Function evaluations: 195\n",
      "         Gradient evaluations: 195\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1088.797473\n",
      "         Iterations: 149\n",
      "         Function evaluations: 207\n",
      "         Gradient evaluations: 207\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 833.521810\n",
      "         Iterations: 149\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 198\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1420.458632\n",
      "         Iterations: 148\n",
      "         Function evaluations: 200\n",
      "         Gradient evaluations: 200\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1173.099486\n",
      "         Iterations: 149\n",
      "         Function evaluations: 203\n",
      "         Gradient evaluations: 203\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2305.557925\n",
      "         Iterations: 149\n",
      "         Function evaluations: 199\n",
      "         Gradient evaluations: 199\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 903.597358\n",
      "         Iterations: 153\n",
      "         Function evaluations: 199\n",
      "         Gradient evaluations: 199\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1843.155216\n",
      "         Iterations: 150\n",
      "         Function evaluations: 200\n",
      "         Gradient evaluations: 200\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 815.291879\n",
      "         Iterations: 149\n",
      "         Function evaluations: 195\n",
      "         Gradient evaluations: 195\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 631.332605\n",
      "         Iterations: 150\n",
      "         Function evaluations: 191\n",
      "         Gradient evaluations: 191\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 898.033260\n",
      "         Iterations: 150\n",
      "         Function evaluations: 190\n",
      "         Gradient evaluations: 190\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1190.345497\n",
      "         Iterations: 149\n",
      "         Function evaluations: 201\n",
      "         Gradient evaluations: 201\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 839.098145\n",
      "         Iterations: 149\n",
      "         Function evaluations: 200\n",
      "         Gradient evaluations: 200\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 502.564379\n",
      "         Iterations: 137\n",
      "         Function evaluations: 180\n",
      "         Gradient evaluations: 180\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2376\\3626031371.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_signals_randomized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mp_intended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcued_target_position_randomized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_batch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_batch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# stack p_intended (2 x 60 timepoints x learning batch size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Band-aid solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mp_intended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_intended\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\pytch\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "p_classify = []\n",
    "accuracy_temp = []\n",
    "\n",
    "#num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "num_updates = 19  # Let this equal the number of actual updates\n",
    "\n",
    "#############################################################################################\n",
    "# RANDOMIZE DATASET\n",
    "# Idk what's going on here\n",
    "randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "filtered_signals_randomized = np.transpose(filtered_signals)  # filtered_signals[randomized_integers]\n",
    "print(filtered_signals_randomized.shape)\n",
    "cued_target_position_randomized = cued_target_position  # cued_target_position[randomized_integers]\n",
    "print(cued_target_position_randomized.shape)\n",
    "#############################################################################################\n",
    "\n",
    "# batches the trials into each of the update batch\n",
    "for ix in range(num_updates):\n",
    "    #s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    # stack s (64 x (60 timepoints x learning batch size))\n",
    "    #s = filtered_signals_randomized[:, int(ix*learning_batch+1):int((ix+1)*learning_batch+1)]\n",
    "    # Idk why they had all these +1s...\n",
    "    s = filtered_signals_randomized[:, int(ix*learning_batch):int((ix+1)*learning_batch)]\n",
    "    #p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    p_intended = np.hstack([np.tile(x[:,np.newaxis],learning_batch) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    # Band-aid solution\n",
    "    p_intended = p_intended[:, :s.shape[1]]\n",
    "    v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "    # CLASSIFY CURRENT DECODER ACCURACY\n",
    "    v_actual = D[-1]@s\n",
    "    for trial in range(learning_batch):\n",
    "        v_trial = v_actual[:,int(trial*num_dps_per_update):int((trial+1)*num_dps_per_update)] # velocities for each trials (2,60)\n",
    "        p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "        p_classify.append(classify(p_final, cued_target_position))\n",
    "\n",
    "    # UPDATE DECODER\n",
    "    u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "    q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "    # emg_windows against intended_targets (trial specific cued target)\n",
    "    F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "    V = copy.deepcopy(q)\n",
    "\n",
    "    # initial decoder estimate for gradient descent\n",
    "    D0 = np.random.rand(2,64)\n",
    "    # Why is this different from D_0?\n",
    "\n",
    "    # set alphas\n",
    "    H = np.zeros((2,2))\n",
    "    # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "    if num_iters is False:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "\n",
    "    # reshape to decoder parameters\n",
    "    W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "    # DO SMOOTHBATCH\n",
    "    W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "    D.append(W_new)\n",
    "\n",
    "    # COMPUTE CLASSIFICATION ACURACY \n",
    "    # This is definitely wrong, not sure what it should be changed to...\n",
    "    p_target = cued_target_position[-learning_batch:, :]  # (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "    accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "p_classify = np.asarray(p_classify)\n",
    "#return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fe71262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1200)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d9121ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 370)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_intended.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8c25d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d93d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd6104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145aa78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46b3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24de58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd857e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e1af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2cf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
