{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.\n",
    "<br>\n",
    "1. Need to figure out what the weights are\n",
    "2. Need to look into asynchronous FL\n",
    "3. Create a global decoder from the last update... can I test it? I don't think so...\n",
    "4. I still think it could be beneficial to create a random external model that we could easily do FL on...\n",
    "5. Does scipy.optimize.minimize() run 1 iter or all necessary? Can it be replaced with SGD?  How are BFGS and SGD related?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py as h5\n",
    "#import aopy \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#from numpy.matlib import repmat\n",
    "#from matplotlib import pyplot as plt\n",
    "#from scipy.signal import detrend, firwin, freqz, lfilter\n",
    "#from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from scipy.optimize import minimize, least_squares\n",
    "import copy\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *\n",
    "from simulations import *\n",
    "import time\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fda900",
   "metadata": {},
   "source": [
    "## Load Our Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71bffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7168, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METACPHS_S106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METACPHS_S106</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METACPHS_S106</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METACPHS_S106</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METACPHS_S106</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Subject  Condition  Channel\n",
       "0  METACPHS_S106          0        0\n",
       "1  METACPHS_S106          0        1\n",
       "2  METACPHS_S106          0        2\n",
       "3  METACPHS_S106          0        3\n",
       "4  METACPHS_S106          0        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emg_labels_df1 = pd.read_csv(\"Data\\emg_full_labels1.csv\")\n",
    "#emg_labels_df2 = pd.read_csv(\"Data\\emg_full_labels2.csv\")\n",
    "emg_labels_df = emg_labels_df1\n",
    "#emg_labels_df = pd.concat((emg_labels_df1, emg_labels_df2))\n",
    "\n",
    "try:\n",
    "    emg_labels_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    # Masterful code here\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "    \n",
    "print(emg_labels_df[\"Subject\"].unique())\n",
    "print()\n",
    "print(emg_labels_df.shape[0]/14/64/8)\n",
    "# 14 participants / 64 channels / 8 conditions\n",
    "# Why do I only have 2 updates...\n",
    "\n",
    "print(emg_labels_df.shape)\n",
    "display(emg_labels_df.head())\n",
    "\n",
    "labels_df = pd.DataFrame(emg_labels_df['Subject'].map(key_to_num))\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165598f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "emg_data_df1 = pd.read_csv(\"Data\\emg_full_data1.csv\")\n",
    "emg_data_df2 = pd.read_csv(\"Data\\emg_full_data2.csv\")\n",
    "emg_data_df = pd.concat((emg_data_df1, emg_data_df2))\n",
    "try:\n",
    "    emg_data_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "print(emg_data_df.shape)\n",
    "emg_data_df.head()\n",
    "'''\n",
    "# Just use the emg data directly from the pickle file for now\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845b1837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "#envelope_df50 = pd.read_csv(\"Data\\envelope_df50.csv\")\n",
    "envelope_df100 = pd.read_csv(\"Data\\envelope_df100.csv\")\n",
    "#envelope_df150 = pd.read_csv(\"Data\\envelope_df150.csv\")\n",
    "#envelope_df200 = pd.read_csv(\"Data\\envelope_df200.csv\")\n",
    "#envelope_df250 = pd.read_csv(\"Data\\envelope_df250.csv\")\n",
    "#envelope_df300 = pd.read_csv(\"Data\\envelope_df300.csv\")\n",
    "#raw_envs = [envelope_df50, envelope_df100, envelope_df150, envelope_df200, envelope_df250, envelope_df300]\n",
    "#all_envs = [env.drop('Unnamed: 0', axis=1) for env in raw_envs]\n",
    "try:\n",
    "    envelope_df100.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "print(envelope_df100.shape)\n",
    "envelope_df100.head()\n",
    "'''\n",
    "# Just use the emg data directly from the pickle file for now\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe511db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.675995349884033\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "with open('Data\\continuous_full_data_block1.pickle', 'rb') as handle:\n",
    "    #refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1, Hs_block1, alphas_block1, pDs_block1, times_block1, conditions_block1 = pickle.load(handle)\n",
    "    refs_block1, _, _, _, emgs_block1, Ws_block1, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "#with open('Data\\continuous_full_data_block2.pickle', 'rb') as handle:\n",
    "    #refs_block2, poss_block2, dec_vels_block2, int_vel_block2, emgs_block2, Ws_block2, Hs_block2, alphas_block2, pDs_block2, times_block2, conditions_block2 = pickle.load(handle)\n",
    "    #refs_block2, _, _, _, emgs_block2, Ws_block2, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c5ad93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 2, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 conditions, 20770 data points (only 19 unique sets!), xy, channels\n",
    "Ws_block1[keys[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9af127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614,\n",
       "       10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432,\n",
       "       20769])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7812773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder: (2, 64)\n",
      "\n",
      "Total difference between dec0 and dec1: 0.0\n",
      "E.g., as previously shown, the first two decs are the same\n",
      "\n",
      "Total difference between dec0 and dec2: 3.1981579823181594\n"
     ]
    }
   ],
   "source": [
    "dec_cond0_user1_update0 = Ws_block1[keys[0]][0,0,:,:]\n",
    "dec_cond0_user1_update1 = Ws_block1[keys[0]][0,update_ix[1],:,:]\n",
    "dec_cond0_user1_update2 = Ws_block1[keys[0]][0,update_ix[2],:,:]\n",
    "\n",
    "print(f\"Shape of decoder: {dec_cond0_user1_update0.shape}\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec1: {(dec_cond0_user1_update0 - dec_cond0_user1_update1).sum()}\")\n",
    "print(\"E.g., as previously shown, the first two decs are the same\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec2: {(dec_cond0_user1_update0 - dec_cond0_user1_update2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd624fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emg_cond0_user1_update0 = emg_data_df.iloc[:64,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e9ae3",
   "metadata": {},
   "source": [
    "## Run One Iteration On Above Data and Check Decoders Are the Same\n",
    "1. Modifying Simulations Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d034aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just 1 person\n",
    "filtered_signals = emgs_block1[keys[0]][0,:,:]\n",
    "# Read in the reference positions from the pickle file\n",
    "cued_target_position = refs_block1[keys[0]][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2cc88fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously created random decoder, but we are trying to rerun\n",
    "#D_0 = np.random.rand(2,64)\n",
    "D_0 = dec_cond0_user1_update0\n",
    "\n",
    "#learning_batch = 8\n",
    "learning_batch = update_ix[1]  # I think this is supposed to be the number of datapoints per update?\n",
    "num_dps_per_update = update_ix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "203958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .95 # higher alpha means more old decoder (slower update)\n",
    "alphaF=1e-1\n",
    "alphaD = 1e-1\n",
    "\n",
    "D = []\n",
    "D_constant = []\n",
    "D_bounded = []\n",
    "D_constant_bounded = []\n",
    "D.append(D_0)\n",
    "D_constant.append(D_0)\n",
    "D_bounded.append(D_0)\n",
    "D_constant_bounded.append(D_0)\n",
    "accuracy = []\n",
    "accuracy_constant = []\n",
    "accuracy_bounded = []\n",
    "accuracy_constant_bounded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de312ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code for running simulations...\n",
    "# for ix in range(10000):\n",
    "    #accuracy_constant_,D_constant,p_constrained_constant = simulation_constant_intent(D_constant,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant.extend(accuracy_constant_)\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n",
    "    #accuracy.extend(accuracy_)\n",
    "    #accuracy_bounded_,D_bounded,p_bounded = simulation_bounded_pos(D_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)  \n",
    "    #accuracy_bounded.extend(accuracy_bounded_)\n",
    "    #accuracy_constant_bounded_,D_constant_bounded,p_constant_bounded = simulation_constant_intent_bounded(D_constant_bounded,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)\n",
    "    #accuracy_constant_bounded.extend(accuracy_constant_bounded_)\n",
    "    \n",
    "# Modified code for running simulations...\n",
    "# Why loop at all right now...\n",
    "#for ix in range(10):\n",
    "    #accuracy_,D,p_constrained = simulation(D,learning_batch,alpha,alphaF=alphaF,alphaD=alphaD)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8494cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 2 new parameters\n",
    "#def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2,display_info=False,num_iters=False):\n",
    "#D  # Already defined\n",
    "#learning_batch  # Already defined\n",
    "#alpha  # Already defined\n",
    "#alphaF=1e-2  #defined as something else earlier...\n",
    "#alphaD=1e-2  #defined as something else earlier...\n",
    "display_info=True\n",
    "num_iters=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "455c7b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15388\\2984258992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mv_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_dps_per_update\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_dps_per_update\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# velocities for each trials (2,60)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mp_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# final position after integration (2,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mp_classify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# UPDATE DECODER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classify' is not defined"
     ]
    }
   ],
   "source": [
    "p_classify = []\n",
    "accuracy_temp = []\n",
    "\n",
    "#num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "num_updates = 19  # Let this equal the number of actual updates\n",
    "\n",
    "#############################################################################################\n",
    "# RANDOMIZE DATASET\n",
    "# Idk what's going on here\n",
    "randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "filtered_signals_randomized = filtered_signals  # filtered_signals[randomized_integers]\n",
    "cued_target_position_randomized = cued_target_position  # cued_target_position[randomized_integers]\n",
    "#############################################################################################\n",
    "\n",
    "# batches the trials into each of the update batch\n",
    "for ix in range(num_updates):\n",
    "    #s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "    # stack s (64 x (60 timepoints x learning batch size))\n",
    "    #s = filtered_signals_randomized[:, int(ix*learning_batch+1):int((ix+1)*learning_batch+1)]\n",
    "    # Idk why they had all these +1s...\n",
    "    s = np.transpose(filtered_signals_randomized[:, int(ix*learning_batch):int((ix+1)*learning_batch+1)])\n",
    "    #p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    p_intended = np.hstack([np.tile(x[:,np.newaxis],learning_batch) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "    # Band-aid solution\n",
    "    p_intended = p_intended[:, :20770]\n",
    "    v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "    # CLASSIFY CURRENT DECODER ACCURACY\n",
    "    v_actual = D[-1]@s\n",
    "    for trial in range(learning_batch):\n",
    "        v_trial = v_actual[:,int(trial*num_dps_per_update):int((trial+1)*num_dps_per_update)] # velocities for each trials (2,60)\n",
    "        p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "        p_classify.append(classify(p_final))\n",
    "\n",
    "    # UPDATE DECODER\n",
    "    u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "    q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "    # emg_windows against intended_targets (trial specific cued target)\n",
    "    F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "    V = copy.deepcopy(q)\n",
    "\n",
    "    # initial decoder estimate for gradient descent\n",
    "    D0 = np.random.rand(2,64)\n",
    "    # Why is this different from D_0?\n",
    "\n",
    "    # set alphas\n",
    "    H = np.zeros((2,2))\n",
    "    # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "    if num_iters is False:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': display_info, 'maxiter':num_iters})\n",
    "\n",
    "    # reshape to decoder parameters\n",
    "    W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "    # DO SMOOTHBATCH\n",
    "    W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "    D.append(W_new)\n",
    "\n",
    "    # COMPUTE CLASSIFICATION ACURACY \n",
    "    p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "    accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "p_classify = np.asarray(p_classify)\n",
    "#return accuracy_temp,D,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d6dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbe44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a47aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ff1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
