{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.\n",
    "<br>\n",
    "1. Need to figure out what the weights are\n",
    "2. Need to look into asynchronous FL\n",
    "3. Create a global decoder from the last update... can I test it? I don't think so...\n",
    "4. I still think it could be beneficial to create a random external model that we could easily do FL on...\n",
    "5. Does scipy.optimize.minimize() run 1 iter or all necessary? Can it be replaced with SGD?  How are BFGS and SGD related?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e38bd",
   "metadata": {},
   "source": [
    "## Code From Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac42bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gradient of cost:\n",
    "# d(c_L2(D))/d(D) = 2*(DF + HV - V+)*F.T + 2*alphaD*D\n",
    "def gradient_cost_l2(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder # TODO: we now have a timeseries component - consult Sam\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64\n",
    "    Nt = learning_batch\n",
    "\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd, Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "\n",
    "    return ((2 * (D@F + H@Vminus - Vplus) @ F.T / (Nd*Nt) \n",
    "        + 2 * alphaD * D / (Nd*Ne)).flatten())\n",
    "\n",
    "  # set up gradient of cost:\n",
    "# d(c_L2(D))/d(D) = 2*(DF + HV - V+)*F.T + 2*alphaD*D\n",
    "def gradient_cost_l2_discrete(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder # TODO: we now have a timeseries component - consult Sam\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64\n",
    "    Nt = learning_batch\n",
    "\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd, Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "    v_unbounded = D@F\n",
    "    theta = np.arctan2(v_unbounded[1],v_unbounded[0])\n",
    "    v_actual = np.asarray([10*np.cos(theta),10*np.sin(theta)])\n",
    "    return ((2 * (v_actual + H@Vminus - Vplus) @ F.T / (Nd*Nt) \n",
    "        + 2 * alphaD * D / (Nd*Ne)).flatten())\n",
    "        \n",
    "# set up the cost function: \n",
    "# c_L2 = (||DF + HV - V+||_2)^2 + alphaD*(||D||_2)^2 + alphaF*(||F||_2)^2\n",
    "def cost_l2_discrete(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64 # default = 64\n",
    "    Nt = learning_batch\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "    v_unbounded = D@F\n",
    "    theta = np.arctan2(v_unbounded[1],v_unbounded[0])\n",
    "    v_actual = np.asarray([10*np.cos(theta),10*np.sin(theta)])\n",
    "    e = ( np.sum( (v_actual + H@Vminus - Vplus)**2 ) / (Nd*Nt) \n",
    "            + alphaD * np.sum( D**2 ) / (Nd*Ne)\n",
    "            + alphaF * np.sum( F**2 ) / (Ne*Nt) )\n",
    "    return e\n",
    "\n",
    "# set up the cost function: \n",
    "# c_L2 = (||DF + HV - V+||_2)^2 + alphaD*(||D||_2)^2 + alphaF*(||F||_2)^2\n",
    "def cost_l2(F, D, H, V, alphaF=1e-2, alphaD=1e-2):\n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder\n",
    "    H: 2 x 2 state transition matrix\n",
    "    ''' \n",
    "    Nd = 2\n",
    "    Ne = 64 # default = 64\n",
    "    Nt = learning_batch\n",
    "    # TODO: add depth (time) to D\n",
    "    D = np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    Vminus = V[:,:-1]\n",
    "\n",
    "    e = ( np.sum( (D @ F + H@Vminus - Vplus)**2 ) / (Nd*Nt) \n",
    "            + alphaD * np.sum( D**2 ) / (Nd*Ne)\n",
    "            + alphaF * np.sum( F**2 ) / (Ne*Nt) )\n",
    "    return e\n",
    "\n",
    "def estimate_decoder(F, H, V):\n",
    "    return (V[:,1:]-H@V[:,:-1])@np.linalg.pinv(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(D,learning_batch,alpha,alphaF=1e-2,alphaD=1e-2):\n",
    "    p_classify = []\n",
    "    accuracy_temp = []\n",
    "    num_updates = int(np.floor((filtered_signals.shape[0]-1)/learning_batch)) # how many times can we update decoder based on learning batch    \n",
    "\n",
    "    # RANDOMIZE DATASET\n",
    "    randomized_integers = np.random.permutation(range(0,cued_target_position.shape[0]))\n",
    "    filtered_signals_randomized = filtered_signals[randomized_integers]\n",
    "    cued_target_position_randomized = cued_target_position[randomized_integers]\n",
    "    # batches the trials into each of the update batch\n",
    "    for ix in range(num_updates):\n",
    "        s = np.hstack([x for x in filtered_signals_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:,:]])# stack s (64 x (60 timepoints x learning batch size))\n",
    "        p_intended = np.hstack([np.tile(x[:,np.newaxis],60) for x in cued_target_position_randomized[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:]]) # stack p_intended (2 x 60 timepoints x learning batch size)\n",
    "        v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "        # CLASSIFY CURRENT DECODER ACCURACY\n",
    "        v_actual = D[-1]@s\n",
    "        for trial in range(learning_batch):\n",
    "            v_trial = v_actual[:,int(trial*60):int((trial+1)*60)] # velocities for each trials (2,60)\n",
    "            p_final = np.sum(v_trial,axis=1)[:,np.newaxis] # final position after integration (2,)\n",
    "            p_classify.append(classify(p_final))\n",
    "        \n",
    "        # UPDATE DECODER\n",
    "        u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "        q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "\n",
    "        # emg_windows against intended_targets (trial specific cued target)\n",
    "        F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "        V = copy.deepcopy(q)\n",
    "\n",
    "        # initial decoder estimate for gradient descent\n",
    "        D0 = np.random.rand(2,64)\n",
    "\n",
    "        # set alphas\n",
    "        H = np.zeros((2,2))\n",
    "        # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V), D0, method='BFGS', jac = lambda D: gradient_cost_l2(F,D,H,V), options={'disp': False})\n",
    "\n",
    "        # reshape to decoder parameters\n",
    "        W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "        # DO SMOOTHBATCH\n",
    "        W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "        D.append(W_new)\n",
    "\n",
    "        # COMPUTE CLASSIFICATION ACURACY \n",
    "        p_target = (cued_target_position[randomized_integers])[int(ix*learning_batch+1):int((ix+1)*learning_batch+1),:] # obtain target\n",
    "        accuracy_temp.append(classification_accuracy(p_target,p_classify[-learning_batch:]))\n",
    "\n",
    "    p_classify = np.asarray(p_classify)\n",
    "    return accuracy_temp,D,p_constrained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
