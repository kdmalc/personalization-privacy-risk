{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.  Start by modifying the Simulations code, worry about (a)synchronicity later.\n",
    "<br>\n",
    "1. The dec matrix is the weights to pass back an forth (I think), although it comes out of SmoothBatch first\n",
    "1. We are assuming we can test on the second half (updates 10-19ish) since learning should be complete by then!\n",
    "1. Scipy.optimize.minimize() runs many iters to fully minimize its cost function.  You can change it to run as many iters as you'd like, although AFAIK you won't know how many it takes to converge.  But this is still a good set up for FL.\n",
    "1. Hmm minimize() is doing BFGS rn and not SGD... not sure if that matters really.  Could probably implement SGD on my own or find it.  BFGS is 2nd order but we don't have a lot of parameters, I don't think.  Plus we can (already have?) solved analytically for the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#from numpy.matlib import repmat\n",
    "#from matplotlib import pyplot as plt\n",
    "#from scipy.signal import detrend, firwin, freqz, lfilter\n",
    "#from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from scipy.optimize import minimize, least_squares\n",
    "import copy\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *\n",
    "from simulations import *\n",
    "import time\n",
    "# Do the below if you're in the pytch environment\n",
    "#import pickle5 as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634c84f",
   "metadata": {},
   "source": [
    "# Reminder of Conditions Order\n",
    "\n",
    "NOTE: \n",
    "\n",
    "* **CONDITIONS** = array(['D_1', 'D_2', 'D_5', 'D_6', 'D_3', 'D_4', 'D_7','D_8']\n",
    "* **LEARNING RATES:** alpha = 0.25 and 0.75; alpha = 0.25 for D1, D2, D5, D6; alpha = 0.75 for D3, D4, D7, D8\n",
    "* **SMOOTHBATCH:** W_next = alpha*W_old + ((1 - alpha) * W_calc)\n",
    "\n",
    "* **DECODER INIT:** pos for D1 - D4, neg for D5 - D8\n",
    "\n",
    "* **PENALTY TERM:** $\\lambda_E$ = 1e-6 for all, $\\lambda_F$ = 1e-7 for all, $\\lambda_D$ = 1e-3 for 1, 3, 5, 7 and 1e-4 for 2, 4, 6, 8 \n",
    "\n",
    "\n",
    "| DECODER | ALPHA | PENALTY | DEC INIT |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0.25 | 1e-3 | + |\n",
    "| 2 | 0.25 | 1e-4 | + |\n",
    "| 3 | 0.75 | 1e-3 | + |\n",
    "| 4 | 0.75 | 1e-4 | + |\n",
    "| 5 | 0.25 | 1e-3 | - |\n",
    "| 6 | 0.25 | 1e-4 | - |\n",
    "| 7 | 0.75 | 1e-3 | - |\n",
    "| 8 | 0.75 | 1e-4 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fda900",
   "metadata": {},
   "source": [
    "## Load Our Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165598f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "emg_data_df1 = pd.read_csv(\"Data\\emg_full_data1.csv\")\n",
    "emg_data_df2 = pd.read_csv(\"Data\\emg_full_data2.csv\")\n",
    "emg_data_df = pd.concat((emg_data_df1, emg_data_df2))\n",
    "try:\n",
    "    emg_data_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "print(emg_data_df.shape)\n",
    "emg_data_df.head()\n",
    "'''\n",
    "# Just use the emg data directly from the pickle file for now\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845b1837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "#envelope_df50 = pd.read_csv(\"Data\\envelope_df50.csv\")\n",
    "envelope_df100 = pd.read_csv(\"Data\\envelope_df100.csv\")\n",
    "#envelope_df150 = pd.read_csv(\"Data\\envelope_df150.csv\")\n",
    "#envelope_df200 = pd.read_csv(\"Data\\envelope_df200.csv\")\n",
    "#envelope_df250 = pd.read_csv(\"Data\\envelope_df250.csv\")\n",
    "#envelope_df300 = pd.read_csv(\"Data\\envelope_df300.csv\")\n",
    "#raw_envs = [envelope_df50, envelope_df100, envelope_df150, envelope_df200, envelope_df250, envelope_df300]\n",
    "#all_envs = [env.drop('Unnamed: 0', axis=1) for env in raw_envs]\n",
    "try:\n",
    "    envelope_df100.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"NO UNNAMED COLUMN DETECTED!\")\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)\n",
    "print(envelope_df100.shape)\n",
    "envelope_df100.head()\n",
    "'''\n",
    "# Just use the emg data directly from the pickle file for now\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe511db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.021999835968018\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "with open('Data\\continuous_full_data_block1.pickle', 'rb') as handle:\n",
    "    #refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1, Hs_block1, alphas_block1, pDs_block1, times_block1, conditions_block1 = pickle.load(handle)\n",
    "    refs_block1, _, _, _, emgs_block1, Ws_block1, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "#with open('Data\\continuous_full_data_block2.pickle', 'rb') as handle:\n",
    "    #refs_block2, poss_block2, dec_vels_block2, int_vel_block2, emgs_block2, Ws_block2, Hs_block2, alphas_block2, pDs_block2, times_block2, conditions_block2 = pickle.load(handle)\n",
    "    #refs_block2, _, _, _, emgs_block2, Ws_block2, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c5ad93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 2, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 conditions, 20770 data points (only 19 unique sets!), xy, channels\n",
    "Ws_block1[keys[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9af127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614,\n",
       "       10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432,\n",
       "       20769])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7812773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder: (2, 64)\n",
      "\n",
      "Total difference between dec0 and dec1: 0.0\n",
      "E.g., as previously shown, the first two decs are the same\n",
      "\n",
      "Total difference between dec0 and dec2: 3.1981579823181594\n"
     ]
    }
   ],
   "source": [
    "dec_cond0_user1_update0 = Ws_block1[keys[0]][0,0,:,:]\n",
    "dec_cond0_user1_update1 = Ws_block1[keys[0]][0,update_ix[1],:,:]\n",
    "dec_cond0_user1_update2 = Ws_block1[keys[0]][0,update_ix[2],:,:]\n",
    "\n",
    "print(f\"Shape of decoder: {dec_cond0_user1_update0.shape}\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec1: {(dec_cond0_user1_update0 - dec_cond0_user1_update1).sum()}\")\n",
    "print(\"E.g., as previously shown, the first two decs are the same\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec2: {(dec_cond0_user1_update0 - dec_cond0_user1_update2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd624fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 20770, 64)\n",
      "(20770, 64)\n"
     ]
    }
   ],
   "source": [
    "#emg_cond0_user1_update0 = emg_data_df.iloc[:64,:].shape\n",
    "\n",
    "# (Condition, datapoints, channels)\n",
    "print(emgs_block1[keys[0]][:,:,:].shape)\n",
    "\n",
    "# Condition 0 of subject 1 (\"0\")\n",
    "print(emgs_block1[keys[0]][0,:,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e9ae3",
   "metadata": {},
   "source": [
    "## Run One Iteration On Above Data and Check Decoders Are the Same\n",
    "1. Modifying Simulations Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d034aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20770, 64)\n",
      "(20770, 2)\n"
     ]
    }
   ],
   "source": [
    "# Just 1 person\n",
    "filtered_signals = emgs_block1[keys[0]][0,:,:]\n",
    "# Read in the reference positions from the pickle file\n",
    "cued_target_position = refs_block1[keys[0]][0,:,:]\n",
    "\n",
    "print(filtered_signals.shape)\n",
    "print(cued_target_position.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2cc88fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously created random decoder, but we are trying to rerun\n",
    "#D_0 = np.random.rand(2,64)\n",
    "D_0 = Ws_block1[keys[0]][0,0,:,:]\n",
    "D = []\n",
    "D.append(D_0)\n",
    "\n",
    "learning_batch = update_ix[1]\n",
    "\n",
    "# Original simulation inputs\n",
    "#alpha = .95 # higher alpha means more old decoder (slower update)\n",
    "#alphaF = 1e-1\n",
    "#alphaD = 1e-1\n",
    "# For condition 1:\n",
    "alpha = .25 # higher alpha means more old decoder (slower update)\n",
    "# Assuming these are the same as lambda's, the decoder cost penalties\n",
    "alphaF = 1e-7\n",
    "alphaD = 1e-3\n",
    "#where is lambda E?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "130563dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_20228\\1343053775.py:75: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 137.753028\n",
      "         Iterations: 92\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 126\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 126.003342\n",
      "         Iterations: 102\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 134\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 170.284743\n",
      "         Iterations: 94\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 123\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 178.425631\n",
      "         Iterations: 93\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 126\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 167.921070\n",
      "         Iterations: 93\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 123\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 124.951363\n",
      "         Iterations: 92\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 126\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 183.885343\n",
      "         Iterations: 92\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 127\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 152.835724\n",
      "         Iterations: 93\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 129\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 197.192955\n",
      "         Iterations: 85\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 120\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 143.735160\n",
      "         Iterations: 97\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 126\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 130.086135\n",
      "         Iterations: 89\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 116\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 177.072275\n",
      "         Iterations: 96\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 125\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 187.387126\n",
      "         Iterations: 94\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 125\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 129.400313\n",
      "         Iterations: 83\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 109\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 170.421919\n",
      "         Iterations: 85\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 117\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 116.544480\n",
      "         Iterations: 91\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 122\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 75.547012\n",
      "         Iterations: 87\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 115\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.623634\n",
      "         Iterations: 118\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 135\n"
     ]
    }
   ],
   "source": [
    "#def simulation(D,learning_batch,alpha,alphaF=1e-7,alphaD=1e-3,display_info=False,num_iters=False):\n",
    "display_info=True\n",
    "num_iters=False\n",
    "\n",
    "# Should I define this a priori? \n",
    "num_updates = 19\n",
    "\n",
    "# batches the trials into each of the update batch\n",
    "# Do num_updates-1 because the very last update is only 1 datapoint, the 2nd to last is only 337\n",
    "for ix in range(num_updates-1):\n",
    "    #print(ix)\n",
    "    # For less cluttering when debugging\n",
    "    #display_info = False\n",
    "    \n",
    "    # Instead of using learning_batch, we should get the same results just using update_ix values\n",
    "    lower_bound = update_ix[ix]\n",
    "    if ix==(num_updates-1):\n",
    "        upper_bound = total_datapoints\n",
    "    else:\n",
    "        upper_bound = update_ix[ix+1]\n",
    "        \n",
    "    s = np.transpose(filtered_signals[lower_bound:upper_bound,:])  # Last working one\n",
    "    p_intended = np.transpose(cued_target_position[lower_bound:upper_bound,:])  # This is the last working one that runs\n",
    "    v_intended,p_constrained = output_new_decoder(s,D[-1],p_intended)\n",
    "\n",
    "    # UPDATE DECODER\n",
    "    u = copy.deepcopy(s) # u is the person's signal s (64 CHANNELS X TIMEPOINTS)\n",
    "    q = copy.deepcopy(v_intended) # use cued positions as velocity vectors for updating decoder should be 2 x num_trials\n",
    "    # emg_windows against intended_targets (trial specific cued target)\n",
    "    F = copy.deepcopy(u[:,:-1]) # note: truncate F for estimate_decoder\n",
    "    V = copy.deepcopy(q)\n",
    "\n",
    "    # initial decoder estimate for gradient descent\n",
    "    D0 = np.random.rand(2,64)\n",
    "    # set alphas\n",
    "    H = np.zeros((2,2))\n",
    "    # use scipy minimize for gradient descent and provide pre-computed analytical gradient for speed\n",
    "    if num_iters is False:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':num_iters})\n",
    "\n",
    "    # reshape to decoder parameters\n",
    "    W_hat = np.reshape(out.x,(2, 64))\n",
    "\n",
    "    # DO SMOOTHBATCH\n",
    "    W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "    D.append(W_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2506d",
   "metadata": {},
   "source": [
    "p_int: (2, 1202) <br>\n",
    "v_int: (2, 1202) <br>\n",
    "u: (64, 1202) <br>\n",
    "q: (2, 1202) <br>\n",
    "F: (64, 1201) <br>\n",
    "V: (2, 1202) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf0f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9683b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb486e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given decoder, what is the new position?\n",
    "def output_new_decoder(s,D,p_intended):\n",
    "    '''\n",
    "    s: (64 x (60 timepoints x learning batch size))\n",
    "    D: (2 x 64) previous D computed or random\n",
    "    p_intended: (2 x 60 timepoints x learning batch size)\n",
    "    '''\n",
    "    # take first trial, random decoder, and do target classification\n",
    "    v = D@s # actual decoded velocity (2,60)\n",
    "\n",
    "    # integrate decoded velocities into positions\n",
    "    p = []\n",
    "    for ix in range(v.shape[1]):\n",
    "        p.append(np.sum(v[:,:ix],axis=1))\n",
    "    p = np.asarray(p).T # actual decoded position (2,60)\n",
    "\n",
    "    # want error between intended and actual velocity but need to constrain actual velocity to target radius\n",
    "    p_constrained = np.asarray([constrain_p_actual(p_) for p_ in p.T]).T # constrained, (2,60)\n",
    "    # compute error between intended and actual position and take derivative to get intended velocity\n",
    "    v_intended = p_intended - p_constrained # (2,60)\n",
    "    return v_intended,p_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93171329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import copy\n",
    "#import time\n",
    "#import pickle\n",
    "#import numpy as np\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#import torch\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#from ARJ_options import args_parser\n",
    "#from ARJ_update import LocalUpdate, test_inference\n",
    "#from ARJ_my_models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "#from ARJ_utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    #path_project = os.path.abspath('..')\n",
    "    #logger = SummaryWriter('../logs')\n",
    "    \n",
    "    # This is actually useful.  Real SWE stuff.\n",
    "    args = args_parser()\n",
    "    exp_details(args)\n",
    "\n",
    "    #if args.gpu_id:\n",
    "    #    torch.cuda.set_device(args.gpu_id)\n",
    "    #device = 'cuda' if args.gpu else 'cpu'\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    #if args.model == 'cnn':\n",
    "    #    # Convolutional neural netork\n",
    "    #    if args.dataset == 'mnist':\n",
    "    #        global_model = CNNMnist(args=args)\n",
    "    #    elif args.dataset == 'fmnist':\n",
    "    #        global_model = CNNFashion_Mnist(args=args)\n",
    "    #    elif args.dataset == 'cifar':\n",
    "    #        global_model = CNNCifar(args=args)\n",
    "    #elif args.model == 'mlp':\n",
    "    #    # Multi-layer preceptron\n",
    "    #    img_size = train_dataset[0][0].shape\n",
    "    #    len_in = 1\n",
    "    #    for x in img_size:\n",
    "    #        len_in *= x\n",
    "    #        global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "    #                           dim_out=args.num_classes)\n",
    "    #else:\n",
    "    #    exit('Error: unrecognized model')\n",
    "    # So for us... just set it to linear regression I guess lol\n",
    "    '''global_model = linear_regression (D@s)'''\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    # KAI: I think this actually sends it to the GPU, not a \"client\" in the FL sense\n",
    "    #global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 2\n",
    "    val_loss_pre, counter = 0, 0\n",
    "\n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        local_weights, local_losses = [], []\n",
    "        print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                      idxs=user_groups[idx], logger=logger)\n",
    "            w, loss = local_model.update_weights(\n",
    "                model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        # update global weights\n",
    "        global_weights = average_weights(local_weights)\n",
    "\n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # Calculate avg training accuracy over all users at every epoch\n",
    "        list_acc, list_loss = [], []\n",
    "        global_model.eval()\n",
    "        for c in range(args.num_users):\n",
    "            local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                      idxs=user_groups[idx], logger=logger)\n",
    "            acc, loss = local_model.inference(model=global_model)\n",
    "            list_acc.append(acc)\n",
    "            list_loss.append(loss)\n",
    "        train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "    print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "    # Saving the objects train_loss and train_accuracy:\n",
    "    file_name = '../save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
    "        format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
    "               args.local_ep, args.local_bs)\n",
    "\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump([train_loss, train_accuracy], f)\n",
    "\n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "\n",
    "    # PLOTTING (optional)\n",
    "    # import matplotlib\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # matplotlib.use('Agg')\n",
    "\n",
    "    # Plot Loss curve\n",
    "    # plt.figure()\n",
    "    # plt.title('Training Loss vs Communication rounds')\n",
    "    # plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "    # plt.ylabel('Training loss')\n",
    "    # plt.xlabel('Communication Rounds')\n",
    "    # plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "    #             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "    #                    args.iid, args.local_ep, args.local_bs))\n",
    "    #\n",
    "    # # Plot Average Accuracy vs Communication rounds\n",
    "    # plt.figure()\n",
    "    # plt.title('Average Accuracy vs Communication rounds')\n",
    "    # plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "    # plt.ylabel('Average Accuracy')\n",
    "    # plt.xlabel('Communication Rounds')\n",
    "    # plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "    #             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "    #                    args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import copy\n",
    "#import time\n",
    "#import pickle\n",
    "#import numpy as np\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#import torch\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#from ARJ_options import args_parser\n",
    "#from ARJ_update import LocalUpdate, test_inference\n",
    "#from ARJ_my_models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "#from ARJ_utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "\n",
    "\n",
    "# define paths\n",
    "#path_project = os.path.abspath('..')\n",
    "#logger = SummaryWriter('../logs')\n",
    "\n",
    "# This is actually useful.  Real SWE stuff.\n",
    "args = args_parser()\n",
    "exp_details(args)\n",
    "\n",
    "# load dataset and user groups\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "# BUILD MODEL\n",
    "# So for us... just set it to linear regression I guess lol\n",
    "# I don't know if I need to explicitly define a model object... would make things more legible tho\n",
    "'''global_model = linear_regression (D@s)'''\n",
    "class model():\n",
    "    pass\n",
    "\n",
    "# Set the model to train and send it to device.\n",
    "# KAI: I think this actually sends it to the GPU, not a \"client\" in the FL sense\n",
    "#global_model.to(device)\n",
    "global_model.train()\n",
    "print(global_model)\n",
    "# minimize()?\n",
    "\n",
    "# copy weights\n",
    "#global_weights = global_model.state_dict()\n",
    "'''global_weights = D'''\n",
    "\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "    global_model.train()\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger)\n",
    "        w, loss = local_model.update_weights(\n",
    "            model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "    # update global weights\n",
    "    global_weights = average_weights(local_weights)\n",
    "\n",
    "    # update global weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    for c in range(args.num_users):\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger)\n",
    "        acc, loss = local_model.inference(model=global_model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n",
    "# Test inference after completion of training\n",
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "# Saving the objects train_loss and train_accuracy:\n",
    "file_name = '../save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
    "    format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
    "           args.local_ep, args.local_bs)\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump([train_loss, train_accuracy], f)\n",
    "\n",
    "print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "\n",
    "# PLOTTING (optional)\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "# plt.figure()\n",
    "# plt.title('Training Loss vs Communication rounds')\n",
    "# plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "# plt.ylabel('Training loss')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))\n",
    "#\n",
    "# # Plot Average Accuracy vs Communication rounds\n",
    "# plt.figure()\n",
    "# plt.title('Average Accuracy vs Communication rounds')\n",
    "# plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "# plt.ylabel('Average Accuracy')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33f420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc684ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
