{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.  Start by modifying the Simulations code, worry about (a)synchronicity later.\n",
    "<br>\n",
    "1. The dec matrix is the weights to pass back an forth (I think), although it comes out of SmoothBatch first\n",
    "1. We are assuming we can test on the second half (updates 10-19ish) since learning should be complete by then!\n",
    "1. Scipy.optimize.minimize() runs many iters to fully minimize its cost function.  You can change it to run as many iters as you'd like, although AFAIK you won't know how many it takes to converge.  But this is still a good set up for FL.\n",
    "1. Hmm minimize() is doing BFGS rn and not SGD... not sure if that matters really.  Could probably implement SGD on my own or find it.  BFGS is 2nd order but we don't have a lot of parameters, I don't think.  Plus we can (already have?) solved analytically for the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize, least_squares\n",
    "import copy\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *\n",
    "from simulations import *\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9450bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data'\n",
    "cond0_filename = r'\\cond0_dict_list.p'\n",
    "all_decs_init_filename = r'\\all_decs_init.p'\n",
    "id2color = {0:'lightcoral', 1:'maroon', 2:'chocolate', 3:'darkorange', 4:'gold', 5:'olive', 6:'olivedrab', \n",
    "            7:'lawngreen', 8:'aquamarine', 9:'deepskyblue', 10:'steelblue', 11:'violet', 12:'darkorchid', 13:'deeppink'}\n",
    "implemented_client_training_methods = ['EtaGradStep', 'EtaScipyMinStep', 'FullScipyMinStep']\n",
    "num_participants = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634c84f",
   "metadata": {},
   "source": [
    "# Reminder of Conditions Order\n",
    "\n",
    "NOTE: \n",
    "\n",
    "* **CONDITIONS** = array(['D_1', 'D_2', 'D_5', 'D_6', 'D_3', 'D_4', 'D_7','D_8']\n",
    "* **LEARNING RATES:** alpha = 0.25 and 0.75; alpha = 0.25 for D1, D2, D5, D6; alpha = 0.75 for D3, D4, D7, D8\n",
    "* **SMOOTHBATCH:** W_next = alpha*W_old + ((1 - alpha) * W_calc)\n",
    "\n",
    "* **DECODER INIT:** pos for D1 - D4, neg for D5 - D8\n",
    "\n",
    "* **PENALTY TERM:** $\\lambda_E$ = 1e-6 for all, $\\lambda_F$ = 1e-7 for all, $\\lambda_D$ = 1e-3 for 1, 3, 5, 7 and 1e-4 for 2, 4, 6, 8 \n",
    "\n",
    "\n",
    "| DECODER | ALPHA | PENALTY | DEC INIT |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0.25 | 1e-3 | + |\n",
    "| 2 | 0.25 | 1e-4 | + |\n",
    "| 3 | 0.75 | 1e-3 | + |\n",
    "| 4 | 0.75 | 1e-4 | + |\n",
    "| 5 | 0.25 | 1e-3 | - |\n",
    "| 6 | 0.25 | 1e-4 | - |\n",
    "| 7 | 0.75 | 1e-3 | - |\n",
    "| 8 | 0.75 | 1e-4 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fda900",
   "metadata": {},
   "source": [
    "## Load Our Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c61fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cond0_dict_list = [0]*num_participants\n",
    "for idx in range(num_participants):\n",
    "    cond0_dict_list[idx] = {'training':emgs_block1[keys[idx]][0,:,:], 'labels':refs_block1[keys[idx]][0,:,:]}\n",
    "\n",
    "with open(path+cond0_filename, 'wb') as fp:\n",
    "    pickle.dump(cond0_dict_list, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "init_decoders = [Ws_block1[keys[i]][:, 0, :, :] for i in range(num_participants)]\n",
    "with open(path+all_decs_init_filename, 'wb') as fp:\n",
    "    pickle.dump(init_decoders, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "'''\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df453f",
   "metadata": {},
   "source": [
    "# Create Federated Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57f85b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python src/emg_fl_main.py --num_users=14, --model=___, --dataset=___, --num_classes=___, --iid=0)\n",
    "\n",
    "#def run_fl_sim(data_path,training_data,labels,epochs=10,num_users=14,C=0.1,local_epochs=10,local_batch_sz=10,lr=0.01,SGD_momentum=0.5,optimizer='sgd',iid=0,unequal=0,stopping_rounds=10,verbose=True,seed=1):\n",
    "    # Other possible parameters\n",
    "    #'num_channels'=64,\n",
    "    #'norm'='batch_norm',    \n",
    "    # Figure out what dataset to use... all EMG data?\n",
    "    # Idk how many classes... we are doing regression...\n",
    "    #parser.add_argument('num_classes', type=int, default=10\n",
    "    # Explanation kept for these\n",
    "    # Our application is probably non-IID?\n",
    "    #parser.add_argument('iid', type=int, default=1,help='Default set to IID. Set to 0 for non-IID.')\n",
    "    # Our splits are currently equal but irl they would not be\n",
    "    #parser.add_argument('unequal', type=int, default=0,\n",
    "    #                    help='whether to use unequal data splits for  \\\n",
    "    #                    non-i.i.d setting (use 0 for equal splits)')\n",
    "    \n",
    "    # Probably need to also pass in alphaF/E/D, maybe D_0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba9acf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase:\n",
    "    # Hard coded attributes --> THESE SHOULD NOT CHANGE, SHARED FOR THE ENTIRE CLASS\n",
    "    num_updates = 19\n",
    "    cphs_starting_update = 10\n",
    "    update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n",
    "    \n",
    "    def __init__(self, ID, w, method, smoothbatch=False, verbose=False, current_round=0):\n",
    "        self.type = 'Base'  # This needs to get overwritten\n",
    "        # Client ID number\n",
    "        self.ID = ID\n",
    "        # Linear regression weights AKA the decoder\n",
    "        self.w = w\n",
    "        self.w_prev = w\n",
    "        self.local_error_log = []\n",
    "        self.global_error_log = []\n",
    "        self.method = method\n",
    "        self.current_round = current_round\n",
    "        self.verbose = verbose\n",
    "        self.smoothbatch = smoothbatch\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return f\"{self.type}{self.ID}\"\n",
    "    \n",
    "    def display_info(self): \n",
    "        return f\"{self.type} model: {self.ID}\\nCurrent Round: {self.current_round}\\nTraining Method: {self.method}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3170c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D[-1], method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n",
    "\n",
    "class TrainingMethods:\n",
    "    # Different training approaches\n",
    "    def train_eta_gradstep(self, w, eta, F, D, H, V, learning_batch, alphaF, alphaD):\n",
    "        grad_cost = np.reshape(gradient_cost_l2(F, D, H, V, learning_batch, alphaF, alphaD),(2, 64))\n",
    "        w_new = w - eta*grad_cost\n",
    "        return w_new\n",
    "\n",
    "    def train_eta_scipyminstep(self, w, eta, F, D, H, V, learning_batch, alphaF, alphaD, D0, display_info, full=False):\n",
    "        if full:\n",
    "            out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n",
    "        else:\n",
    "            out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n",
    "        w_new = np.reshape(out.x,(2, 64))\n",
    "        return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aaddf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server(ModelBase):\n",
    "    def __init__(self, ID, D0, method, all_clients, smoothbatch=False, C=0.1, current_round=0, advance_each_iter=False, verbose=False):\n",
    "        #class ModelBase: __init__(self, ID, w, method, smoothbatch=False, verbose=False, current_round=0)\n",
    "        super().__init__(ID, D0, method, smoothbatch=smoothbatch, current_round=current_round, verbose=verbose)\n",
    "        # Not input\n",
    "        self.type = 'Server'\n",
    "        self.num_avail_clients = 0\n",
    "        self.available_clients_list = [0]*len(all_clients)\n",
    "        self.num_chosen_clients = 0\n",
    "        self.chosen_clients_lst = [0]*len(all_clients)\n",
    "        self.all_clients = all_clients\n",
    "        # FL Specific Params\n",
    "        self.C = C\n",
    "        self.advance_each_iter = advance_each_iter\n",
    "        \n",
    "    def train_client_and_log(self, client_set, set_client_global_method=False):\n",
    "        current_local_lst = []\n",
    "        current_global_lst = []\n",
    "        for my_client in client_set:\n",
    "            if set_client_global_method:\n",
    "                my_client.global_method = self.method\n",
    "            my_client.execute_training_loop()\n",
    "            current_local_lst.append((my_client.ID, my_client.eval_model(which='local')))\n",
    "            if not set_client_global_method:  # Using this as a proxy for NoFL, where we cannot run the global model\n",
    "                current_global_lst.append((my_client.ID, my_client.eval_model(which='global')))\n",
    "            if self.advance_each_iter:\n",
    "                my_client.advance_each_iter = 1\n",
    "        self.local_error_log.append(current_local_lst)\n",
    "        if not set_client_global_method: #Same reason as above\n",
    "            self.global_error_log.append(current_global_lst)\n",
    "        \n",
    "    def execute_FL_loop(self):\n",
    "        # Update global round number\n",
    "        self.current_round += 1\n",
    "        \n",
    "        if self.method=='FedAvg':\n",
    "            #print('FedAvg')\n",
    "            # Choose fraction C of available clients\n",
    "            self.set_available_clients_list()\n",
    "            self.choose_clients()\n",
    "            # Send those clients the current global model\n",
    "            for my_client in self.chosen_clients_lst:\n",
    "                my_client.global_w = self.w\n",
    "            # Let those clients train (this autoselects the chosen_client_lst to use)\n",
    "            self.train_client_and_log(client_set=self.chosen_clients_lst)\n",
    "            # AGGREGATION\n",
    "            self.w_prev = copy.copy(self.w)\n",
    "            self.agg_local_weights()  # This func sets self.w, eg the new decoder\n",
    "            # Do SmoothBatch\n",
    "            if self.smoothbatch!=0:\n",
    "                #W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "                self.w = self.smoothbatch*self.w + ((1 - self.smoothbatch)*self.w_prev)\n",
    "        elif self.method=='NoFL':\n",
    "            self.train_client_and_log(client_set=self.all_clients, set_client_global_method=True)\n",
    "        else:\n",
    "            print('Method not currently supported')\n",
    "            print('Please reset method to FedAvg')\n",
    "    \n",
    "    def get_num_available_clients(self):\n",
    "        return len(self.available_clients_list)\n",
    "            \n",
    "    def set_available_clients_list(self):\n",
    "        self.num_avail_clients = 0\n",
    "        self.available_clients_list = [0]*len(self.all_clients)\n",
    "        for idx, my_client in enumerate(self.all_clients):\n",
    "            if my_client.availability:\n",
    "                self.available_clients_list[idx] = my_client\n",
    "                self.num_avail_clients += 1\n",
    "    \n",
    "    def choose_clients(self):\n",
    "        # First reset all clients to be not chosen\n",
    "        for my_client in self.all_clients:\n",
    "            my_client.reset_chosen()\n",
    "        # Then check what client are available this round\n",
    "        self.set_available_clients_list()\n",
    "        # Now choose frac C clients from the resulting available clients\n",
    "        if self.num_avail_clients > 0:\n",
    "            self.num_chosen_clients = int(np.ceil(self.num_avail_clients*self.C))\n",
    "            # Right now it chooses 2 at random: 14*.1=1.4 --> 2\n",
    "            self.chosen_clients_lst = random.sample(self.available_clients_list, len(self.available_clients_list))[:self.num_chosen_clients]\n",
    "            for my_client in self.chosen_clients_lst:\n",
    "                my_client.chosen_status = 1\n",
    "        else:\n",
    "            print(f\"ERROR: Number of available clients must be greater than 0: {self.num_avail_clients}\")\n",
    "    \n",
    "    def agg_local_weights(self):\n",
    "        # From McMahan 2017 (vanilla FL)\n",
    "        # I think they actually use the number of datapoints, not the learning rates...\n",
    "        # Other paper was using dynamic learning rate adjustment step, not this paper\n",
    "        # Aggregate learning rates from each local model\n",
    "        # When aggregating irl it would be better to query each client for weights and lr at the same time\n",
    "        #summed_lr = 0\n",
    "        #for my_client in self.chosen_clients_lst:\n",
    "        #    summed_lr += my_client.lr\n",
    "        summed_num_datapoints = 0\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            summed_num_datapoints += my_client.learning_batch\n",
    "        # Aggregate local model weights, weighted by normalized local learning rate\n",
    "        aggr_w = 0\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            aggr_w += (my_client.learning_batch/summed_num_datapoints) * my_client.w\n",
    "        # Loop is complete, new global decoder is self.w\n",
    "        self.w = aggr_w\n",
    "        # ^ Is this just gonna grow to infinity since all the values are positive?\n",
    "        # E.g. it seems like more clients just means bigger dec?\n",
    "        # Still not clear how the global decoder will be able to adapt to different channels for different orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "920b4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client(ModelBase, TrainingMethods):\n",
    "    def __init__(self, ID, w, method, local_data, smoothbatch=True, current_round=0, availability=1, full_data=False, global_method='FedAvg', streaming=False, advance_each_iter=False, eta=1, num_steps=1, delay_scaling=5, random_delays=False, download_delay=1, upload_delay=1, local_round_threshold=250, condition_number=0, verbose=False):\n",
    "        #class ModelBase: __init__(self, ID, w, method, smoothbatch=False, verbose=False, current_round=0)\n",
    "        super().__init__(ID, w, method, smoothbatch=smoothbatch, current_round=current_round, verbose=verbose)\n",
    "        # NOT INPUT\n",
    "        self.type = 'Client'\n",
    "        self.chosen_status = 0\n",
    "        self.local_error_log = []\n",
    "        self.global_error_log = []\n",
    "        # Sentinel Values\n",
    "        self.F = None\n",
    "        self.V = None\n",
    "        self.D = None\n",
    "        self.H = np.zeros((2,2))\n",
    "        self.learning_batch = None\n",
    "        self.eta = eta\n",
    "        # Local dataset\n",
    "        self.training_data = local_data['training']\n",
    "        self.labels = local_data['labels']\n",
    "        self.global_method = global_method\n",
    "        if self.global_method=='NoFL':\n",
    "            starting_update = 0\n",
    "        else:\n",
    "            starting_update = self.cphs_starting_update\n",
    "        self.global_method = None\n",
    "        # Availability for training\n",
    "        self.availability = availability\n",
    "        # Toggle streaming aspect of data collection --> eg each round, use a new update's data or not\n",
    "        # If the 3 of these are mutually exclusive, then I should probably condense into 1 variable...\n",
    "        self.full_data = full_data  #e.g. just ignore updates and use all the data\n",
    "        self.streaming = streaming\n",
    "        self.advance_each_iter = advance_each_iter\n",
    "        # Number of gradient steps to take when training (eg amount of local computation)\n",
    "        self.num_steps = num_steps\n",
    "        # Boolean setting whether or not up/download delays should be random or predefined\n",
    "        self.random_delays = random_delays\n",
    "        # Scaling from random [0,1] to number of seconds\n",
    "        self.delay_scaling = delay_scaling\n",
    "        # Set the delay times\n",
    "        if self.random_delays: \n",
    "            self.download_delay = random.random()*self.delay_scaling\n",
    "            self.upload_delay = random.random()*self.delay_scaling\n",
    "        else:\n",
    "            self.download_delay = download_delay\n",
    "            self.upload_delay = upload_delay\n",
    "        self.current_update = starting_update\n",
    "        self.local_round_threshold = local_round_threshold\n",
    "        # ML Parameters / Conditions\n",
    "        # This should probably be a dictionary at some point\n",
    "        if condition_number==0:\n",
    "            self.lr = 0.25\n",
    "            self.alphaF = 1e-7\n",
    "            self.alphaD = 1e-3\n",
    "        self.verbose = verbose\n",
    "            \n",
    "    def execute_training_loop(self):\n",
    "        #self.global_round\n",
    "        self.simulate_data_stream()\n",
    "        self.train_model()\n",
    "        self.eval_model(which='local')\n",
    "        if self.global_method!=\"NoFL\":\n",
    "            self.eval_model(which='global')\n",
    "        \n",
    "    def simulate_delay(self, incoming):\n",
    "        if incoming:\n",
    "            time.sleep(self.download_delay+random.random())\n",
    "        else:\n",
    "            time.sleep(self.upload_delay+random.random())\n",
    "            \n",
    "    def simulate_data_stream(self):\n",
    "        self.current_round += 1\n",
    "        if self.full_data:\n",
    "            #print(\"FULL\")\n",
    "            lower_bound = update_ix[0]\n",
    "            upper_bound = update_ix[-1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "        elif self.streaming:\n",
    "            #print(\"STREAMING\")\n",
    "            if self.current_round > self.local_round_threshold:\n",
    "                self.local_round_threshold += self.current_round\n",
    "                self.current_update += 1\n",
    "            lower_bound = update_ix[self.current_update]\n",
    "            upper_bound = update_ix[self.current_update+1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "        elif self.advance_each_iter:\n",
    "            #print(\"ADVANCE\")\n",
    "            self.current_update += 1\n",
    "            lower_bound = update_ix[self.current_update]\n",
    "            upper_bound = update_ix[self.current_update+1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "        else:\n",
    "            raise ValueError('This data streaming functionality is not supported')\n",
    "        ####################################################################################################\n",
    "        # FIX THIS BASED ON NB200\n",
    "        s = np.transpose(self.training_data[lower_bound:upper_bound,:])\n",
    "        v_actual = self.w@s\n",
    "        dt=1/60\n",
    "        p_actual = np.sum(v_actual, axis=1)*dt  # dt=1/60\n",
    "        p_actual = np.reshape(p_actual, (p_actual.shape[0], 1))\n",
    "        p_reference = np.transpose(self.labels[lower_bound:upper_bound,:])\n",
    "        \n",
    "        self.F = s[:,:-1] # note: truncate F for estimate_decoder\n",
    "        self.V = (p_reference - p_actual)*dt\n",
    "        self.D = copy.copy(self.w)\n",
    "        self.H = np.zeros((2,2))\n",
    "        ####################################################################################################\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Set the w_prev equal to the current w:\n",
    "        self.w_prev = copy.copy(self.w)\n",
    "        if self.global_method!=\"NoFL\":\n",
    "            # Overwrite local model with the new global model\n",
    "            self.w = self.global_w\n",
    "        # Should D0 get set to self.w_prev instead?\n",
    "        D_0 = np.random.rand(2,64)\n",
    "        for i in range(self.num_steps):\n",
    "            if self.method=='EtaGradStep':\n",
    "                self.w = self.train_eta_gradstep(self.w, self.eta, self.F, self.D, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD)\n",
    "            elif self.method=='EtaScipyMinStep':\n",
    "                self.w = self.train_eta_scipyminstep(self.w, self.eta, self.F, self.D, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, D_0, self.verbose)\n",
    "            elif self.method=='FullScipyMinStep':\n",
    "                self.w = self.train_eta_scipyminstep(self.w, self.eta, self.F, self.D, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, D_0, self.verbose, full=True)\n",
    "            else:\n",
    "                print(\"Unrecognized method\")\n",
    "        #if self.smoothbatch!=0:\n",
    "        #        #W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "        #        self.w = self.smoothbatch*self.w + ((1 - self.smoothbatch)*self.w_prev)\n",
    "        # Do SmoothBatch\n",
    "        #W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "        self.w = self.lr*self.w + ((1 - self.lr) * self.w_prev)\n",
    "        #^^ IS IT JUST THE LEARNING RATE? OR IS IT A SEPARATE ALPHA?  I THINK IT IS THE SAME...\n",
    "        # Should I still add a way to toggle it?\n",
    "    \n",
    "    def reset_chosen(self):\n",
    "        self.chosen_status = 0\n",
    "        \n",
    "    def eval_model(self, which):\n",
    "        if which=='local':\n",
    "            my_dec = self.w\n",
    "            my_error_log = self.local_error_log\n",
    "        elif which=='global':\n",
    "            #print(\"You ran the global model...\")\n",
    "            my_dec = self.global_w\n",
    "            my_error_log = self.global_error_log\n",
    "        else:\n",
    "            print(\"Please set <which> to either local or global\")\n",
    "        temp = np.ceil(cost_l2(self.F, my_dec, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD))\n",
    "        try:\n",
    "            out = int(temp)\n",
    "        except (OverflowError,ValueError):  # inf, nan\n",
    "            out = 1_000_000_000_000\n",
    "        # Ensure that this actually appends to the refernced list and not the temp list\n",
    "        my_error_log.append(out)\n",
    "        return out\n",
    "        \n",
    "    def test_inference(self):\n",
    "        # Essentially, choose a random(?) section of data and compare how dec performs\n",
    "        # Is this really any different from the eval funcs?\n",
    "        print(\"Testing Functionality Not Written Yet\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b2b6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+cond0_filename, 'rb') as fp:\n",
    "    cond0_dict_list = pickle.load(fp)\n",
    "    \n",
    "with open(path+all_decs_init_filename, 'rb') as fp:\n",
    "    init_decoders = pickle.load(fp)\n",
    "cond0_init_decs = [dec[0, :, :] for dec in init_decoders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "582dfd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_0 = np.random.rand(2,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5f77b",
   "metadata": {},
   "source": [
    "Check streaming condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0c17153",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_c0_etascipy_streaming = [Client(i, cond0_init_decs[i], 'EtaScipyMinStep', cond0_dict_list[i], smoothbatch=True, streaming=True, delay_scaling=0) for i in range(14)]\n",
    "\n",
    "#ID, D0, method, all_clients, smoothbatch=False, C=0.1, current_round=0, advance_each_iter=False, verbose=False):\n",
    "global_model = Server(-100, D_0, 'FedAvg', user_c0_etascipy_streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d65de0b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 10)\n",
      "(2, 1, 10)\n",
      "(3, 1, 10)\n",
      "\n",
      "Global Error Log\n",
      "[[(0, 477207), (11, 658237)], [(4, 9961), (8, 19656)], [(8, 39730), (4, 19258)]]\n",
      "\n",
      "Local Error Log\n",
      "[[(0, 14412), (11, 22012)], [(4, 10650), (8, 21666)], [(8, 74825), (4, 41184)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_3944\\3624381326.py:14: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n"
     ]
    }
   ],
   "source": [
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "print()\n",
    "print(\"Global Error Log\")\n",
    "print(global_model.global_error_log)\n",
    "print()\n",
    "print(\"Local Error Log\")\n",
    "print(global_model.local_error_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6807e",
   "metadata": {},
   "source": [
    "Check full data condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0c9e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_3944\\3624381326.py:14: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 10)\n",
      "(2, 0, 10)\n",
      "(3, 1, 10)\n",
      "\n",
      "Global Error Log\n",
      "[[(8, 488311), (6, 902159)], [(13, 46840), (6, 1025509)], [(0, 59049), (1, 187080)]]\n",
      "\n",
      "Local Error Log\n",
      "[[(8, 21590), (6, 38313)], [(13, 38423), (6, 1266235)], [(0, 35179), (1, 117299)]]\n"
     ]
    }
   ],
   "source": [
    "user_c0_etascipy_full = [Client(i, cond0_init_decs[i], 'EtaScipyMinStep', cond0_dict_list[i], smoothbatch=True, full_data=True, delay_scaling=0) for i in range(14)]\n",
    "#ID, D0, method, all_clients, smoothbatch=False, C=0.1, current_round=0, advance_each_iter=False, verbose=False):\n",
    "global_model = Server(100, D_0, 'FedAvg', user_c0_etascipy_full)\n",
    "\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "print()\n",
    "print(\"Global Error Log\")\n",
    "print(global_model.global_error_log)\n",
    "print()\n",
    "print(\"Local Error Log\")\n",
    "print(global_model.local_error_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6595f73",
   "metadata": {},
   "source": [
    "Check advance_each_iter condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c25fbebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 10)\n",
      "(2, 0, 10)\n",
      "(3, 0, 10)\n",
      "\n",
      "Global Error Log\n",
      "[[(10, 104572), (3, 5679910)], [(9, 7633), (8, 14521)], [(10, 7040), (6, 31257)]]\n",
      "\n",
      "Local Error Log\n",
      "[[(10, 3949), (3, 155357)], [(9, 8301), (8, 12609)], [(10, 15723), (6, 22243)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_3944\\3624381326.py:14: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n"
     ]
    }
   ],
   "source": [
    "user_c0_etascipy_advance = [Client(i, cond0_init_decs[i], 'EtaScipyMinStep', cond0_dict_list[i], smoothbatch=True, advance_each_iter=True, delay_scaling=0) for i in range(14)]\n",
    "#ID, D0, method, all_clients, smoothbatch=False, C=0.1, current_round=0, advance_each_iter=False, verbose=False):\n",
    "global_model = Server(-101, D_0, 'FedAvg', user_c0_etascipy_advance)\n",
    "\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "print()\n",
    "print(\"Global Error Log\")\n",
    "print(global_model.global_error_log)\n",
    "print()\n",
    "print(\"Local Error Log\")\n",
    "print(global_model.local_error_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f96dcd",
   "metadata": {},
   "source": [
    "## Double Checking Running the No-FL Case\n",
    "> E.g. we should see some kind of convergence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e27495c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round\n",
      "0\n",
      "\n",
      "Local Round\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "user_c0_fullscipy = [Client(i, cond0_init_decs[i], 'FullScipyMinStep', cond0_dict_list[i], global_method='NoFL', smoothbatch=True, advance_each_iter=True, delay_scaling=0) for i in range(14)]\n",
    "global_model_no_fl = Server(-1, D_0, 'NoFL', user_c0_fullscipy)\n",
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8b4695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_3944\\3624381326.py:12: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round\n",
      "1\n",
      "\n",
      "Local Round\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "global_model_no_fl.execute_FL_loop()\n",
    "\n",
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "661de729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_3944\\3624381326.py:12: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round\n",
      "2\n",
      "\n",
      "Local Round\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "global_model_no_fl.execute_FL_loop()\n",
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0cb35607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2202e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_3944\\3624381326.py:12: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2\n",
      "Iter 4\n",
      "Iter 6\n",
      "Iter 8\n",
      "Iter 10\n",
      "Iter 12\n",
      "Iter 14\n",
      "Complete\n",
      "----------------------------------\n",
      "Global Round\n",
      "17\n",
      "\n",
      "Local Round\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# We are at update 2/19.  Therefore 19-2=17-1=16\n",
    "# Why does it break at 16?  Local round/update=18... 18+1 hmm\n",
    "num_updates_left = 15\n",
    "for i in range(num_updates_left):\n",
    "    if i%np.ceil(num_updates_left*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model_no_fl.execute_FL_loop()\n",
    "print(\"Complete\")\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fcceb08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee865a",
   "metadata": {},
   "source": [
    "## 1 Scipy Step, 1000 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85218300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_3944\\3624381326.py:14: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100\n",
      "Iter 200\n",
      "Iter 300\n",
      "Iter 400\n",
      "Iter 500\n",
      "Iter 600\n",
      "Iter 700\n",
      "Iter 800\n",
      "Iter 900\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "user_c0_1ScipyStep = [Client(i, cond0_init_decs[i], 'EtaScipyMinStep', cond0_dict_list[i], smoothbatch=True, streaming=True, delay_scaling=0) for i in range(14)]\n",
    "#ID, D0, method, all_clients, smoothbatch=False, C=0.1, current_round=0, advance_each_iter=False, verbose=False):\n",
    "global_model1 = Server(1, D_0, 'FedAvg', user_c0_1ScipyStep)\n",
    "\n",
    "big_loop_iters = 1000\n",
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model1.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd69857",
   "metadata": {},
   "source": [
    "Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b69c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.local_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Local Cost Eval As a Function of Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.local_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Local Cost Eval As a Function of Iteration')\n",
    "plt.ylim(0, 1.1*10**7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2cd61",
   "metadata": {},
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.global_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Global Cost Eval As a Function of Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.global_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Global Cost Eval As a Function of Iteration')\n",
    "plt.ylim(0, 0.5*10**7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56c245",
   "metadata": {},
   "source": [
    "## 10 Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cca93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_c0_10scipy = [Client(i, cond0_init_decs[i], 'EtaScipyMinStep', cond0_dict_list[i], num_steps=10, smoothbatch=True, streaming=True, delay_scaling=0) for i in range(14)]\n",
    "global_model2 = Server(2, D_0, 'FedAvg', user_c0_10scipy)\n",
    "\n",
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model2.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbcdb3",
   "metadata": {},
   "source": [
    "## Varying Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc4b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_c0_eta10 = [Client(i, cond0_init_decs[i], 'EtaGradStep', cond0_dict_list[i], eta=10, smoothbatch=True, streaming=True, delay_scaling=0) for i in range(14)]\n",
    "#ID, D0, method, all_clients, smoothbatch=False, C=0.1, current_round=0, advance_each_iter=False, verbose=False):\n",
    "global_model3 = Server(3, D_0, 'FedAvg', user_c0_eta10)\n",
    "\n",
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model3.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dabdb5",
   "metadata": {},
   "source": [
    "## Full Scipy.Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1e17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_c0_fullscipy = [Client(i, cond0_init_decs[i], 'FullScipyMinStep', cond0_dict_list[i], smoothbatch=True, streaadvance_each_iterming=True, delay_scaling=0) for i in range(14)]\n",
    "global_model4 = Server(4, D_0, 'FedAvg', user_c0_fullscipy)\n",
    "\n",
    "for i in range(18):\n",
    "    if i%np.ceil(18*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model4.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a54aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc684ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
