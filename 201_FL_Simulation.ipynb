{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.  Start by modifying the Simulations code, worry about (a)synchronicity later.\n",
    "<br>\n",
    "1. The dec matrix is the weights to pass back an forth (I think), although it comes out of SmoothBatch first\n",
    "1. We are assuming we can test on the second half (updates 10-19ish) since learning should be complete by then!\n",
    "1. Scipy.optimize.minimize() runs many iters to fully minimize its cost function.  You can change it to run as many iters as you'd like, although AFAIK you won't know how many it takes to converge.  But this is still a good set up for FL.\n",
    "1. Hmm minimize() is doing BFGS rn and not SGD... not sure if that matters really.  Could probably implement SGD on my own or find it.  BFGS is 2nd order but we don't have a lot of parameters, I don't think.  Plus we can (already have?) solved analytically for the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#from numpy.matlib import repmat\n",
    "#from matplotlib import pyplot as plt\n",
    "#from scipy.signal import detrend, firwin, freqz, lfilter\n",
    "#from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from scipy.optimize import minimize, least_squares\n",
    "import copy\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *\n",
    "from simulations import *\n",
    "import time\n",
    "# Do the below if you're in the pytch environment\n",
    "#import pickle5 as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634c84f",
   "metadata": {},
   "source": [
    "# Reminder of Conditions Order\n",
    "\n",
    "NOTE: \n",
    "\n",
    "* **CONDITIONS** = array(['D_1', 'D_2', 'D_5', 'D_6', 'D_3', 'D_4', 'D_7','D_8']\n",
    "* **LEARNING RATES:** alpha = 0.25 and 0.75; alpha = 0.25 for D1, D2, D5, D6; alpha = 0.75 for D3, D4, D7, D8\n",
    "* **SMOOTHBATCH:** W_next = alpha*W_old + ((1 - alpha) * W_calc)\n",
    "\n",
    "* **DECODER INIT:** pos for D1 - D4, neg for D5 - D8\n",
    "\n",
    "* **PENALTY TERM:** $\\lambda_E$ = 1e-6 for all, $\\lambda_F$ = 1e-7 for all, $\\lambda_D$ = 1e-3 for 1, 3, 5, 7 and 1e-4 for 2, 4, 6, 8 \n",
    "\n",
    "\n",
    "| DECODER | ALPHA | PENALTY | DEC INIT |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0.25 | 1e-3 | + |\n",
    "| 2 | 0.25 | 1e-4 | + |\n",
    "| 3 | 0.75 | 1e-3 | + |\n",
    "| 4 | 0.75 | 1e-4 | + |\n",
    "| 5 | 0.25 | 1e-3 | - |\n",
    "| 6 | 0.25 | 1e-4 | - |\n",
    "| 7 | 0.75 | 1e-3 | - |\n",
    "| 8 | 0.75 | 1e-4 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fda900",
   "metadata": {},
   "source": [
    "## Load Our Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe511db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.269370555877686\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "with open('Data\\continuous_full_data_block1.pickle', 'rb') as handle:\n",
    "    #refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1, Hs_block1, alphas_block1, pDs_block1, times_block1, conditions_block1 = pickle.load(handle)\n",
    "    refs_block1, _, _, _, emgs_block1, Ws_block1, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "#with open('Data\\continuous_full_data_block2.pickle', 'rb') as handle:\n",
    "    #refs_block2, poss_block2, dec_vels_block2, int_vel_block2, emgs_block2, Ws_block2, Hs_block2, alphas_block2, pDs_block2, times_block2, conditions_block2 = pickle.load(handle)\n",
    "    #refs_block2, _, _, _, emgs_block2, Ws_block2, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0  \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e867144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emgs_block1[keys[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9136b293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs_block1[keys[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c61fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_length = 14\n",
    "cond0_dict_list = [0]*list_length\n",
    "for idx in range(list_length):\n",
    "    cond0_dict_list[idx] = {'training':emgs_block1[keys[idx]][0,:,:], 'labels':refs_block1[keys[idx]][0,:,:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858cee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data'\n",
    "filename = r'\\cond0_dict_list.p'\n",
    "\n",
    "with open(path+filename, 'wb') as fp:\n",
    "    pickle.dump(cond0_dict_list, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c5ad93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 2, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 conditions, 20770 data points (only 19 unique sets!), xy, channels\n",
    "Ws_block1[keys[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9af127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614,\n",
       "       10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432,\n",
       "       20769])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7812773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder: (2, 64)\n",
      "\n",
      "Total difference between dec0 and dec1: 0.0\n",
      "E.g., as previously shown, the first two decs are the same\n",
      "\n",
      "Total difference between dec0 and dec2: 3.1981579823181594\n"
     ]
    }
   ],
   "source": [
    "dec_cond0_user1_update0 = Ws_block1[keys[0]][0,0,:,:]\n",
    "dec_cond0_user1_update1 = Ws_block1[keys[0]][0,update_ix[1],:,:]\n",
    "dec_cond0_user1_update2 = Ws_block1[keys[0]][0,update_ix[2],:,:]\n",
    "\n",
    "print(f\"Shape of decoder: {dec_cond0_user1_update0.shape}\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec1: {(dec_cond0_user1_update0 - dec_cond0_user1_update1).sum()}\")\n",
    "print(\"E.g., as previously shown, the first two decs are the same\")\n",
    "print()\n",
    "print(f\"Total difference between dec0 and dec2: {(dec_cond0_user1_update0 - dec_cond0_user1_update2).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df453f",
   "metadata": {},
   "source": [
    "# Create Federated Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f85b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python src/emg_fl_main.py --num_users=14, --model=___, --dataset=___, --num_classes=___, --iid=0)\n",
    "\n",
    "def run_fl_sim(data_path,training_data,labels,epochs=10,num_users=14,C=0.1,local_epochs=10,local_batch_sz=10,lr=0.01,SGD_momentum=0.5,optimizer='sgd',iid=0,unequal=0,stopping_rounds=10,verbose=True,seed=1):\n",
    "    # Other possible parameters\n",
    "    #'num_channels'=64,\n",
    "    #'norm'='batch_norm',    \n",
    "    # Figure out what dataset to use... all EMG data?\n",
    "    # Idk how many classes... we are doing regression...\n",
    "    #parser.add_argument('num_classes', type=int, default=10\n",
    "    # Explanation kept for these\n",
    "    # Our application is probably non-IID?\n",
    "    #parser.add_argument('iid', type=int, default=1,help='Default set to IID. Set to 0 for non-IID.')\n",
    "    # Our splits are currently equal but irl they would not be\n",
    "    #parser.add_argument('unequal', type=int, default=0,\n",
    "    #                    help='whether to use unequal data splits for  \\\n",
    "    #                    non-i.i.d setting (use 0 for equal splits)')\n",
    "    \n",
    "    # Probably need to also pass in alphaF/E/D, maybe D_0?\n",
    "    \n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "075c96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different training approaches\n",
    "def train_1gradstep(w, eta, F, D, H, V, learning_batch, alphaF, alphaD):\n",
    "        w = w - eta*gradient_cost_l2(F, D, H, V, learning_batch, alphaF, alphaD)\n",
    "        return w\n",
    "    \n",
    "def train_1scipyminstep(w, eta, F, D, H, V, learning_batch, alphaF, alphaD, D0, display_info):\n",
    "    w = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':1})\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba9acf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_base:\n",
    "    def __init__(self, ID, w, verbose=False):\n",
    "        #self.type # If this isn't even input, does it need to be here? I guess so for the repr function, but it needs to get overwritten/supered\n",
    "        #current_round?\n",
    "        # Client ID number\n",
    "        self.ID = ID\n",
    "        # Linear regression weights AKA the decoder\n",
    "        self.w = w\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"{self.type} model: {self.ID}\\n{self.type} Round: {self.current_round}\\nTraining Method: {self.method}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaddf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class server:\n",
    "    def __init__(self, ID, all_clients, D0, method='FedAvg', C=0.1, current_round=0, lr=0.25, verbose=False):\n",
    "        # Not input\n",
    "        self.type = 'Server'\n",
    "        self.num_avail_clients = 0\n",
    "        self.available_clients_list = [0]*len(all_clients)\n",
    "        self.num_chosen_clients = 0\n",
    "        self.chosen_clients_lst = [0]*len(all_clients)\n",
    "        # Input\n",
    "        self.ID = ID\n",
    "        self.all_clients = all_clients\n",
    "        self.w = D0\n",
    "        self.method = method\n",
    "        self.current_round = current_round\n",
    "        # ML Parameters / Conditions\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return f\"{self.type} model: {self.ID}\\n{self.type} Round: {self.current_round}\\nTraining Method: {self.method}\"\n",
    "        \n",
    "    def vanilla_FL_loop(self):        \n",
    "        # Choose fraction C of available clients\n",
    "        self.set_num_available_clients()\n",
    "        self.choose_clients()\n",
    "        # Send those clients the current global model\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            my_client.set_update(self.w)\n",
    "        # Let those clients train\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            # This really ought to be happening in parallel\n",
    "            my_client.execute_training_loop()\n",
    "        # Recieve local models from those clients\n",
    "        ## Right now I just take lr and w separately within agg_local_weights().  Not ideal set up probably\n",
    "        # AGGREGATION\n",
    "        self.agg_local_weights()  # This func sets self.w, eg the new decoder\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "    \n",
    "    def get_num_available_clients(self):\n",
    "        return len(self.available_clients_list)\n",
    "            \n",
    "    def set_available_clients_list(self):\n",
    "        self.num_avail_clients = 0\n",
    "        self.available_clients_list = [0]*len(self.all_clients)\n",
    "        for idx, my_client in enumerate(self.all_clients):\n",
    "            self.num_avail_clients += my_client.get_availability()\n",
    "            if my_client.get_availability():\n",
    "                self.available_clients_list[idx] = my_client\n",
    "    \n",
    "    def choose_clients(self):\n",
    "        # First reset all clients to be not chosen\n",
    "        for my_client in self.all_clients:\n",
    "            my_client.reset_chosen()\n",
    "        \n",
    "        # Then check what client are available this round\n",
    "        self.set_available_clients_list()\n",
    "        \n",
    "        if self.num_avail_clients > 0:\n",
    "            self.num_chosen_clients = np.ceil(self.num_avail_clients*self.C)\n",
    "            print(f\"Choosing {self.num_chosen_clients} clients for computation\")\n",
    "            self.chosen_clients_lst = random.shuffle(copy.deepcopy(self.available_clients_list))[:self.num_chosen_clients]\n",
    "            \n",
    "            for my_client in self.chosen_clients_lst:\n",
    "                my_client.you_have_been_chosen()\n",
    "        else:\n",
    "            print(f\"ERROR: Number of available clients must be greater than 0: {self.num_avail_clients}\")\n",
    "    \n",
    "    def agg_local_weights(self):\n",
    "        # Update global round number\n",
    "        self.current_round += 1\n",
    "        \n",
    "        # From McMahan 2017 (vanilla FL)\n",
    "        # Aggregate learning rates from each local model\n",
    "        # When aggregating irl it would be better to query each client for weights and lr at the same time\n",
    "        summed_lr = 0\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            summed_lr += my_client.get_learning_rate()\n",
    "        # Aggregate local model weights, weighted by normalized local learning rate\n",
    "        aggr_w = 0\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            aggr_w += (my_client.get_learning_rate()/summed_lr) * my_client.get_weights()\n",
    "        # Loop is complete, new global decoder is self.w\n",
    "        self.w = aggr_w\n",
    "        # ^ Is this just gonna grow to infinity since all the values are positive?\n",
    "        # E.g. it seems like more clients just means bigger dec?\n",
    "        # Still not clear how the global decoder will be able to adapt to different channels for different orientations\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "920b4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class client:\n",
    "    def __init__(self, ID, local_data, method, availability, num_steps=1, delay_scaling=5, random_delays=False, download_delay=1, upload_delay=1, current_round=0, lr=0.25, alphaF=1e-7, alphaD=1e-3, verbose=False):\n",
    "        # NOT INPUT\n",
    "        self.type = 'Client'\n",
    "        self.chosen_status = 0\n",
    "        # INPUT\n",
    "        # Client ID number\n",
    "        self.ID = ID\n",
    "        # Local dataset\n",
    "        self.training_data = local_data['training']\n",
    "        self.labels = local_data['labels']\n",
    "        # Linear regression weights AKA the decoder\n",
    "        self.w = np.zeros((2,64))\n",
    "        # Which training algorithm to use\n",
    "        self.method = method\n",
    "        # Availability for training\n",
    "        self.availability = availability\n",
    "        # Number of gradient steps to take when training (eg amount of local computation)\n",
    "        self.num_steps = num_steps\n",
    "        # Boolean setting whether or not up/download delays should be random or predefined\n",
    "        self.random_delays = random_delays\n",
    "        # Scaling from random [0,1] to number of seconds\n",
    "        self.delay_scaling = delay_scaling\n",
    "        # Set the delay times\n",
    "        if self.random_delays: \n",
    "            self.download_delay = random()*self.delay_scaling\n",
    "            self.upload_delay = random()*self.delay_scaling\n",
    "        else:\n",
    "            self.download_delay = download_delay\n",
    "            self.upload_delay = upload_delay\n",
    "        # Local round number (for asynch FL)\n",
    "        self.current_round = current_round\n",
    "        # ML Parameters / Conditions\n",
    "        self.lr = lr\n",
    "        self.alphaF = alphaF\n",
    "        self.alphaD = alphaD\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        #w, eta, F, D, H, V, learning_batch, alphaF, alphaD, D0, display_info\n",
    "        \n",
    "        # Hard coded attributes\n",
    "        # THESE SHOULD NOT CHANGE, SHARED FOR THE ENTIRE CLASS\n",
    "        num_updates = 19\n",
    "        starting_update = 10\n",
    "        self.local_update = starting_update\n",
    "        # I didn't load this in just copied it in cause it's faster\n",
    "        update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n",
    "        \n",
    "        # Handle train-test split\n",
    "        # Should probably create F, D, H, V, learning_batch, alphaF, alphaD\n",
    "        # Note that these would need to be updated with each update\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return f\"{self.type} model: {self.ID}\\nCurrent Round: {self.current_round}\\nTraining Method: {self.method}\"\n",
    "    \n",
    "    def execute_training_loop(self):\n",
    "        self.set_up_update()\n",
    "        self.train_model()\n",
    "        #self.send_update()  # Should this be contained in the loop or no\n",
    "    \n",
    "    def simulate_delay(self, incoming):\n",
    "        '''\n",
    "        Inputs:\n",
    "            incoming: [0, 1] --> [upload, download]\n",
    "            \n",
    "        Purpose:\n",
    "            Simulate the random delay associated with ___\n",
    "        '''\n",
    "        \n",
    "        if incoming:\n",
    "            time.sleep(self.download_delay+random())\n",
    "        else:\n",
    "            time.sleep(self.upload_delay+random())\n",
    "            \n",
    "    def set_up_update(self):\n",
    "        lower_bound = update_ix[self.current_round-1]\n",
    "        upper_bound = update_ix[self.current_round]\n",
    "        self.learning_batch = upper_bound - lower_bound\n",
    "        # FIX THIS BASED NO NB200\n",
    "        s = np.transpose(self.emg_dataset[lower_bound:upper_bound,:])\n",
    "        p_intended = np.transpose(self.cued_target_position[lower_bound:upper_bound,:])\n",
    "        v_intended, p_constrained = output_new_decoder(s, self.w, p_intended)\n",
    "        \n",
    "        self.F = s[:,:-1] # note: truncate F for estimate_decoder\n",
    "        self.V = v_intended\n",
    "        self.D = self.w\n",
    "        self.H = np.zeros((2,2))\n",
    "        # Do I need to return anything since I'm already setting them with self?\n",
    "    \n",
    "    def train_model(self):\n",
    "        for i in range(self.num_steps):\n",
    "            if self.method==1:\n",
    "                self.w = train_1gradstep(self.w, self.eta, self.F, self.D, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD)\n",
    "            elif self.method==2:\n",
    "                D0 = np.random.rand(2,64)\n",
    "                self.w = train_1scipyminstep(self.w, self.eta, self.F, self.D, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, D0, self.verbose)\n",
    "            else:\n",
    "                print(\"Unrecognized method\")\n",
    "            \n",
    "    def test_model(self):\n",
    "        # Execute a training loop but don't update D?\n",
    "        # Then report accuracy in terms of predicted vs ground truth\n",
    "        pass\n",
    "    \n",
    "    def set_update(self, new_model):\n",
    "        #simulate_delay(incoming=True)\n",
    "        self.global_model = new_model\n",
    "        # Update the local round number to reflect the new data\n",
    "        # I don't think it matters if the update happens on up/download, as long as everyone is consistent\n",
    "        self.current_round += 1\n",
    "    \n",
    "    def get_update(self):\n",
    "        #simulate_delay(incoming=False)\n",
    "        return self.local_model\n",
    "    \n",
    "    def you_have_been_chosen(self):\n",
    "        self.chosen_status = 1\n",
    "    \n",
    "    def set_availability(self, input_avail):\n",
    "        self.availability = input_avail\n",
    "    \n",
    "    def get_availability(self):\n",
    "        return self.availability\n",
    "    \n",
    "    def get_chosen_status(self):\n",
    "        return self.chosen_status\n",
    "    \n",
    "    def reset_chosen(self):\n",
    "        self.chosen_status = 0\n",
    "        \n",
    "    def get_learning_rate(self):\n",
    "        return self.lr\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    #####################################################################################################################\n",
    "    #####################################################################################################################\n",
    "    \n",
    "    def do_train_test_split(self):\n",
    "        pass\n",
    "    \n",
    "    def simulate_data_streams(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b2b6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "num_users=14\n",
    "C=0.1\n",
    "local_epochs=10\n",
    "local_batch_sz=10\n",
    "lr=0.01\n",
    "SGD_momentum=0.5\n",
    "optimizer='sgd'\n",
    "iid=0\n",
    "unequal=0\n",
    "stopping_rounds=10\n",
    "verbose=True\n",
    "seed=1\n",
    "\n",
    "with open(path+filename, 'rb') as fp:\n",
    "    cond0_dict_list = pickle.load(fp)\n",
    "\n",
    "# There has to be a better way to do this lol\n",
    "user0 = client(0, cond0_dict_list[0], 2, 1, delay_scaling=0)\n",
    "user1 = client(1, cond0_dict_list[1], 2, 1, delay_scaling=0)\n",
    "user2 = client(2, cond0_dict_list[2], 2, 1, delay_scaling=0)\n",
    "user_database = [user0, user1, user2]\n",
    "#user_database = [user0, user1, user2, user3, user4, user5, user6, user7, user8, user9, user10, user11, user12, user13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3df819e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_0 = np.random.rand(2,64)\n",
    "#ID, all_clients, D0\n",
    "global_model = server(0, user_database, D_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2429ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97be1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf56bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import copy\n",
    "#import time\n",
    "#import pickle\n",
    "#import numpy as np\n",
    "#from tqdm import tqdm\n",
    "#from ARJ_update import LocalUpdate, test_inference\n",
    "#from ARJ_my_models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "#from ARJ_utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# define paths\n",
    "#path_project = os.path.abspath('..')\n",
    "#logger = SummaryWriter('../logs')\n",
    "\n",
    "# load dataset and user groups\n",
    "#train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "# Change the above to use these:\n",
    "#'data_path','training_data','labels'\n",
    "\n",
    "# Set the model to train and send it to device.\n",
    "# KAI: I think this actually sends it to the GPU, not a \"client\" in the FL sense\n",
    "#global_model.to(device)\n",
    "#global_model.train()\n",
    "D_0 = np.random.rand(2,64)  #Ws_block1[keys[0]][0,0,:,:]\n",
    "#global_model_obj = global_model()\n",
    "print(global_model_obj)\n",
    "\n",
    "# copy weights\n",
    "# In this case, our \"model\" [linear regression] is represented by the matrix D\n",
    "# And D is also just the weights\n",
    "global_weights = global_model_obj.get_weights()\n",
    "\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "\n",
    "# Recall, I am not working with epochs\n",
    "# I don't think? Maybe replace epochs with iterations\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "    global_model.train()\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger)\n",
    "        w, loss = local_model.update_weights(\n",
    "            model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "    # update global weights\n",
    "    global_weights = average_weights(local_weights)\n",
    "\n",
    "    # update global weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    for c in range(args.num_users):\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger)\n",
    "        acc, loss = local_model.inference(model=global_model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n",
    "# Test inference after completion of training\n",
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "# Saving the objects train_loss and train_accuracy:\n",
    "file_name = '../save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
    "    format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
    "           args.local_ep, args.local_bs)\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump([train_loss, train_accuracy], f)\n",
    "\n",
    "print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "\n",
    "# PLOTTING (optional)\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "# plt.figure()\n",
    "# plt.title('Training Loss vs Communication rounds')\n",
    "# plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "# plt.ylabel('Training loss')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))\n",
    "#\n",
    "# # Plot Average Accuracy vs Communication rounds\n",
    "# plt.figure()\n",
    "# plt.title('Average Accuracy vs Communication rounds')\n",
    "# plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "# plt.ylabel('Average Accuracy')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33f420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc684ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
