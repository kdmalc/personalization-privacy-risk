{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d39628",
   "metadata": {},
   "source": [
    "__Purpose:__ Introduce Federated Learning, specifically by implementing FedAveraging on our dataset and moving on to more advanced methods.  Start by modifying the Simulations code, worry about (a)synchronicity later.\n",
    "<br>\n",
    "1. The dec matrix is the weights to pass back an forth (I think), although it comes out of SmoothBatch first\n",
    "1. We are assuming we can test on the second half (updates 10-19ish) since learning should be complete by then!\n",
    "1. Scipy.optimize.minimize() runs many iters to fully minimize its cost function.  You can change it to run as many iters as you'd like, although AFAIK you won't know how many it takes to converge.  But this is still a good set up for FL.\n",
    "1. Hmm minimize() is doing BFGS rn and not SGD... not sure if that matters really.  Could probably implement SGD on my own or find it.  BFGS is 2nd order but we don't have a lot of parameters, I don't think.  Plus we can (already have?) solved analytically for the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f09a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize, least_squares\n",
    "import copy\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f58c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_params import *\n",
    "from simulations import *\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9450bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data'\n",
    "cond0_filename = r'\\cond0_dict_list.p'\n",
    "all_decs_init_filename = r'\\all_decs_init.p'\n",
    "id2color = {0:'lightcoral', 1:'maroon', 2:'chocolate', 3:'darkorange', 4:'gold', 5:'olive', 6:'olivedrab', \n",
    "            7:'lawngreen', 8:'aquamarine', 9:'deepskyblue', 10:'steelblue', 11:'violet', 12:'darkorchid', 13:'deeppink'}\n",
    "implemented_client_training_methods = ['EtaGradStep', 'EtaScipyMinStep', 'FullScipyMinStep']\n",
    "num_participants = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634c84f",
   "metadata": {},
   "source": [
    "# Reminder of Conditions Order\n",
    "\n",
    "NOTE: \n",
    "\n",
    "* **CONDITIONS** = array(['D_1', 'D_2', 'D_5', 'D_6', 'D_3', 'D_4', 'D_7','D_8']\n",
    "* **LEARNING RATES:** alpha = 0.25 and 0.75; alpha = 0.25 for D1, D2, D5, D6; alpha = 0.75 for D3, D4, D7, D8\n",
    "* **SMOOTHBATCH:** W_next = alpha*W_old + ((1 - alpha) * W_calc)\n",
    "\n",
    "* **DECODER INIT:** pos for D1 - D4, neg for D5 - D8\n",
    "\n",
    "* **PENALTY TERM:** $\\lambda_E$ = 1e-6 for all, $\\lambda_F$ = 1e-7 for all, $\\lambda_D$ = 1e-3 for 1, 3, 5, 7 and 1e-4 for 2, 4, 6, 8 \n",
    "\n",
    "\n",
    "| DECODER | ALPHA | PENALTY | DEC INIT |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0.25 | 1e-3 | + |\n",
    "| 2 | 0.25 | 1e-4 | + |\n",
    "| 3 | 0.75 | 1e-3 | + |\n",
    "| 4 | 0.75 | 1e-4 | + |\n",
    "| 5 | 0.25 | 1e-3 | - |\n",
    "| 6 | 0.25 | 1e-4 | - |\n",
    "| 7 | 0.75 | 1e-3 | - |\n",
    "| 8 | 0.75 | 1e-4 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fda900",
   "metadata": {},
   "source": [
    "## Load Our Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe511db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "\n",
    "#with open('Data\\continuous_full_data_block1.pickle', 'rb') as handle:\n",
    "#    #refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1, Hs_block1, alphas_block1, pDs_block1, times_block1, conditions_block1 = pickle.load(handle)\n",
    "#    _, _, _, _, _, Ws_block1, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "#with open('Data\\continuous_full_data_block2.pickle', 'rb') as handle:\n",
    "#    #refs_block2, poss_block2, dec_vels_block2, int_vel_block2, emgs_block2, Ws_block2, Hs_block2, alphas_block2, pDs_block2, times_block2, conditions_block2 = pickle.load(handle)\n",
    "#    #refs_block2, _, _, _, emgs_block2, Ws_block2, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "#t1 = time.time()\n",
    "#total = t1-t0  \n",
    "#print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c61fc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cond0_dict_list = [0]*num_participants\n",
    "for idx in range(num_participants):\n",
    "    cond0_dict_list[idx] = {'training':emgs_block1[keys[idx]][0,:,:], 'labels':refs_block1[keys[idx]][0,:,:]}\n",
    "\n",
    "with open(path+cond0_filename, 'wb') as fp:\n",
    "    pickle.dump(cond0_dict_list, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "init_decoders = [Ws_block1[keys[i]][:, 0, :, :] for i in range(num_participants)]\n",
    "with open(path+all_decs_init_filename, 'wb') as fp:\n",
    "    pickle.dump(init_decoders, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "'''\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df453f",
   "metadata": {},
   "source": [
    "# Create Federated Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57f85b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python src/emg_fl_main.py --num_users=14, --model=___, --dataset=___, --num_classes=___, --iid=0)\n",
    "\n",
    "#def run_fl_sim(data_path,training_data,labels,epochs=10,num_users=14,C=0.1,local_epochs=10,local_batch_sz=10,lr=0.01,SGD_momentum=0.5,optimizer='sgd',iid=0,unequal=0,stopping_rounds=10,verbose=True,seed=1):\n",
    "    # Other possible parameters\n",
    "    #'num_channels'=64,\n",
    "    #'norm'='batch_norm',    \n",
    "    # Figure out what dataset to use... all EMG data?\n",
    "    # Idk how many classes... we are doing regression...\n",
    "    #parser.add_argument('num_classes', type=int, default=10\n",
    "    # Explanation kept for these\n",
    "    # Our application is probably non-IID?\n",
    "    #parser.add_argument('iid', type=int, default=1,help='Default set to IID. Set to 0 for non-IID.')\n",
    "    # Our splits are currently equal but irl they would not be\n",
    "    #parser.add_argument('unequal', type=int, default=0,\n",
    "    #                    help='whether to use unequal data splits for  \\\n",
    "    #                    non-i.i.d setting (use 0 for equal splits)')\n",
    "    \n",
    "    # Probably need to also pass in alphaF/E/D, maybe D_0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "075c96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different training approaches\n",
    "def train_eta_gradstep(w, eta, F, D, H, V, learning_batch, alphaF, alphaD):\n",
    "    grad_cost = np.reshape(gradient_cost_l2(F, D, H, V, learning_batch, alphaF, alphaD),(2, 64))\n",
    "    w_new = w - eta*grad_cost\n",
    "    return w_new\n",
    "    \n",
    "def train_eta_scipyminstep(w, eta, F, D, H, V, learning_batch, alphaF, alphaD, D0, display_info, full=False):\n",
    "    if full:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n",
    "    else:\n",
    "        out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n",
    "    w_new = np.reshape(out.x,(2, 64))\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba9acf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using this yet... not worth setting up super just for 3 attrs\n",
    "class model_base:\n",
    "    def __init__(self, ID, w, method, current_round, lr, smoothbatch=False, verbose=False):\n",
    "        #self.type # If this isn't even input, does it need to be here? I guess so for the repr function, but it needs to get overwritten/supered\n",
    "        # Client ID number\n",
    "        self.ID = ID\n",
    "        # Linear regression weights AKA the decoder\n",
    "        self.w = w\n",
    "        self.w_prev = w\n",
    "        self.local_error_log = []\n",
    "        self.global_error_log = []\n",
    "        self.method = method\n",
    "        self.current_round = current_round\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.smoothbatch = smoothbatch\n",
    "        #self.eta = eta\n",
    "        # Hard coded attributes\n",
    "        # THESE SHOULD NOT CHANGE, SHARED FOR THE ENTIRE CLASS\n",
    "        num_updates = 19\n",
    "        cphs_starting_update = 10\n",
    "        update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"{self.type}{self.ID}\"\n",
    "    \n",
    "    def display_info(self): \n",
    "        return f\"{self.type} model: {self.ID}\\nCurrent Round: {self.current_round}\\nTraining Method: {self.method}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aaddf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class server:\n",
    "    def __init__(self, ID, all_clients, D0, method='FedAvg', smoothbatch=False, C=0.1, current_round=0, lr=0.25, advance_each_iter=False, verbose=False):\n",
    "        # Not input\n",
    "        self.type = 'Server'\n",
    "        self.num_avail_clients = 0\n",
    "        self.available_clients_list = [0]*len(all_clients)\n",
    "        self.num_chosen_clients = 0\n",
    "        self.chosen_clients_lst = [0]*len(all_clients)\n",
    "        self.local_error_log = []\n",
    "        self.global_error_log = []\n",
    "        # Input\n",
    "        self.ID = ID\n",
    "        self.all_clients = all_clients\n",
    "        self.w = D0\n",
    "        self.w_prev = D0\n",
    "        self.method = method\n",
    "        self.current_round = current_round\n",
    "        # ML Parameters / Conditions\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.smoothbatch = smoothbatch\n",
    "        # FL Specific Params\n",
    "        self.C = C\n",
    "        self.advance_each_iter = advance_each_iter\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return f\"{self.type} model: {self.ID}\\n{self.type} Round: {self.current_round}\\nTraining Method: {self.method}\"\n",
    "        \n",
    "    def train_client_and_log(self, client_set, set_client_global_method=False):\n",
    "        current_local_lst = []\n",
    "        current_global_lst = []\n",
    "        for my_client in client_set:\n",
    "            if set_client_global_method:\n",
    "                my_client.global_method = self.method\n",
    "            my_client.execute_training_loop()\n",
    "            current_local_lst.append((my_client.ID, my_client.eval_model(which='local')))\n",
    "            if not set_client_global_method:  # Using this as a proxy for NoFL, where we cannot run the global model\n",
    "                current_global_lst.append((my_client.ID, my_client.eval_model(which='global')))\n",
    "            if self.advance_each_iter:\n",
    "                my_client.advance_each_iter = 1\n",
    "        self.local_error_log.append(current_local_lst)\n",
    "        if not set_client_global_method: #Same reason as above\n",
    "            self.global_error_log.append(current_global_lst)\n",
    "        \n",
    "    def execute_FL_loop(self):\n",
    "        # Update global round number\n",
    "        self.current_round += 1\n",
    "        \n",
    "        if self.method=='FedAvg':\n",
    "            # Choose fraction C of available clients\n",
    "            self.set_available_clients_list()\n",
    "            self.choose_clients()\n",
    "            # Send those clients the current global model\n",
    "            for my_client in self.chosen_clients_lst:\n",
    "                my_client.pull_update(self.w)\n",
    "            # Let those clients train (this autoselects the chosen_client_lst to use)\n",
    "            self.train_client_and_log(client_set=self.chosen_clients_lst)\n",
    "            # AGGREGATION\n",
    "            self.w_prev = copy.copy(self.w)\n",
    "            self.agg_local_weights()  # This func sets self.w, eg the new decoder\n",
    "            # Do SmoothBatch\n",
    "            if self.smoothbatch:\n",
    "                #W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "                self.w = self.lr*self.w + ((1 - self.lr)*self.w_prev)\n",
    "        elif self.method=='NoFL':\n",
    "            self.train_client_and_log(client_set=self.all_clients, set_client_global_method=True)\n",
    "        else:\n",
    "            print('Method not currently supported')\n",
    "            print('Please reset method to FedAvg')\n",
    "    \n",
    "    def get_num_available_clients(self):\n",
    "        return len(self.available_clients_list)\n",
    "            \n",
    "    def set_available_clients_list(self):\n",
    "        self.num_avail_clients = 0\n",
    "        self.available_clients_list = [0]*len(self.all_clients)\n",
    "        for idx, my_client in enumerate(self.all_clients):\n",
    "            if my_client.availability:\n",
    "                self.available_clients_list[idx] = my_client\n",
    "                self.num_avail_clients += 1\n",
    "    \n",
    "    def choose_clients(self):\n",
    "        # First reset all clients to be not chosen\n",
    "        for my_client in self.all_clients:\n",
    "            my_client.reset_chosen()\n",
    "        # Then check what client are available this round\n",
    "        self.set_available_clients_list()\n",
    "        # Now choose frac C clients from the resulting available clients\n",
    "        if self.num_avail_clients > 0:\n",
    "            self.num_chosen_clients = int(np.ceil(self.num_avail_clients*self.C))\n",
    "            # Right now it chooses 2 at random: 14*.1=1.4 --> 2\n",
    "            self.chosen_clients_lst = random.sample(self.available_clients_list, len(self.available_clients_list))[:self.num_chosen_clients]\n",
    "            for my_client in self.chosen_clients_lst:\n",
    "                my_client.you_have_been_chosen()\n",
    "        else:\n",
    "            print(f\"ERROR: Number of available clients must be greater than 0: {self.num_avail_clients}\")\n",
    "    \n",
    "    def agg_local_weights(self):\n",
    "        # From McMahan 2017 (vanilla FL)\n",
    "        # I think they actually use the number of datapoints, not the learning rates...\n",
    "        # Other paper was using dynamic learning rate adjustment step, not this paper\n",
    "        # Aggregate learning rates from each local model\n",
    "        # When aggregating irl it would be better to query each client for weights and lr at the same time\n",
    "        #summed_lr = 0\n",
    "        #for my_client in self.chosen_clients_lst:\n",
    "        #    summed_lr += my_client.lr\n",
    "        summed_num_datapoints = 0\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            summed_num_datapoints += my_client.learning_batch\n",
    "        # Aggregate local model weights, weighted by normalized local learning rate\n",
    "        aggr_w = 0\n",
    "        for my_client in self.chosen_clients_lst:\n",
    "            aggr_w += (my_client.learning_batch/summed_num_datapoints) * my_client.w\n",
    "        # Loop is complete, new global decoder is self.w\n",
    "        self.w = aggr_w\n",
    "        # ^ Is this just gonna grow to infinity since all the values are positive?\n",
    "        # E.g. it seems like more clients just means bigger dec?\n",
    "        # Still not clear how the global decoder will be able to adapt to different channels for different orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "920b4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class client:\n",
    "    def __init__(self, ID, local_data, method, D0, availability=1, full_data=False, global_method='FedAvg', streaming=False, advance_each_iter=False, eta=1, num_steps=1, delay_scaling=5, random_delays=False, download_delay=1, upload_delay=1, current_round=0, local_round_threshold=250, lr=0.25, alphaF=1e-7, alphaD=1e-3, verbose=False):\n",
    "        # NOT INPUT\n",
    "        self.type = 'Client'\n",
    "        self.chosen_status = 0\n",
    "        self.local_error_log = []\n",
    "        self.global_error_log = []\n",
    "        # Sentinel Values\n",
    "        self.F = None\n",
    "        self.V = None\n",
    "        self.D = None\n",
    "        self.H = np.zeros((2,2))\n",
    "        self.learning_batch = None\n",
    "        self.eta = eta\n",
    "        # Hard coded attributes\n",
    "        # THESE SHOULD NOT CHANGE, SHARED FOR THE ENTIRE CLASS\n",
    "        num_updates = 19\n",
    "        cphs_starting_update = 10\n",
    "        update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n",
    "        \n",
    "        # INPUT\n",
    "        # Client ID number\n",
    "        self.ID = ID\n",
    "        # Local dataset\n",
    "        self.training_data = local_data['training']\n",
    "        self.labels = local_data['labels']\n",
    "        # Linear regression weights AKA the decoder\n",
    "        self.w = D0  # Use the supplied decoder from CPHS\n",
    "        #^If redoing natively would just need to choose init decoder ahead of time\n",
    "        self.w_prev = None\n",
    "        self.global_model = None\n",
    "        # Which training algorithm to use\n",
    "        self.method = method\n",
    "        self.global_method = global_method\n",
    "        if self.global_method=='NoFL':\n",
    "            starting_update = 0\n",
    "        else:\n",
    "            starting_update = cphs_starting_update\n",
    "        self.global_method = None\n",
    "        # Availability for training\n",
    "        self.availability = availability\n",
    "        # Toggle streaming aspect of data collection --> eg each round, use a new update's data or not\n",
    "        self.full_data = full_data  #e.g. just ignore updates and use all the data\n",
    "        self.streaming = streaming\n",
    "        self.advance_each_iter = advance_each_iter\n",
    "        # Number of gradient steps to take when training (eg amount of local computation)\n",
    "        self.num_steps = num_steps\n",
    "        # Boolean setting whether or not up/download delays should be random or predefined\n",
    "        self.random_delays = random_delays\n",
    "        # Scaling from random [0,1] to number of seconds\n",
    "        self.delay_scaling = delay_scaling\n",
    "        # Set the delay times\n",
    "        if self.random_delays: \n",
    "            self.download_delay = random.random()*self.delay_scaling\n",
    "            self.upload_delay = random.random()*self.delay_scaling\n",
    "        else:\n",
    "            self.download_delay = download_delay\n",
    "            self.upload_delay = upload_delay\n",
    "        # Local round number (for asynch FL)\n",
    "        self.current_round = current_round\n",
    "        self.current_update = starting_update\n",
    "        self.local_round_threshold = local_round_threshold\n",
    "        # ML Parameters / Conditions\n",
    "        self.lr = lr\n",
    "        self.alphaF = alphaF\n",
    "        self.alphaD = alphaD\n",
    "        self.verbose = verbose\n",
    "            \n",
    "    def __repr__(self): \n",
    "        return f\"{self.type}{self.ID}\"\n",
    "    \n",
    "    def display_info(self): \n",
    "        return f\"{self.type} model: {self.ID}\\nCurrent Round: {self.current_round}\\nTraining Method: {self.method}\"\n",
    "    \n",
    "    def execute_training_loop(self):\n",
    "        #self.global_round\n",
    "        self.simulate_data_stream()\n",
    "        self.train_model()\n",
    "        self.eval_model(which='local')\n",
    "        if self.global_method!=\"NoFL\":\n",
    "            self.eval_model(which='global')\n",
    "        \n",
    "    \n",
    "    def simulate_delay(self, incoming):\n",
    "        if incoming:\n",
    "            time.sleep(self.download_delay+random.random())\n",
    "        else:\n",
    "            time.sleep(self.upload_delay+random.random())\n",
    "            \n",
    "    def simulate_data_stream(self):\n",
    "        self.current_round += 1\n",
    "        if self.full_data:\n",
    "            #print(\"FULL\")\n",
    "            lower_bound = update_ix[0]\n",
    "            upper_bound = update_ix[-1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "        elif self.streaming:\n",
    "            #print(\"STREAMING\")\n",
    "            if self.current_round > self.local_round_threshold:\n",
    "                self.local_round_threshold += self.current_round\n",
    "                self.current_update += 1\n",
    "            lower_bound = update_ix[self.current_update]\n",
    "            upper_bound = update_ix[self.current_update+1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "        # What case is this...\n",
    "        elif self.advance_each_iter:\n",
    "            #print(\"ADVANCE\")\n",
    "            self.current_update += 1\n",
    "            lower_bound = update_ix[self.current_update]\n",
    "            upper_bound = update_ix[self.current_update+1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "        else:\n",
    "            raise ValueError('This data streaming functionality is not supported')\n",
    "        ####################################################################################################\n",
    "        # FIX THIS BASED ON NB200\n",
    "        s = np.transpose(self.training_data[lower_bound:upper_bound,:])\n",
    "        v_actual = self.w@s\n",
    "        dt=1/60\n",
    "        p_actual = np.sum(v_actual, axis=1)*dt  # dt=1/60\n",
    "        p_actual = np.reshape(p_actual, (p_actual.shape[0], 1))\n",
    "        p_reference = np.transpose(self.labels[lower_bound:upper_bound,:])\n",
    "        \n",
    "        self.F = s[:,:-1] # note: truncate F for estimate_decoder\n",
    "        self.V = (p_reference - p_actual)*dt\n",
    "        self.D = self.w\n",
    "        self.H = np.zeros((2,2))\n",
    "        ####################################################################################################\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Set the w_prev equal to the current w:\n",
    "        self.w_prev = copy.copy(self.w)\n",
    "        if self.global_method!=\"NoFL\":\n",
    "            # Overwrite local model with the new global model\n",
    "            self.w = self.global_model\n",
    "        # Should D0 get set to self.w_prev instead?\n",
    "        D_0 = np.random.rand(2,64)\n",
    "        # Pass in self.w twice for now since D=self.w... could probably consolidate but make sure it works first\n",
    "        for i in range(self.num_steps):\n",
    "            if self.method=='EtaGradStep':\n",
    "                self.w = train_eta_gradstep(self.w, self.eta, self.F, self.w, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD)\n",
    "            elif self.method=='EtaScipyMinStep':\n",
    "                self.w = train_eta_scipyminstep(self.w, self.eta, self.F, self.w, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, D_0, self.verbose)\n",
    "            elif self.method=='FullScipyMinStep':\n",
    "                self.w = train_eta_scipyminstep(self.w, self.eta, self.F, self.w, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, D_0, self.verbose, full=True)\n",
    "            else:\n",
    "                print(\"Unrecognized method\")\n",
    "        # Do SmoothBatch\n",
    "        #W_new = alpha*D[-1] + ((1 - alpha) * W_hat)\n",
    "        self.w = self.lr*self.w + ((1 - self.lr) * self.w_prev)\n",
    "    \n",
    "    def pull_update(self, new_model):\n",
    "        #simulate_delay(incoming=True)\n",
    "        self.global_model = new_model\n",
    "        # Update the local round number to reflect the new data\n",
    "        # I don't think it matters if the update happens on up/download, as long as everyone is consistent\n",
    "        self.current_round += 1\n",
    "    \n",
    "    def you_have_been_chosen(self):\n",
    "        self.chosen_status = 1\n",
    "    \n",
    "    def reset_chosen(self):\n",
    "        self.chosen_status = 0\n",
    "        \n",
    "    def eval_model(self, which):\n",
    "        if which=='local':\n",
    "            my_dec = self.w\n",
    "            my_error_log = self.local_error_log\n",
    "        elif which=='global':\n",
    "            #print(\"You ran the global model...\")\n",
    "            my_dec = self.global_model\n",
    "            my_error_log = self.global_error_log\n",
    "        else:\n",
    "            print(\"Please set <which> to either local or global\")\n",
    "        temp = np.ceil(cost_l2(self.F, my_dec, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD))\n",
    "        try:\n",
    "            out = int(temp)\n",
    "        except (OverflowError,ValueError):  # inf, nan\n",
    "            out = 1_000_000_000_000\n",
    "        # Ensure that this actually appends to the refernced list and not the temp list\n",
    "        my_error_log.append(out)\n",
    "        return out\n",
    "        \n",
    "    def test_inference(self):\n",
    "        # Essentially, choose a random(?) section of data and compare how dec performs\n",
    "        # Is this really any different from the eval funcs?\n",
    "        print(\"Testing Functionality Not Written Yet\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b2b6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+cond0_filename, 'rb') as fp:\n",
    "    cond0_dict_list = pickle.load(fp)\n",
    "    \n",
    "with open(path+all_decs_init_filename, 'rb') as fp:\n",
    "    init_decoders = pickle.load(fp)\n",
    "cond0_init_decs = [dec[0, :, :] for dec in init_decoders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "582dfd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_0 = np.random.rand(2,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5f77b",
   "metadata": {},
   "source": [
    "Check streaming condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0c17153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 10)\n",
      "(2, 0, 10)\n",
      "(3, 0, 10)\n",
      "\n",
      "Global Error Log\n",
      "[[(1, 1663649), (5, 451690)], [(12, 9697), (1, 79682)], [(11, 46804), (6, 49209)]]\n",
      "\n",
      "Local Error Log\n",
      "[[(1, 34134), (5, 19614)], [(12, 8172), (1, 152301)], [(11, 26586), (6, 21093)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_13004\\1720557157.py:11: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n"
     ]
    }
   ],
   "source": [
    "# ID, local_data, method, streaming=False, eta=1, num_steps=1, delay_scaling=5,\n",
    "user_c0_etascipy_streaming = [client(i, cond0_dict_list[i], 'EtaScipyMinStep', D0=cond0_init_decs[i], streaming=True, delay_scaling=0) for i in range(14)]\n",
    "#ID, all_clients, D0\n",
    "global_model = server(0, user_c0_etascipy_streaming, D_0)\n",
    "#global_model_no_fl = server(method='NoFL', advance_each_iter=True)\n",
    "\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "print()\n",
    "print(\"Global Error Log\")\n",
    "print(global_model.global_error_log)\n",
    "print()\n",
    "print(\"Local Error Log\")\n",
    "print(global_model.local_error_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6807e",
   "metadata": {},
   "source": [
    "Check full data condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0c9e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_13004\\1720557157.py:11: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 10)\n",
      "(2, 0, 10)\n",
      "(3, 0, 10)\n",
      "\n",
      "Global Error Log\n",
      "[[(11, 665685), (5, 583439)], [(4, 14163), (6, 41514)], [(6, 979584), (11, 706538)]]\n",
      "\n",
      "Local Error Log\n",
      "[[(11, 28666), (5, 25055)], [(4, 18552), (6, 35735)], [(6, 1169155), (11, 876258)]]\n"
     ]
    }
   ],
   "source": [
    "# ID, local_data, method, streaming=False, eta=1, num_steps=1, delay_scaling=5,\n",
    "user_c0_etascipy_full = [client(i, cond0_dict_list[i], 'EtaScipyMinStep', D0=cond0_init_decs[i], full_data=True, delay_scaling=0) for i in range(14)]\n",
    "#ID, all_clients, D0\n",
    "global_model = server(0, user_c0_etascipy_full, D_0)\n",
    "#global_model_no_fl = server(method='NoFL', advance_each_iter=True)\n",
    "\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "print()\n",
    "print(\"Global Error Log\")\n",
    "print(global_model.global_error_log)\n",
    "print()\n",
    "print(\"Local Error Log\")\n",
    "print(global_model.local_error_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6595f73",
   "metadata": {},
   "source": [
    "Check advance_each_iter condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c25fbebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 11)\n",
      "(2, 2, 11)\n",
      "(3, 4, 12)\n",
      "\n",
      "Global Error Log\n",
      "[[(10, 113159), (0, 891157)], [(3, 199912), (1, 118128)], [(0, 35380), (9, 10337)]]\n",
      "\n",
      "Local Error Log\n",
      "[[(10, 3835), (0, 24578)], [(3, 285889), (1, 102466)], [(0, 60092), (9, 9182)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_13004\\1720557157.py:11: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info, 'maxiter':eta})\n"
     ]
    }
   ],
   "source": [
    "# ID, local_data, method, streaming=False, eta=1, num_steps=1, delay_scaling=5,\n",
    "user_c0_etascipy_advance = [client(i, cond0_dict_list[i], 'EtaScipyMinStep', D0=cond0_init_decs[i], advance_each_iter=True, delay_scaling=0) for i in range(14)]\n",
    "#ID, all_clients, D0\n",
    "global_model = server(0, user_c0_etascipy_advance, D_0)\n",
    "#global_model_no_fl = server(method='NoFL', advance_each_iter=True)\n",
    "\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "global_model.execute_FL_loop()\n",
    "print((global_model.current_round, global_model.all_clients[0].current_round, global_model.all_clients[0].current_update))\n",
    "print()\n",
    "print(\"Global Error Log\")\n",
    "print(global_model.global_error_log)\n",
    "print()\n",
    "print(\"Local Error Log\")\n",
    "print(global_model.local_error_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f96dcd",
   "metadata": {},
   "source": [
    "## Double Checking Running the No-FL Case\n",
    "> E.g. we should see some kind of convergence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e27495c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID, local_data, method, full_data=True, streaming=False\n",
    "user_c0_fullscipy = [client(i, cond0_dict_list[i], 'FullScipyMinStep', global_method='NoFL', D0=cond0_init_decs[i], full_data=False, advance_each_iter=True, delay_scaling=0) for i in range(14)]\n",
    "\n",
    "# ID, all_clients, D0, method='FedAvg', advance_each_iter=False\n",
    "global_model_no_fl = server(-1, user_c0_fullscipy, D_0, method='NoFL', advance_each_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f5946cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round\n",
      "0\n",
      "\n",
      "Local Round\n",
      "0\n",
      "0\n",
      "\n",
      "Use full data? False\n",
      "Try streaming? False\n",
      "[Else condition] What is advance set to (True means advance to next round)? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)\n",
    "print()\n",
    "print(f\"Use full data? {user_c0_fullscipy[0].full_data}\")\n",
    "print(f\"Try streaming? {user_c0_fullscipy[0].streaming}\")\n",
    "print(f\"[Else condition] What is advance set to (True means advance to next round)? {user_c0_fullscipy[0].advance_each_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8b4695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_13004\\1720557157.py:9: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round\n",
      "1\n",
      "\n",
      "Local Round\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "global_model_no_fl.execute_FL_loop()\n",
    "\n",
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "661de729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_13004\\1720557157.py:9: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round\n",
      "2\n",
      "\n",
      "Local Round\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "global_model_no_fl.execute_FL_loop()\n",
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cb35607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2202e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\AppData\\Local\\Temp\\ipykernel_13004\\1720557157.py:9: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  out = minimize(lambda D: cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), D0, method='BFGS', jac=lambda D: gradient_cost_l2(F,D,H,V,learning_batch,alphaF,alphaD), options={'disp': display_info})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2\n",
      "Iter 4\n",
      "Iter 6\n",
      "Iter 8\n",
      "Iter 10\n",
      "Iter 12\n",
      "Iter 14\n",
      "Complete\n",
      "----------------------------------\n",
      "Global Round\n",
      "17\n",
      "\n",
      "Local Round\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# We are at update 2/19.  Therefore 19-2=17-1=16\n",
    "# Why does it break at 16?  Local round/update=18... 18+1 hmm\n",
    "num_updates_left = 15\n",
    "for i in range(num_updates_left):\n",
    "    if i%np.ceil(num_updates_left*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model_no_fl.execute_FL_loop()\n",
    "print(\"Complete\")\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "print(\"Global Round\")\n",
    "print(global_model_no_fl.current_round)\n",
    "print()\n",
    "print(\"Local Round\")\n",
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fcceb08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(user_c0_fullscipy[0].current_round)\n",
    "print(user_c0_fullscipy[0].current_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee865a",
   "metadata": {},
   "source": [
    "## 1 Scipy Step, 1000 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85218300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID, local_data, method, streaming=False, eta=1, num_steps=1, delay_scaling=5,\n",
    "user_c0_1ScipyStep = [client(i, cond0_dict_list[i], 'EtaScipyMinStep', D0=cond0_init_decs[i], delay_scaling=0) for i in range(14)]\n",
    "\n",
    "#ID, all_clients, D0\n",
    "global_model1 = server(1, user_c0_1ScipyStep, D_0)\n",
    "\n",
    "big_loop_iters = 1000\n",
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model1.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd69857",
   "metadata": {},
   "source": [
    "Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b69c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.local_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Local Cost Eval As a Function of Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.local_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Local Cost Eval As a Function of Iteration')\n",
    "plt.ylim(0, 1.1*10**7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2cd61",
   "metadata": {},
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.global_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Global Cost Eval As a Function of Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    for (client_id, error) in global_model1.global_error_log[i]:\n",
    "        plt.scatter(i, error, color=id2color[client_id])\n",
    "        \n",
    "plt.ylabel('Cost L2')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.title('Global Cost Eval As a Function of Iteration')\n",
    "plt.ylim(0, 0.5*10**7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56c245",
   "metadata": {},
   "source": [
    "## 10 Iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cca93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID, local_data, method, streaming=False, eta=1, num_steps=1, delay_scaling=5,\n",
    "user_c0_10scipy = [client(i, cond0_dict_list[i], 'EtaScipyMinStep', D0=cond0_init_decs[i], num_steps=10, delay_scaling=0) for i in range(14)]\n",
    "\n",
    "#ID, all_clients, D0\n",
    "global_model2 = server(2, user_c0_10scipy, D_0)\n",
    "\n",
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model2.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbcdb3",
   "metadata": {},
   "source": [
    "## Varying Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc4b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ID, local_data, method, streaming=False, eta=1, num_steps=1, delay_scaling=5,\n",
    "user_c0_eta10 = [client(i, cond0_dict_list[i], 'EtaGradStep', D0=cond0_init_decs[i], eta=10, delay_scaling=0) for i in range(14)]\n",
    "\n",
    "#ID, all_clients, D0\n",
    "global_model3 = server(3, user_c0_eta10, D_0)\n",
    "\n",
    "for i in range(big_loop_iters):\n",
    "    if i%(big_loop_iters*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model3.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dabdb5",
   "metadata": {},
   "source": [
    "## Full Scipy.Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1e17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ID, local_data, method, full_data=True, streaming=False\n",
    "print(\"YOU ARE USING STREAMING WHICH WILL PROBABLY GET CHANGED IN THE FUTURE\")\n",
    "user_c0_fullscipy = [client(i, cond0_dict_list[i], 'FullScipyMinStep', D0=cond0_init_decs[i], full_data=False, streaming=True, delay_scaling=0) for i in range(14)]\n",
    "\n",
    "# ID, all_clients, D0, method='FedAvg', advance_each_iter=False\n",
    "global_model4 = server(4, user_c0_fullscipy, D_0, advance_each_iter=True)\n",
    "\n",
    "for i in range(18):\n",
    "    if i%np.ceil(18*.1)==0:\n",
    "        print(f\"Iter {i}\")\n",
    "    global_model4.execute_FL_loop()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a54aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc684ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
