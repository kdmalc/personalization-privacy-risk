{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecfb2e",
   "metadata": {},
   "source": [
    "## Base Loss Code From PyTorch Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(x, y):\n",
    "    return torch.mean((x - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f11be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(Module):\n",
    "    reduction: str\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction: str = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(_Loss):\n",
    "    r\"\"\"Creates a criterion that measures the mean squared error (squared L2 norm) between\n",
    "    each element in the input :math:`x` and target :math:`y`.\n",
    "\n",
    "    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = \\left( x_n - y_n \\right)^2,\n",
    "\n",
    "    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n",
    "    (default ``'mean'``), then:\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) =\n",
    "        \\begin{cases}\n",
    "            \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n",
    "            \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n",
    "        \\end{cases}\n",
    "\n",
    "    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total\n",
    "    of :math:`n` elements each.\n",
    "\n",
    "    The mean operation still operates over all the elements, and divides by :math:`n`.\n",
    "\n",
    "    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.\n",
    "\n",
    "    Args:\n",
    "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
    "            the losses are averaged over each loss element in the batch. Note that for\n",
    "            some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
    "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
    "            when :attr:`reduce` is ``False``. Default: ``True``\n",
    "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
    "            losses are averaged or summed over observations for each minibatch depending\n",
    "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
    "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
    "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
    "            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
    "        - Target: :math:`(*)`, same shape as the input.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> loss = nn.MSELoss()\n",
    "        >>> input = torch.randn(3, 5, requires_grad=True)\n",
    "        >>> target = torch.randn(3, 5)\n",
    "        >>> output = loss(input, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        return F.mse_loss(input, target, reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce1572",
   "metadata": {},
   "source": [
    "## Update This Func to Reflect The Above Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_l2_torch(F, D, V, learning_batch, lambdaF=1e-7, lambdaD=1e-3, lambdaE=1e-6, Nd=2, Ne=64):\n",
    "    # c_L2 = (lambdaE||DF + V+||_2)^2 + lambdaD*(||D||_2)^2 + lambdaF*(||F||_2)^2\n",
    "    \n",
    "    '''\n",
    "    F: 64 channels x time EMG signals\n",
    "    V: 2 x time target velocity\n",
    "    D: 2 (x y vel) x 64 channels decoder\n",
    "    H: 2 x 2 state transition matrix\n",
    "    alphaE is 1e-6 for all conditions\n",
    "    ''' \n",
    "    \n",
    "    # Hmm should I detach and use numpy or just stick with tensor ops?\n",
    "    # I don't want gradient to be tracked here but idk if it matters...\n",
    "\n",
    "    Nt = learning_batch\n",
    "    D = D.view(Nd, Ne)  #np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    # Performance\n",
    "    term1 = lambdaE*(torch.linalg.matrix_norm((torch.matmul(D, F) - Vplus))**2)\n",
    "    # D Norm\n",
    "    term2 = lambdaD*(torch.linalg.matrix_norm((D)**2))\n",
    "    # F Norm\n",
    "    term3 = lambdaF*(torch.linalg.matrix_norm((F)**2))\n",
    "    return (term1 + term2 + term3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPHSLoss(_Loss):\n",
    "    def __init__(self, F, D, V, learning_batch, lambdaF=0, lambdaD=1e-6, lambdaE=1e-3, Nd=2, Ne=64) -> None:\n",
    "        super().__init__(F, D, V, learning_batch, lambdaF=0, lambdaD=1e-6, lambdaE=1e-3, Nd=2, Ne=64)\n",
    "        self.F = F\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.learning_batch = learning_batch\n",
    "        self.lambdaF = lambdaF\n",
    "        self.lambdaD = lambdaD\n",
    "        self.lambdaE = lambdaE\n",
    "        self.Nd = Nd\n",
    "        self.Ne = Ne\n",
    "        \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        Nt = self.learning_batch\n",
    "        self.D = self.D.view(self.Nd, self.Ne)\n",
    "        Vplus = self.V[:,1:]\n",
    "        # Performance\n",
    "        term1 = self.lambdaE*(torch.linalg.matrix_norm((torch.matmul(self.D, self.F) - Vplus))**2)\n",
    "        # D Norm\n",
    "        term2 = self.lambdaD*(torch.linalg.matrix_norm((self.D)**2))\n",
    "        # F Norm\n",
    "        term3 = self.lambdaF*(torch.linalg.matrix_norm((self.F)**2))\n",
    "        return (term1 + term2 + term3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
