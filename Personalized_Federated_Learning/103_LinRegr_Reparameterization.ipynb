{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06f7715",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom_loss_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Personalized_Federated_Learning.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_linregr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\Research\\personalization-privacy-risk\\Personalized_Federated_Learning\\utils\\torch_linregr.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_loss_func\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CPHSLoss\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost_l2_torch\u001b[39m(F, D, V, learning_batch, lambdaF\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, lambdaD\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, lambdaE\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, Nd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, Ne\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, return_cost_func_comps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# c_L2 = (lambdaE||DF + V+||_2)^2 + lambdaD*(||D||_2)^2 + lambdaF*(||F||_2)^2\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Don't use return_cost_func_comps since I don't think loss.item() will return a tuple, it only returns scalaras AFAIK\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    F: 64 channels x time EMG signals\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    V: 2 x time target velocity\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    alphaE is 1e-6 for all conditions\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'custom_loss_func'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Personalized_Federated_Learning.\n",
    "from utils.torch_linregr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aaa27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_loss_func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1677f",
   "metadata": {},
   "source": [
    "# Integrating CPHS Data and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6131067",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data\\continuous_full_data_block1.pickle\", 'rb') as handle:\n",
    "    #refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1, Hs_block1, alphas_block1, pDs_block1, times_block1, conditions_block1 = pickle.load(handle)\n",
    "    refs_block1, _, _, _, emgs_block1, _, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "#with open(r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data\\continuous_full_data_block2.pickle\", 'rb') as handle:\n",
    "#    #refs_block2, poss_block2, dec_vels_block2, int_vel_block2, emgs_block2, Ws_block2, Hs_block2, alphas_block2, pDs_block2, times_block2, conditions_block2 = pickle.load(handle)\n",
    "#    refs_block2, _, _, _, emgs_block2, _, _, _, _, _, _ = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['METACPHS_S106', 'METACPHS_S107', 'METACPHS_S108', 'METACPHS_S109', 'METACPHS_S110', 'METACPHS_S111', 'METACPHS_S112', 'METACPHS_S113', 'METACPHS_S114', 'METACPHS_S115', 'METACPHS_S116', 'METACPHS_S117', 'METACPHS_S118', 'METACPHS_S119']\n",
    "num_conds = 8\n",
    "num_channels = 64\n",
    "num_updates = 19\n",
    "cphs_starting_update = 10\n",
    "update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a356a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaF = 0\n",
    "lambdaD = 1e-10\n",
    "lambdaE = 1e-6\n",
    "lambdasFDE=[lambdaF, lambdaD, lambdaE]\n",
    "\n",
    "# Base case: 1e-7/0, 1e-3, 1e-6\n",
    "# Looks pretty similar to the base case @ 0,1e-6,1e-6\n",
    "# @0,1e-4,1e-4: all are broken except FLDP (still static), but EMG Norm Off with PCA On is a great (very steep) elbow plot\n",
    "# @0,1e-5,1e-5: the only one that blows up is the full dataset! FLDP still static.  Magnitudes start around 6-10\n",
    "# @0,1e-7,1e-6: Nothing is broken! FLDP still static.  Magnitudes start around 1\n",
    "# @0,1e-7,1e-5: Full breaks but everything else looks way better! FLDP still static.\n",
    "# @0,1e-8,1e-5: Full breaks but everything else looks way better! FLDP still static.\n",
    "# @0,1e-8,1e-6: Everything works! FLDP still static.  \n",
    "# @0,1e-9,1e-6: Everything works! FLDP still static.  \n",
    "# @0,1e-10,1e-6: Everything works! FLDP still static.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Condition 1\")\n",
    "F1tens_full = torch.from_numpy(emgs_block1[keys[0]][0, :, :]).type(torch.float32)\n",
    "PREF1tens_full = torch.from_numpy(refs_block1[keys[0]][0, :, :]).type(torch.float32)\n",
    "\n",
    "print(f\"EMG Input size: {F1tens_full.size()}\")\n",
    "print(f\"2D Velocity Label size: {PREF1tens_full.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cfd40",
   "metadata": {},
   "source": [
    "## PyTorch Linear Regression With Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = 19\n",
    "starting_update = 0\n",
    "normalize_emg = False\n",
    "pca_channel_default = 64\n",
    "PCA_comps = 64\n",
    "\n",
    "total_epochs_FullData = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_channels = 64  # Change once I add PCA\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_FullData = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_FullData.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_FullData, loss_log_FullDat = full_train_linregr_updates(untrained_model_FullData, F1tens_full, PREF1tens_full, learning_rate, num_iters_per_update=total_epochs_FullData, lambdasFDE=[lambdaF, lambdaD, lambdaE], stream_data_updates=False, use_full_input_data=True, starting_update=0)\n",
    "\n",
    "plt.plot(range(len(loss_log_FullData)), loss_log_FullData, linewidth=3)\n",
    "plt.title(\"Training Loss Per Iteration On Full Data\")\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e57b0",
   "metadata": {},
   "source": [
    "Observe performance on first update only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01772240",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_channels = 64  # Change once I add PCA\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_FirstUpdate = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_FirstUpdate.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_FirstUpdate, loss_log_FirstUpdate = full_train_linregr_updates(untrained_model_FirstUpdate, F1tens_full[update_ix[0]:update_ix[1]], PREF1tens_full[update_ix[0]:update_ix[1]], learning_rate, lambdasFDE=[lambdaF, lambdaD, lambdaE], num_iters_per_update=total_epochs_FullData, stream_data_updates=False, use_full_input_data=True)\n",
    "\n",
    "plt.plot(range(len(loss_log_FirstUpdate)), loss_log_FirstUpdate, linewidth=3)\n",
    "plt.title(\"Training Loss Per Iteration On First Update's Data\")\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc59fd9",
   "metadata": {},
   "source": [
    "## PyTorch Linear Regression With Streamed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = 19\n",
    "starting_update = 0\n",
    "normalize_emg = False\n",
    "pca_channel_default = 64\n",
    "PCA_comps = 64\n",
    "\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b399520",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_per_update = 10\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_NoFL = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_NoFL.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_NoFL, loss_log_NoFL = full_train_linregr_updates(untrained_model_NoFL, F1tens_full, PREF1tens_full, learning_rate, lambdasFDE=[lambdaF, lambdaD, lambdaE], num_iters_per_update=num_iters_per_update, starting_update=0)\n",
    "\n",
    "plt.plot(range(len(loss_log_NoFL)), loss_log_NoFL, linewidth=3)\n",
    "plt.title(\"10 Iters/Up: Training Loss Per Iteration With Data Streaming\")\n",
    "update_advancement_idxs = range(0,num_iters_per_update*(num_updates-1), num_iters_per_update)\n",
    "for i in update_advancement_idxs:\n",
    "    plt.axvline(i, linewidth=0.75, linestyle='--', color='black')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_per_update = 100\n",
    "\n",
    "num_input_channels = 64  # Change once I add PCA\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_NoFL = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_NoFL.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_NoFL, loss_log_NoFL = full_train_linregr_updates(untrained_model_NoFL, F1tens_full, PREF1tens_full, learning_rate, lambdasFDE=[lambdaF, lambdaD, lambdaE], num_iters_per_update=num_iters_per_update, starting_update=0)\n",
    "\n",
    "plt.plot(range(len(loss_log_NoFL)), loss_log_NoFL, linewidth=3)\n",
    "plt.title(\"100 Iters/Up: Training Loss Per Iteration With Data Streaming\")\n",
    "update_advancement_idxs = range(0,num_iters_per_update*(num_updates-1), num_iters_per_update)\n",
    "for i in update_advancement_idxs:\n",
    "    plt.axvline(i, linewidth=0.75, linestyle='--', color='black')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092d027",
   "metadata": {},
   "source": [
    "## Redo but with FL Data Processing Turned On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062261c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "num_updates = 9\n",
    "starting_update = 10\n",
    "normalize_emg = True\n",
    "pca_channel_default = 64\n",
    "PCA_comps = 7\n",
    "\n",
    "# BUILD MODEL\n",
    "num_input_channels = PCA_comps \n",
    "input_size = num_input_channels\n",
    "# ^ IRL I'm not sure we are guaranteed to get equalength trials\n",
    "#  Thus may be better to do every time point individually? Not sure\n",
    "output_size = 2  # 2D Velocity\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad396c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_per_update=10\n",
    "\n",
    "num_input_channels = PCA_comps  # Change once I add PCA\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_FLDP = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_FLDP.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_FLDP, loss_log_FLDP = full_train_linregr_updates(untrained_model_FLDP, F1tens_full, PREF1tens_full, learning_rate, num_iters_per_update=num_iters_per_update, starting_update=10, lambdasFDE=[lambdaF, lambdaD, lambdaE], normalize_emg=True, PCA_comps=7, loss_log=[])\n",
    "\n",
    "plt.plot(range(len(loss_log_FLDP)), loss_log_FLDP, linewidth=3)\n",
    "plt.title(\"10 Iters/Up: Training Loss Per Iteration With Streaming and FL-DataProcessing\")\n",
    "update_advancement_idxs = range(0,num_iters_per_update*(num_updates-1), num_iters_per_update)\n",
    "for i in update_advancement_idxs:\n",
    "    plt.axvline(i, linewidth=0.75, linestyle='--', color='black')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_per_update=100\n",
    "\n",
    "num_input_channels = PCA_comps  # Change once I add PCA\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_FLDP100 = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_FLDP100.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_FLDP100, loss_log_FLDP100 = full_train_linregr_updates(untrained_model_FLDP100, F1tens_full, PREF1tens_full, learning_rate, num_iters_per_update=num_iters_per_update, lambdasFDE=[lambdaF, lambdaD, lambdaE], starting_update=10, normalize_emg=True, PCA_comps=7, loss_log=[])\n",
    "\n",
    "plt.plot(range(len(loss_log_FLDP100)), loss_log_FLDP100, linewidth=3)\n",
    "plt.title(\"100 Iters/Up: Training Loss Per Iteration With Streaming and FL-DataProcessing\")\n",
    "update_advancement_idxs = range(0,num_iters_per_update*(num_updates-1), num_iters_per_update)\n",
    "for i in update_advancement_idxs:\n",
    "    plt.axvline(i, linewidth=0.75, linestyle='--', color='black')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a36f6c",
   "metadata": {},
   "source": [
    "Investigating why the above code is trash with static cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374ccd2",
   "metadata": {},
   "source": [
    "Norm EMG but no PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "num_updates = 9\n",
    "starting_update = 10\n",
    "normalize_emg = True\n",
    "pca_channel_default = 64\n",
    "PCA_comps = 64\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "num_iters_per_update=500\n",
    "\n",
    "num_input_channels = PCA_comps  # Change once I add PCA\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_FLDP100 = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_FLDP100.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_FLDP100, loss_log_FLDP100,  = full_train_linregr_updates(untrained_model_FLDP100, F1tens_full, PREF1tens_full, learning_rate, num_iters_per_update=num_iters_per_update, lambdasFDE=[lambdaF, lambdaD, lambdaE], starting_update=10, normalize_emg=normalize_emg, PCA_comps=PCA_comps, loss_log=[])\n",
    "\n",
    "plt.plot(range(len(loss_log_FLDP100)), loss_log_FLDP100, linewidth=3)\n",
    "plt.title(\"EMG Normalized But No PCA\")\n",
    "update_advancement_idxs = range(0,num_iters_per_update*(num_updates-1), num_iters_per_update)\n",
    "for i in update_advancement_idxs:\n",
    "    plt.axvline(i, linewidth=0.75, linestyle='--', color='black')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8dcaa",
   "metadata": {},
   "source": [
    "Don't Normalize EMG but do use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "num_updates = 9\n",
    "starting_update = 10\n",
    "normalize_emg = False\n",
    "pca_channel_default = 64\n",
    "PCA_comps = 7\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "num_iters_per_update=500\n",
    "\n",
    "num_input_channels = PCA_comps  # Change once I add PCA\n",
    "input_size = num_input_channels\n",
    "output_size = 2  # 2D Velocity\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Other option which should give same result:\n",
    "untrained_model_FLDP100 = torch.nn.Linear(input_size, output_size)  # Single layer nn for Lin Regr\n",
    "optimizer = torch.optim.SGD(untrained_model_FLDP100.parameters(), lr=learning_rate)\n",
    "\n",
    "trained_model_FLDP100, loss_log_FLDP100 = full_train_linregr_updates(untrained_model_FLDP100, F1tens_full, PREF1tens_full, learning_rate, num_iters_per_update=num_iters_per_update, starting_update=10, lambdasFDE=[lambdaF, lambdaD, lambdaE], normalize_emg=normalize_emg, PCA_comps=PCA_comps, loss_log=[])\n",
    "\n",
    "plt.plot(range(len(loss_log_FLDP100)), loss_log_FLDP100, linewidth=3)\n",
    "plt.title(\"No EMG Norm, PCA=7\")\n",
    "update_advancement_idxs = range(0,num_iters_per_update*(num_updates-1), num_iters_per_update)\n",
    "for i in update_advancement_idxs:\n",
    "    plt.axvline(i, linewidth=0.75, linestyle='--', color='black')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Cost L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c373a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
