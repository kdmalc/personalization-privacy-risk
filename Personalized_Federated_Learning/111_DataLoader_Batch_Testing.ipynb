{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d646d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "\n",
    "from flcore.pflniid_utils.data_utils import read_data, read_client_data\n",
    "from utils.custom_loss_class import CPHSLoss\n",
    "#from flcore.clients.clienbase import Client\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2504e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd05827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def read_client_data(dataset, idx, is_train=True, condition_number=1, test_split=0.2, test_split_each_update=False):\n",
    "#    # KAI'S EDITED VERSION WHICH IS NOW USED IN THE CODE\n",
    "#    print(f\"Client{idx}, train={is_train}: read_CLIENT_data() called!\")\n",
    "#    \n",
    "#    dataset_obj = read_data(dataset, idx, is_train, condition_number=condition_number, test_split=test_split, test_split_each_update=test_split_each_update)\n",
    "#    X_data = torch.Tensor(dataset_obj['x']).type(torch.float32)\n",
    "#    y_data = torch.Tensor(dataset_obj['y']).type(torch.float32)\n",
    "#    zipped_data = [(x, y) for x, y in zip(X_data, y_data)]\n",
    "#    return zipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7daeabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client0, train=True: read_data() called!\n"
     ]
    }
   ],
   "source": [
    "dataset_test = read_data('cphs', 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bb148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07cd2e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client0, train=True: read_CLIENT_data() called!\n",
      "Client0, train=True: read_data() called!\n"
     ]
    }
   ],
   "source": [
    "zd = read_client_data('cphs', 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e2e773b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4994b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_data = zd[0]\n",
    "emg_labels = zd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a3424f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d860e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e9fba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00e90c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip\n",
    "emg_data, emg_labels = list(zip(*zd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "064bcfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb4d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emg_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e41e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08cadc6a",
   "metadata": {},
   "source": [
    "# Manual Data Split (No Dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a6813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ix=[0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n",
    "\n",
    "all_user_keys = ['METACPHS_S106', 'METACPHS_S107', 'METACPHS_S108', 'METACPHS_S109', 'METACPHS_S110', 'METACPHS_S111', 'METACPHS_S112', 'METACPHS_S113', 'METACPHS_S114', 'METACPHS_S115', 'METACPHS_S116', 'METACPHS_S117', 'METACPHS_S118', 'METACPHS_S119']\n",
    "with open(r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data\\continuous_full_data_block1.pickle\", 'rb') as handle:\n",
    "    refs_block1, _, _, _, emgs_block1, _, _, _, _, _, _ = pickle.load(handle)\n",
    "\n",
    "idx = 0\n",
    "condition_number = 0\n",
    "test_split=0.2\n",
    "my_user = all_user_keys[idx]\n",
    "upper_bound = round((1-test_split)*(emgs_block1[my_user][condition_number,:,:].shape[0]))\n",
    "#return CustomEMGDataset(emgs_block1[my_user][condition_number,:upper_bound,:], refs_block1[my_user][condition_number,:upper_bound,:])\n",
    "\n",
    "train_emg_c0_cli0 = emgs_block1[my_user][condition_number,:upper_bound,:]\n",
    "ref_emg_c0_cli0 = refs_block1[my_user][condition_number,:upper_bound,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec381837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20770\n"
     ]
    }
   ],
   "source": [
    "print(emgs_block1[my_user][condition_number,:,:].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65c1763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16616\n"
     ]
    }
   ],
   "source": [
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5975525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16616, 64)\n",
      "\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [33.86026003 35.25307787 13.02375661 ...  4.80226271 12.93678155\n",
      "   9.29002013]\n",
      " [33.86026003 35.25307787 13.02375661 ...  4.80226271 12.93678155\n",
      "   9.29002013]\n",
      " [33.86026003 35.25307787 13.02375661 ...  4.80226271 12.93678155\n",
      "   9.29002013]]\n"
     ]
    }
   ],
   "source": [
    "print(train_emg_c0_cli0.shape)\n",
    "print()\n",
    "print(train_emg_c0_cli0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba365721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16616, 2)\n",
      "\n",
      "[[-8.57531218e-05 -5.68816068e-04]\n",
      " [-8.57531218e-05 -5.68816068e-04]\n",
      " [-2.01000378e-04 -1.32411493e-03]\n",
      " ...\n",
      " [ 2.18631066e+01 -1.77762132e+01]\n",
      " [ 2.18631066e+01 -1.77762132e+01]\n",
      " [ 2.24220511e+01 -1.73817723e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(ref_emg_c0_cli0.shape)\n",
    "print()\n",
    "print(ref_emg_c0_cli0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9db32",
   "metadata": {},
   "source": [
    "# Toy Example of DataLoader\n",
    "- https://www.youtube.com/watch?v=3GVUzwXXihs\n",
    "- https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html#BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0c54eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [5., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [7., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [8., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [9., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = torch.zeros((10,11))\n",
    "tp[:,0] = torch.arange(10)\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c9c29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    dataset=tp,\n",
    "    batch_size=10, \n",
    "    drop_last=False) \n",
    "\n",
    "it = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e107c1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [5., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [7., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [8., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [9., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827f8fe",
   "metadata": {},
   "source": [
    "# Validating read_client_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9a3ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client0, train=True: read_CLIENT_data() called!\n",
      "Client0, train=True: read_data() called!\n"
     ]
    }
   ],
   "source": [
    "train_data = read_client_data('cphs', 0, 0, is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d5ebba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl1 = DataLoader(\n",
    "                dataset=train_data,\n",
    "                batch_sampler=torch.utils.data.BatchSampler(\n",
    "                    torch.utils.data.SequentialSampler(train_data), \n",
    "                    batch_size=1200, \n",
    "                    drop_last=False), \n",
    "                shuffle=False\n",
    "        )\n",
    "it1 = iter(dl1)\n",
    "\n",
    "dl2 = DataLoader(\n",
    "                dataset=train_data,\n",
    "                batch_size=1200,\n",
    "                drop_last=False,\n",
    "                shuffle=False) \n",
    "it2 = iter(dl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3566a739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 0 in dl1: x.size(): torch.Size([1200, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(dl1):\n",
    "    print(f\"Pair {i} in dl1: x.size(): {x.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1514163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 0 in dl2: x.size(): torch.Size([1200, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(dl2):\n",
    "    print(f\"Pair {i} in dl2: x.size(): {x.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1144e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39msum(s1[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m s2[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m s1 \u001b[38;5;241m=\u001b[39m \u001b[43mit1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m s2 \u001b[38;5;241m=\u001b[39m it2\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39msum(s1[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m s2[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s1 = it1.__next__()\n",
    "s2 = it2.__next__()\n",
    "print(torch.sum(s1[0] - s2[0]))\n",
    "print()\n",
    "s1 = it1.__next__()\n",
    "s2 = it2.__next__()\n",
    "print(torch.sum(s1[0] - s2[0]))\n",
    "print()\n",
    "s1 = it1.__next__()\n",
    "s2 = it2.__next__()\n",
    "print(torch.sum(s1[0] - s2[0]))\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def _train(self):\n",
    "#    trainloader = self.load_train_data()\n",
    "#    self.model.train()\n",
    "#\n",
    "#    start_time = time.time()\n",
    "#\n",
    "#    max_local_steps = self.local_epochs\n",
    "#\n",
    "#    for step in range(max_local_steps):\n",
    "#        for i, (x, y) in enumerate(trainloader):\n",
    "#            print(f\"Step {step}, pair {i} in traindl\")\n",
    "#            print(f\"x.size(): {x.size()}\")\n",
    "#            if type(x) == type([]):\n",
    "#                x[0] = x[0].to(self.device)\n",
    "#            else:\n",
    "#                x = x.to(self.device)\n",
    "#            y = y.to(self.device)\n",
    "#            output = self.model(x)\n",
    "#            loss = self.loss(output, y, self.model)\n",
    "#            self.optimizer.zero_grad()\n",
    "#            loss.backward()\n",
    "#            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_streaming(dl, model, my_pca_channels=64, dt=1/60):\n",
    "    it = iter(dl)\n",
    "    s0 = it.__next__()\n",
    "    s_temp = s0[0][:1200,:]\n",
    "    p_reference = torch.transpose(s0[1][:1200,:], 0, 1)\n",
    "\n",
    "    # First, normalize the entire s matrix\n",
    "    if False:\n",
    "        s_normed = s_temp / torch.linalg.norm(s_temp, ord='fro')\n",
    "        assert (torch.linalg.norm(s_normed, ord='fro')<1.2) and (torch.linalg.norm(s_normed, ord='fro')>0.8)\n",
    "    else:\n",
    "        s_normed = s_temp\n",
    "    # Apply PCA if applicable\n",
    "    if my_pca_channels!=64:  # 64 is the number of channels present on the recording armband\n",
    "        pca = PCA(n_components=my_pca_channels)\n",
    "        s = torch.transpose(torch.tensor(pca.fit_transform(s_normed), dtype=torch.float32), 0, 1)\n",
    "    else:\n",
    "        s = torch.transpose(s_normed, 0, 1)\n",
    "\n",
    "    F = s[:,:-1]\n",
    "    v_actual =  torch.matmul(model.weight, s)\n",
    "    p_actual = torch.cumsum(v_actual, dim=1)*dt  # Numerical integration of v_actual to get p_actual\n",
    "    V = (p_reference - p_actual)*dt\n",
    "    if False:\n",
    "        pass\n",
    "        #self.V = self.V/torch.linalg.norm(self.V, ord='fro')\n",
    "        #assert (torch.linalg.norm(self.V, ord='fro')<1.2) and (torch.linalg.norm(self.V, ord='fro')>0.8)\n",
    "    Y = p_reference[:, :-1]  # To match the input\n",
    "    return F, V, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd71042",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dl = dl2\n",
    "model = torch.nn.Linear(64, 2)\n",
    "F, V, Y = simulate_data_streaming(dl2, model)\n",
    "\n",
    "loss = CPHSLoss(F, model.weight, V, F.size()[1])\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)  #self.learning_rate)\n",
    "#learning_rate_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "#    optimizer=self.optimizer, \n",
    "#    gamma=args.learning_rate_decay_gamma\n",
    "#)\n",
    "#self.learning_rate_decay = args.learning_rate_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_iterator = iter(dl2)\n",
    "#s = training_iterator.__next__()\n",
    "\n",
    "trainloader = dl2  #self.load_train_data()\n",
    "# No idea what model.train() does\n",
    "model.train()\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "max_local_steps = 1  #self.local_epochs\n",
    "loss_log = []\n",
    "for step in range(max_local_steps):\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        print(f\"Step {step}, pair {i} in traindl\")\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        output = model(x)\n",
    "        current_loss = loss(output, y, model)\n",
    "        loss_log.append(current_loss.detach().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef098231",
   "metadata": {},
   "outputs": [],
   "source": [
    "1200*13+1016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af1513",
   "metadata": {},
   "source": [
    "Idk... 16616 is the upper limit of the training split.  Should I make all of the updates the same size? By using the actual update size or making new updates? Probably want to use the actual updates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a06fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92606151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f251ee8",
   "metadata": {},
   "source": [
    "# Deconstructed load_train_data() From Clientbase.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df583a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.model = copy.deepcopy(args.model)\n",
    "algorithm = 'FedAvg'\n",
    "dataset = 'cphs'\n",
    "device = 'cpu'\n",
    "ID = 0  # integer\n",
    "#self.save_folder_name = args.save_folder_name\n",
    "\n",
    "#self.num_classes = args.num_classes\n",
    "#train_samples = train_samples\n",
    "#test_samples = test_samples\n",
    "batch_size = 1200\n",
    "learning_rate = 0.005\n",
    "local_epochs = 1\n",
    "\n",
    "# My additional parameters\n",
    "pca_channels = 64\n",
    "lambdas = [0, 1e-3, 1e-4]\n",
    "lambdaF = lambdas[0]\n",
    "lambdaD = lambdas[1]\n",
    "lambdaE = lambdas[2]\n",
    "current_update = 0\n",
    "local_round = 0\n",
    "last_global_round = 0\n",
    "local_round_threshold = 50\n",
    "update_ix=[0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb85959",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_round += 1\n",
    "if local_round%local_round_threshold==0:\n",
    "    current_update += 1\n",
    "\n",
    "if batch_size == None:\n",
    "    batch_size = batch_size\n",
    "train_data = read_client_data(dataset, ID, 0, is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c952324",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_sampler=torch.utils.data.BatchSampler(\n",
    "            torch.utils.data.SequentialSampler(dataset), \n",
    "            batch_size=batch_size, \n",
    "            drop_last=False) \n",
    ")\n",
    "sit1 = iter(loader)\n",
    "s1 = sit1.__next__()\n",
    "print(s1[0].size())\n",
    "print(s1[0][:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f62753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think it is doing Sequential Sampler by default, if shuffle is False (which is true by default)\n",
    "\n",
    "loader2 = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size, \n",
    "        drop_last=False) \n",
    "sit2 = iter(loader2)\n",
    "s2 = sit2.__next__()\n",
    "print(s2[0].size())\n",
    "print(s2[0][:4, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c59c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s2[0][:10, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80270d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"s2 type: {type(s2)}\")\n",
    "print(f\"s2 len: {len(s2)}\")\n",
    "print(f\"s2[0] (training data) size: {s2[0].size()}\")\n",
    "print(f\"s2[1] (training data) size: {s2[1].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5a226",
   "metadata": {},
   "source": [
    "## Integrating With CPHS Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_temp = s2[0][0:update_ix[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e49d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_EMG = False\n",
    "PCA_comps = 64\n",
    "\n",
    "# First, normalize the entire s matrix\n",
    "if normalize_EMG:\n",
    "    s_normed = s_temp/torch.max(s_temp)\n",
    "else:\n",
    "    s_normed = s_temp\n",
    "# Now do PCA unless it is set to 64 (AKA the default num channels i.e. no reduction)\n",
    "# Also probably ought to find a global transform if possible so I don't recompute it every time...\n",
    "if PCA_comps!=64:  \n",
    "    pca = PCA(n_components=PCA_comps)\n",
    "    s_normed = pca.fit_transform(s_normed)\n",
    "#s = np.transpose(s_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_normed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.transpose(s_normed, 0, 1)\n",
    "s.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6260f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't run this since I don't have the model weights\n",
    "\n",
    "#F = s[:,:-1] # note: truncate F for estimate_decoder\n",
    "#v_actual = self.w@s\n",
    "#p_actual = np.cumsum(v_actual, axis=1)*self.dt  # Numerical integration of v_actual to get p_actual\n",
    "#p_reference = np.transpose(self.labels[lower_bound:upper_bound,:])\n",
    "#self.V = (p_reference - p_actual)*self.dt\n",
    "#\n",
    "#self.loss = CPHSLoss(self.F, self.model.weight, self.V, torch.view(self.F)[0], lambdaF=self.lambdaF, lambdaD=self.lambdaD, lambdaE=self.lambdaE, Nd=2, Ne=self.pca_channels, return_cost_func_comps=False)\n",
    "#\n",
    "#self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "#self.learning_rate_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "#    optimizer=self.optimizer, \n",
    "#    gamma=args.learning_rate_decay_gamma\n",
    "#)\n",
    "#self.learning_rate_decay = args.learning_rate_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245d58c",
   "metadata": {},
   "source": [
    "## Checking How Many Iterations The Iter Object Has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f100bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s2 = sit2.__next__()\n",
    "    print(\"We can run multiple times!\")\n",
    "except StopIteration:\n",
    "    print(\"StopIteration Error: Can only call next once!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8592324",
   "metadata": {},
   "source": [
    "Why can we run multiple times... can we run 18 or 19 total times (once for each update?)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    try:\n",
    "        s2 = sit2.__next__()\n",
    "        print(i+2)  # +2 since we have already called __next__() twice in the code above\n",
    "    except StopIteration:\n",
    "        print(\"StopIteration Error: Can only call next once!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808cd89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
