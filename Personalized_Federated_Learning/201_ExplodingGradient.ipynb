{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5caefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from flcore.pflniid_utils.data_utils import read_client_data\n",
    "from utils.custom_loss_class import CPHSLoss\n",
    "from utils.emg_dataset_class import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdf8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from flcore.pflniid_utils.privacy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f53a5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flcore.clients.clientbase import Client\n",
    "from flcore.clients.clientavg import clientAVG\n",
    "from flcore.servers.serverbase import Server\n",
    "from flcore.servers.serveravg import FedAvg\n",
    "from flcore.servers.serverlocal import Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77d5061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f529d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10c3c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-nnc', '--num_new_clients'], dest='num_new_clients', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general\n",
    "parser.add_argument('-go', \"--goal\", type=str, default=\"test\", \n",
    "                    help=\"The goal for this experiment\")\n",
    "parser.add_argument('-dev', \"--device\", type=str, default=\"cpu\",  # KAI: Changed the default to cpu\n",
    "                    choices=[\"cpu\", \"cuda\"])\n",
    "parser.add_argument('-did', \"--device_id\", type=str, default=\"0\")\n",
    "parser.add_argument('-data', \"--dataset\", type=str, default=\"cphs\")  # KAI: Changed the default to cphs (from mnist)\n",
    "#parser.add_argument('-nb', \"--num_classes\", type=int, default=10)  # Not doing classification...\n",
    "parser.add_argument('-m', \"--model\", type=str, default=\"LinearRegression\")  # KAI: Changed the default to Linear Regression\n",
    "parser.add_argument('-lbs', \"--batch_size\", type=int, default=1200)  # Setting it to a full update would be 1300ish... how many batches does it run? In one epoch? Not even sure where that is set\n",
    "# The 1300 and the batch size are 2 separate things...\n",
    "# I want to restrict the given dataset to just the 1300, but then iterate in batches... or do I since we don't have that much data and can probably just use all the data at once? Make batch size match the update size? ...\n",
    "parser.add_argument('-lr', \"--local_learning_rate\", type=float, default=0.005,\n",
    "                    help=\"Local learning rate\")\n",
    "parser.add_argument('-ld', \"--learning_rate_decay\", type=bool, default=False)\n",
    "parser.add_argument('-ldg', \"--learning_rate_decay_gamma\", type=float, default=0.99)\n",
    "parser.add_argument('-gr', \"--global_rounds\", type=int, default=250)  # KAI: Switched to 250 down from 2000\n",
    "parser.add_argument('-ls', \"--local_epochs\", type=int, default=1, \n",
    "                    help=\"Multiple update steps in one local epoch.\")  # KAI: I think it was 1 originally.  I'm gonna keep it there.  Does this mean I can set batchsize to 1300 and cook?Is my setup capable or running multiple epochs? Implicitly I was doing 1 epoch before, using the full update data I believe...\n",
    "parser.add_argument('-algo', \"--algorithm\", type=str, default=\"FedAvg\")\n",
    "parser.add_argument('-jr', \"--join_ratio\", type=float, default=0.2,\n",
    "                    help=\"Ratio of clients per round\")\n",
    "parser.add_argument('-rjr', \"--random_join_ratio\", type=bool, default=False,\n",
    "                    help=\"Random ratio of clients per round\")\n",
    "parser.add_argument('-nc', \"--num_clients\", type=int, default=14,\n",
    "                    help=\"Total number of clients\")\n",
    "parser.add_argument('-dp', \"--privacy\", type=bool, default=False,\n",
    "                    help=\"differential privacy\")\n",
    "parser.add_argument('-dps', \"--dp_sigma\", type=float, default=0.0)\n",
    "parser.add_argument('-sfn', \"--save_folder_name\", type=str, default='items')\n",
    "\n",
    "# SECTION: practical\n",
    "parser.add_argument('-cdr', \"--client_drop_rate\", type=float, default=0.0,\n",
    "                    help=\"Rate for clients that train but drop out\")\n",
    "parser.add_argument('-tsr', \"--train_slow_rate\", type=float, default=0.0,\n",
    "                    help=\"The rate for slow clients when training locally\")\n",
    "parser.add_argument('-ssr', \"--send_slow_rate\", type=float, default=0.0,\n",
    "                    help=\"The rate for slow clients when sending global model\")\n",
    "parser.add_argument('-ts', \"--time_select\", type=bool, default=False,\n",
    "                    help=\"Whether to group and select clients at each round according to time cost\")\n",
    "parser.add_argument('-tth', \"--time_threthold\", type=float, default=10000,\n",
    "                    help=\"The threthold for droping slow clients\")\n",
    "\n",
    "# SECTION: Kai's additional args\n",
    "parser.add_argument('-pca_channels', \"--pca_channels\", type=int, default=64,\n",
    "                    help=\"Number of principal components. 64 means do not use any PCA\")\n",
    "parser.add_argument('-lambdaF', \"--lambdaF\", type=float, default=0.0,\n",
    "                    help=\"Penalty term for user EMG input (user effort)\")\n",
    "parser.add_argument('-lambdaD', \"--lambdaD\", type=float, default=1e-3,\n",
    "                    help=\"Penalty term for the decoder norm (interface effort)\")\n",
    "parser.add_argument('-lambdaE', \"--lambdaE\", type=float, default=1e-4,\n",
    "                    help=\"Penalty term on performance error norm\")\n",
    "parser.add_argument('-starting_update', \"--starting_update\", type=int, default=0,\n",
    "                    help=\"Which update to start on (for CPHS Simulation). Use 0 or 10.\")\n",
    "parser.add_argument('-test_split_fraction', \"--test_split_fraction\", type=float, default=0.2,\n",
    "                    help=\"Fraction of data to use for testing\")\n",
    "parser.add_argument('-device_channels', \"--device_channels\", type=int, default=64,\n",
    "                    help=\"Number of recording channels with the used EMG device\")\n",
    "parser.add_argument('-dt', \"--dt\", type=float, default=1/60,\n",
    "                    help=\"Delta time, amount of time (sec?) between measurements\")\n",
    "parser.add_argument('-normalize_emg', \"--normalize_emg\", type=bool, default=False,\n",
    "                    help=\"Normalize the input EMG signals\")\n",
    "parser.add_argument('-normalize_V', \"--normalize_V\", type=bool, default=False,\n",
    "                    help=\"Normalize the V term in the cost function\")\n",
    "parser.add_argument('-local_round_threshold', \"--local_round_threshold\", type=int, default=50,\n",
    "                    help=\"Number of communication rounds per client until a client will advance to the next batch of streamed data\")\n",
    "parser.add_argument('-debug_mode', \"--debug_mode\", type=bool, default=False,\n",
    "                    help=\"In debug mode, the code is run to minimize overhead time in order to debug as fast as possible.  Namely, the data is held at the server to decrease init time, and communication delays are ignored.\")\n",
    "parser.add_argument('-condition_number', \"--condition_number\", type=int, default=1,\n",
    "                    help=\"Which condition number (trial) to train on\")\n",
    "parser.add_argument('-test_split_each_update', \"--test_split_each_update\", type=bool, default=False,\n",
    "                    help=\"Implement train/test split within each update or on the entire dataset\")\n",
    "parser.add_argument('-verbose', \"--verbose\", type=bool, default=False,\n",
    "                    help=\"Print out a bunch of extra stuff\")\n",
    "parser.add_argument('-slow_clients_bool', \"--slow_clients_bool\", type=bool, default=False,\n",
    "                    help=\"Control whether or not to have ANY slow clients\")\n",
    "parser.add_argument('-return_cost_func_comps', \"--return_cost_func_comps\", type=bool, default=False, #True\n",
    "                    help=\"Return Loss, Error, DTerm, FTerm from loss class\")\n",
    "parser.add_argument('-test_split_users', \"--test_split_users\", type=bool, default=False,\n",
    "                    help=\"Split testing data by holding out some users (fraction held out determined by test_split_fraction)\")\n",
    "    \n",
    "parser.add_argument('-t', \"--times\", type=int, default=1,\n",
    "                    help=\"Running times\")\n",
    "parser.add_argument('-ab', \"--auto_break\", type=bool, default=False)\n",
    "parser.add_argument('-dlg', \"--dlg_eval\", type=bool, default=False)\n",
    "parser.add_argument('-dlgg', \"--dlg_gap\", type=int, default=100)\n",
    "parser.add_argument('-bnpc', \"--batch_num_per_client\", type=int, default=2)  # Only used with DLG\n",
    "parser.add_argument('-eg', \"--eval_gap\", type=int, default=1,\n",
    "                    help=\"Rounds gap for evaluation\")\n",
    "parser.add_argument('-nnc', \"--num_new_clients\", type=int, default=0)\n",
    "\n",
    "# This one for sure breaks it\n",
    "#parser.add_argument('-fte', \"--fine_tuning_epoch\", type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32fccd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parser.parse_args()\n",
    "args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0579f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(goal='test', device='cpu', device_id='0', dataset='cphs', model='LinearRegression', batch_size=1200, local_learning_rate=0.005, learning_rate_decay=False, learning_rate_decay_gamma=0.99, global_rounds=250, local_epochs=1, algorithm='FedAvg', join_ratio=0.2, random_join_ratio=False, num_clients=14, privacy=False, dp_sigma=0.0, save_folder_name='items', client_drop_rate=0.0, train_slow_rate=0.0, send_slow_rate=0.0, time_select=False, time_threthold=10000, pca_channels=64, lambdaF=0.0, lambdaD=0.001, lambdaE=0.0001, starting_update=0, test_split_fraction=0.2, device_channels=64, dt=0.016666666666666666, normalize_emg=False, normalize_V=False, local_round_threshold=50, debug_mode=False, condition_number=1, test_split_each_update=False, verbose=False, slow_clients_bool=False, return_cost_func_comps=False, test_split_users=False, times=1, auto_break=False, dlg_eval=False, dlg_gap=100, batch_num_per_client=2, eval_gap=1, num_new_clients=0, fine_tuning_epoch=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = args[0]\n",
    "args.fine_tuning_epoch=0\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f044a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe479760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Running time: 0th =============\n",
      "Creating server and clients ...\n",
      "Linear(in_features=64, out_features=2, bias=True)\n",
      "Serveravg init(): set_slow_clients()\n",
      "Serveravg init(): set_clients()\n",
      "SBSC: iter 0\n",
      "SBSC: iter 1\n",
      "SBSC: iter 2\n",
      "SBSC: iter 3\n",
      "SBSC: iter 4\n",
      "SBSC: iter 5\n",
      "SBSC: iter 6\n",
      "SBSC: iter 7\n",
      "SBSC: iter 8\n",
      "SBSC: iter 9\n",
      "SBSC: iter 10\n",
      "SBSC: iter 11\n",
      "SBSC: iter 12\n",
      "SBSC: iter 13\n",
      "\n",
      "Join ratio / total clients: 0.2 / 14\n",
      "Finished creating server and clients.\n"
     ]
    }
   ],
   "source": [
    "time_list = []\n",
    "#reporter = MemReporter()\n",
    "model_str = args.model\n",
    "\n",
    "# Switched args.prev to 0 since it wasn't working\n",
    "#for i in range(0, args.times):\n",
    "print(f\"\\n============= Running time: {0}th =============\")\n",
    "print(\"Creating server and clients ...\")\n",
    "start = time.time()\n",
    "\n",
    "# Generate args.model\n",
    "args.model = torch.nn.Linear(args.pca_channels, 2)  #input_size, output_size\n",
    "\n",
    "print(args.model)\n",
    "\n",
    "# select algorithm\n",
    "if args.algorithm == \"FedAvg\":\n",
    "    server = FedAvg(args, 0)\n",
    "elif args.algorithm == \"Local\":\n",
    "    server = Local(args, 0)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "#server.train()\n",
    "\n",
    "#time_list.append(time.time()-start)\n",
    "#print(f\"\\nAverage time cost: {round(np.average(time_list), 2)}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e68086a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------Round number: 0-------------\n",
      "Selected client IDs: [10, 2]\n"
     ]
    }
   ],
   "source": [
    "server.selected_clients = server.clients\n",
    "with torch.no_grad():\n",
    "    # subscript global_model with [0] if it is sequential instead of linear model --> does that return just the first layer then?\n",
    "    server.global_model.weight.fill_(0)\n",
    "\n",
    "#for i in range(self.global_rounds+1):\n",
    "if 0%server.eval_gap == 0:\n",
    "    print(f\"\\n-------------Round number: {0}-------------\")\n",
    "    if 0!=0:\n",
    "        print(\"\\nEvaluate personalized models\")\n",
    "        server.evaluate()\n",
    "\n",
    "        #print(f\"len: {len(self.rs_train_loss[-1])}\")\n",
    "        if type(server.rs_train_loss[-1]) in [int, float]:\n",
    "            print(f\"rs_train_loss: {server.rs_train_loss[-1]}\")\n",
    "        else:\n",
    "            print(f\"len: {len(server.rs_train_loss[-1])}\")\n",
    "        print()\n",
    "\n",
    "server.selected_clients = server.select_clients()\n",
    "print(f\"Selected client IDs: {[client.ID for client in server.selected_clients]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa18673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"CLIENT TRAINING\")\n",
    "#for client in server.selected_clients:\n",
    "#    client.train()\n",
    "#    print(f\"Client{client.ID} loss: {client.loss_log[-1]:0,.3f}\")\n",
    "\n",
    "my_client = server.selected_clients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc17c34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.local_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "430d1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(self):\n",
    "trainloader = my_client.load_train_data()\n",
    "# self.model.to(self.device)\n",
    "my_client.model.train()\n",
    "\n",
    "# differential privacy\n",
    "#if self.privacy:\n",
    "#    self.model, self.optimizer, trainloader, privacy_engine = \\\n",
    "#        initialize_dp(self.model, self.optimizer, trainloader, self.dp_sigma)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "max_local_steps = my_client.local_epochs\n",
    "#if self.train_slow:\n",
    "#    max_local_steps = np.random.randint(1, max_local_steps // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8f5c545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2a68c0eba60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0275b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "base_data_path = 'C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\personalization-privacy-risk\\\\Data\\\\Client_Specific_Files\\\\'\n",
    "client = clientAVG(server.args, \n",
    "                    ID=i, \n",
    "                    train_samples = base_data_path + \"UserID\" + str(i) + \"_TrainData_8by20770by64.npy\", \n",
    "                    test_samples = base_data_path + \"UserID\" + str(i) + \"_Labels_8by20770by2.npy\", \n",
    "                    train_slow=False, \n",
    "                    send_slow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca38014c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.test_split_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e03f657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.test_split_each_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b806988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM CLIENTBASE.PY\n",
    "\n",
    "#def load_train_data(self, batch_size=None):\n",
    "batch_size=None\n",
    "# Load full client dataasets\n",
    "if my_client.local_round == 0:\n",
    "\n",
    "    ###########################################################################################\n",
    "    #self._load_train_data()   # Returns nothing, sets self variables\n",
    "    # Load in client's data\n",
    "    with open(my_client.samples_path, 'rb') as handle:\n",
    "        samples_npy = np.load(handle)\n",
    "    with open(my_client.labels_path, 'rb') as handle:\n",
    "        labels_npy = np.load(handle)\n",
    "    # Select for given condition #THIS IS THE ACTUAL TRAINING DATA AND LABELS FOR THE GIVEN TRIAL\n",
    "    my_client.cond_samples_npy = samples_npy[my_client.condition_number,:,:]\n",
    "    my_client.cond_labels_npy = labels_npy[my_client.condition_number,:,:]\n",
    "    # Split data into train and test sets\n",
    "    testsplit_upper_bound = round((1-my_client.test_split_fraction)*(my_client.cond_samples_npy.shape[0]))\n",
    "    # Set the number of examples (used to be done on init) --> ... THIS IS ABOUT TRAIN/TEST SPLIT\n",
    "    my_client.train_samples = testsplit_upper_bound\n",
    "    my_client.test_samples = my_client.cond_samples_npy.shape[0] - testsplit_upper_bound\n",
    "    train_test_update_number_split = min(my_client.update_ix, key=lambda x:abs(x-testsplit_upper_bound))\n",
    "    my_client.max_training_update_upbound = my_client.update_ix.index(train_test_update_number_split)\n",
    "    ###########################################################################################\n",
    "\n",
    "    # Why is this in local_round=0...\n",
    "    #if my_client.current_update < my_client.max_training_update_upbound:\n",
    "    #    my_client.update_lower_bound = my_client.update_ix[my_client.current_update]\n",
    "    #    my_client.update_upper_bound = my_client.update_ix[my_client.current_update+1]\n",
    "    # I just added this, should really be idx bound not update bound...\n",
    "    my_client.update_lower_bound = my_client.update_ix[my_client.current_update]\n",
    "    my_client.update_upper_bound = my_client.update_ix[my_client.current_update+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf2cbe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update lower bound: 0\n",
      "update upper bound: 1200\n"
     ]
    }
   ],
   "source": [
    "print(f\"update lower bound: {my_client.update_lower_bound}\")\n",
    "print(f\"update upper bound: {my_client.update_upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee49e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.local_round += 1\n",
    "# Check if you need to advance the update\n",
    "# ---> THIS IMPLIES THAT I AM CREATING A NEW TRAINING LOADER FOR EACH UPDATE...\n",
    "# Uh why is 16 hardcoded...\n",
    "# This is the update logic\n",
    "if (my_client.local_round>1) and (my_client.current_update < 16) and (my_client.local_round%my_client.local_round_threshold==0):\n",
    "    my_client.current_update += 1\n",
    "    print(f\"Client{my_client.ID} advances to update {my_client.current_update}\")\n",
    "    # Slice the full client dataset based on the current update number\n",
    "    if my_client.current_update < my_client.max_training_update_upbound:\n",
    "        my_client.update_lower_bound = my_client.update_ix[my_client.current_update]\n",
    "        my_client.update_upper_bound = my_client.update_ix[my_client.current_update+1]\n",
    "    else:\n",
    "        my_client.update_lower_bound = my_client.max_training_update_upbound - 1\n",
    "        my_client.update_upper_bound = my_client.max_training_update_upbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Dataset Obj\n",
    "# Uhhhh is this creating a new one each time? As long as its not re-reading in the data it probably doesn't matter...\n",
    "#train_data = read_client_data(self.dataset, self.ID, self.current_update, is_train=True)  # Original code\n",
    "#CustomEMGDataset(emgs_block1[my_user][condition_idx,update_lower_bound:update_upper_bound,:], refs_block1[my_user][condition_idx,update_lower_bound:update_upper_bound,:])\n",
    "training_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:], my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:])\n",
    "X_data = torch.Tensor(training_dataset_obj['x']).type(torch.float32)\n",
    "y_data = torch.Tensor(training_dataset_obj['y']).type(torch.float32)\n",
    "training_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7b90c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:], my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edbc9978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70c65e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dacefc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200, 64])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(training_dataset_obj['x']).type(torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b13500a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_for_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "979ea185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_for_dataloader[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b2ad6",
   "metadata": {},
   "source": [
    "That all looks fine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd3136a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataloader\n",
    "if batch_size == None:\n",
    "    batch_size = my_client.batch_size\n",
    "trainloader = DataLoader(\n",
    "    dataset=training_data_for_dataloader,\n",
    "    batch_size=batch_size, \n",
    "    drop_last=False,  # Yah idk if this should be true or false or if it matters...\n",
    "    shuffle=False) \n",
    "#return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b47cacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_samples_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04f5035e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_labels_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9e53793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(trainloader):\n",
    "    print(f\"Batch {i}: x has size {x.size()}; y has size {y.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3f913",
   "metadata": {},
   "source": [
    "Lmao so on rerun it works now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62c252de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18370"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200*15+370"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88fe7d",
   "metadata": {},
   "source": [
    "It shouldn't be going through all the batches at once right... it should only be going through the first updates worth...\n",
    "- There's 16 batches, but 18 updates... looks like it held the last few batches out for testing? \n",
    "- Don't really wanna test on the actual last batch\n",
    "- Also shouldn't the training updates by broken up perfectly and not with some leftover? Code isn't working as expected...\n",
    "- Does simulate data streaming have any affect?\n",
    "- Where was it in the code that I kept auto-remaking trainloaders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01a320d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(update_ix)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bc54b",
   "metadata": {},
   "source": [
    "Uhh is this code only used once lol\n",
    "- It's literaly only used in the client init..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before this I need to run the INIT update segmentation code...\n",
    "#init_dl = self.load_train_data()\n",
    "#self.simulate_data_streaming(init_dl)\n",
    "# ^ This func sets F, V, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bdbdb",
   "metadata": {},
   "source": [
    "update setting code SHOULD NOT be in test data (unless each update has its own separate test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fff00ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_test_data(self, batch_size=None):\n",
    "batch_size=None\n",
    "# Make sure this runs AFTER load_train_data so the data is already loaded in\n",
    "if batch_size == None:\n",
    "    batch_size = my_client.batch_size\n",
    "\n",
    "#test_data = read_client_data(self.dataset, self.ID, self.current_update, is_train=False)\n",
    "testing_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_upper_bound:,:], my_client.cond_labels_npy[my_client.update_upper_bound:,:])\n",
    "X_data = torch.Tensor(testing_dataset_obj['x']).type(torch.float32)\n",
    "y_data = torch.Tensor(testing_dataset_obj['y']).type(torch.float32)\n",
    "testing_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]\n",
    "\n",
    "testloader = DataLoader(\n",
    "    dataset=testing_data_for_dataloader,\n",
    "    batch_size=batch_size, \n",
    "    drop_last=False,  # Yah idk if this should be true or false or if it matters...\n",
    "    shuffle=False) \n",
    "#return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc5e5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 1: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 2: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 3: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 4: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 5: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 6: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 7: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 8: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 9: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 10: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 11: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 12: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 13: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 14: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 15: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 16: x has size torch.Size([370, 64]); y has size torch.Size([370, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(testloader):\n",
    "    print(f\"Batch {i}: x has size {x.size()}; y has size {y.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8db0f",
   "metadata": {},
   "source": [
    "Uhhhh why is this exactly the same as the trainloader....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c4276d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.update_upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f27e9d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.max_training_update_upbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46454728",
   "metadata": {},
   "source": [
    "Uhhh so I mean that's actually fine, given that that is the UPDATE NUMBER! not the index to split at..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1307677",
   "metadata": {},
   "source": [
    "# CORRECTION TO LOAD_TEST_DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f66ca6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED\n",
    "#def load_test_data(self, batch_size=None):\n",
    "batch_size=None\n",
    "# Make sure this runs AFTER load_train_data so the data is already loaded in\n",
    "if batch_size == None:\n",
    "    batch_size = my_client.batch_size\n",
    "\n",
    "##########################################################################\n",
    "# Idk if update_ix would need self or if it even exists within the object yet lmao\n",
    "my_client.test_split_idx = update_ix[my_client.max_training_update_upbound]\n",
    "##########################################################################\n",
    "    \n",
    "#test_data = read_client_data(self.dataset, self.ID, self.current_update, is_train=False)\n",
    "testing_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.test_split_idx:,:], my_client.cond_labels_npy[my_client.test_split_idx:,:])\n",
    "X_data = torch.Tensor(testing_dataset_obj['x']).type(torch.float32)\n",
    "y_data = torch.Tensor(testing_dataset_obj['y']).type(torch.float32)\n",
    "testing_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]\n",
    "\n",
    "correctedtestloader = DataLoader(\n",
    "    dataset=testing_data_for_dataloader,\n",
    "    batch_size=batch_size, \n",
    "    drop_last=False,  # Yah idk if this should be true or false or if it matters...\n",
    "    shuffle=False) \n",
    "#return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58241c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 1: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 2: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 3: x has size torch.Size([344, 64]); y has size torch.Size([344, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(correctedtestloader):\n",
    "    print(f\"Batch {i}: x has size {x.size()}; y has size {y.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f63f1",
   "metadata": {},
   "source": [
    "Seems like it fixed it... \n",
    "- Do I wanna just drop the last batch? It might mess things up since it's not the same size\n",
    "- Idk what would happen in real-time trials though if everything has to be set up into uniform sized buckets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070f532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e6645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe96213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHICH OF THESE LOOPS IS EQUIVALENT TO MY EPOCHS...\n",
    "running_num_samples = 0\n",
    "step = 0\n",
    "#for step in range(max_local_steps):  # I'm assuming this is gradient steps?... are local epochs the same as gd steps?\n",
    "for i, (x, y) in enumerate(trainloader):  # This is all the data in a given batch, I think? Can I just kill this... PITA\n",
    "    print(f\"Step {step}, pair {i} in traindl: x.size(): {x.size()}\")\n",
    "    if type(x) == type([]):\n",
    "        x[0] = x[0].to(my_client.device)\n",
    "    else:\n",
    "        x = x.to(my_client.device)\n",
    "    y = y.to(my_client.device)\n",
    "    #if self.train_slow:\n",
    "    #    time.sleep(0.1 * np.abs(np.random.rand()))\n",
    "    output = my_client.model(x)\n",
    "    #print(f\"clientAVG ----> Training LOSS {i}\")  # What is this even tellimg me lol\n",
    "    loss = my_client.loss(output, y, self.model)\n",
    "    if my_client.return_cost_func_comps:\n",
    "        my_client.cost_func_comps_log.append(loss[1:])\n",
    "        loss = loss[0]\n",
    "    else:\n",
    "        # .item() ONLY WORKS WITH 1D TENSORS!!!\n",
    "        t1 = my_client.loss.term1_error.item()\n",
    "        t2 = my_client.loss.term2_ld_decnorm.item()\n",
    "        t3 = my_client.loss.term3_lf_emgnorm.item()\n",
    "        if np.isnan(t1):\n",
    "            print(\"CLIENTAVG: Error term is None...\")\n",
    "            t1 = -1\n",
    "        if np.isnan(t2):\n",
    "            print(\"CLIENTAVG: Decoder Effort term is None...\")\n",
    "            t2 = -1\n",
    "        if np.isnan(t3):\n",
    "            print(\"CLIENTAVG: User Effort term is None...\")\n",
    "            t3 = -1\n",
    "        my_client.cost_func_comps_log.append((t1, t2, t3))\n",
    "    weight_grad = my_client.model.weight.grad\n",
    "    if weight_grad == None:\n",
    "        print(\"Weight gradient is None...\")\n",
    "        my_client.gradient_norm_log.append(-1)\n",
    "    else:\n",
    "        #grad_norm = torch.linalg.norm(self.model.weight.grad, ord='fro')\n",
    "        grad_norm = np.linalg.norm(my_client.model.weight.grad.detach().numpy())\n",
    "        my_client.gradient_norm_log.append(grad_norm)\n",
    "    my_client.loss_log.append(loss.item())\n",
    "    #self.running_epoch_loss.append(loss.item() * x.size(0))  # From: running_epoch_loss.append(loss.item() * images.size(0))\n",
    "    running_num_samples += x.size(0)\n",
    "    my_client.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    my_client.optimizer.step()\n",
    "\n",
    "my_client.train_time_cost['num_rounds'] += 1\n",
    "#my_client.train_time_cost['total_cost'] += time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb599a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "\n",
    "if self.auto_break and self.check_done(acc_lss=[self.rs_test_acc], top_cnt=self.top_cnt):\n",
    "    print(\"Breaking\")\n",
    "    break\n",
    "\n",
    "self.evaluate(train=False, test=True)\n",
    "print(\"\\nBest Loss.\")\n",
    "print(min(self.rs_test_loss))\n",
    "\n",
    "for idx, client in enumerate(self.clients):\n",
    "    #self.cost_func_comps_dict[idx] = client.cost_func_comps_log\n",
    "    #self.gradient_dict[idx] = client.gradient_norm_log\n",
    "    self.cost_func_comps_log.append(client.cost_func_comps_log)\n",
    "    self.gradient_norm_log.append(client.gradient_norm_log)\n",
    "\n",
    "self.save_results(save_cost_func_comps=True, save_gradient=True)\n",
    "model_path = os.path.join(\"models\", self.dataset)\n",
    "model_path = os.path.join(model_path, \"Local\")\n",
    "for client in self.clients:\n",
    "    client.save_item(client.model, 'local_client_model', item_path=model_path)\n",
    "# No idea where this global model is coming from? Why did they save it...\n",
    "self.save_global_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d56043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
