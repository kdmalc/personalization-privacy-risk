{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5caefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from flcore.pflniid_utils.data_utils import read_client_data\n",
    "from utils.custom_loss_class import CPHSLoss\n",
    "from utils.emg_dataset_class import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdf8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from flcore.pflniid_utils.privacy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53a5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flcore.clients.clientbase import Client\n",
    "from flcore.clients.clientavg import clientAVG\n",
    "from flcore.servers.serverbase import Server\n",
    "from flcore.servers.serveravg import FedAvg\n",
    "from flcore.servers.serverlocal import Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d5061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f529d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10c3c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-nnc', '--num_new_clients'], dest='num_new_clients', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general\n",
    "parser.add_argument('-go', \"--goal\", type=str, default=\"test\", \n",
    "                    help=\"The goal for this experiment\")\n",
    "parser.add_argument('-dev', \"--device\", type=str, default=\"cpu\",  # KAI: Changed the default to cpu\n",
    "                    choices=[\"cpu\", \"cuda\"])\n",
    "parser.add_argument('-did', \"--device_id\", type=str, default=\"0\")\n",
    "parser.add_argument('-data', \"--dataset\", type=str, default=\"cphs\")  # KAI: Changed the default to cphs (from mnist)\n",
    "#parser.add_argument('-nb', \"--num_classes\", type=int, default=10)  # Not doing classification...\n",
    "parser.add_argument('-m', \"--model\", type=str, default=\"LinearRegression\")  # KAI: Changed the default to Linear Regression\n",
    "parser.add_argument('-lbs', \"--batch_size\", type=int, default=1200)  # Setting it to a full update would be 1300ish... how many batches does it run? In one epoch? Not even sure where that is set\n",
    "# The 1300 and the batch size are 2 separate things...\n",
    "# I want to restrict the given dataset to just the 1300, but then iterate in batches... or do I since we don't have that much data and can probably just use all the data at once? Make batch size match the update size? ...\n",
    "parser.add_argument('-lr', \"--local_learning_rate\", type=float, default=0.005,\n",
    "                    help=\"Local learning rate\")\n",
    "parser.add_argument('-ld', \"--learning_rate_decay\", type=bool, default=False)\n",
    "parser.add_argument('-ldg', \"--learning_rate_decay_gamma\", type=float, default=0.99)\n",
    "parser.add_argument('-gr', \"--global_rounds\", type=int, default=250)  # KAI: Switched to 250 down from 2000\n",
    "parser.add_argument('-ls', \"--local_epochs\", type=int, default=1, \n",
    "                    help=\"Multiple update steps in one local epoch.\")  # KAI: I think it was 1 originally.  I'm gonna keep it there.  Does this mean I can set batchsize to 1300 and cook?Is my setup capable or running multiple epochs? Implicitly I was doing 1 epoch before, using the full update data I believe...\n",
    "parser.add_argument('-algo', \"--algorithm\", type=str, default=\"FedAvg\")\n",
    "parser.add_argument('-jr', \"--join_ratio\", type=float, default=0.2,\n",
    "                    help=\"Ratio of clients per round\")\n",
    "parser.add_argument('-rjr', \"--random_join_ratio\", type=bool, default=False,\n",
    "                    help=\"Random ratio of clients per round\")\n",
    "parser.add_argument('-nc', \"--num_clients\", type=int, default=14,\n",
    "                    help=\"Total number of clients\")\n",
    "parser.add_argument('-dp', \"--privacy\", type=bool, default=False,\n",
    "                    help=\"differential privacy\")\n",
    "parser.add_argument('-dps', \"--dp_sigma\", type=float, default=0.0)\n",
    "parser.add_argument('-sfn', \"--save_folder_name\", type=str, default='items')\n",
    "\n",
    "# SECTION: practical\n",
    "parser.add_argument('-cdr', \"--client_drop_rate\", type=float, default=0.0,\n",
    "                    help=\"Rate for clients that train but drop out\")\n",
    "parser.add_argument('-tsr', \"--train_slow_rate\", type=float, default=0.0,\n",
    "                    help=\"The rate for slow clients when training locally\")\n",
    "parser.add_argument('-ssr', \"--send_slow_rate\", type=float, default=0.0,\n",
    "                    help=\"The rate for slow clients when sending global model\")\n",
    "parser.add_argument('-ts', \"--time_select\", type=bool, default=False,\n",
    "                    help=\"Whether to group and select clients at each round according to time cost\")\n",
    "parser.add_argument('-tth', \"--time_threthold\", type=float, default=10000,\n",
    "                    help=\"The threthold for droping slow clients\")\n",
    "\n",
    "# SECTION: Kai's additional args\n",
    "parser.add_argument('-pca_channels', \"--pca_channels\", type=int, default=64,\n",
    "                    help=\"Number of principal components. 64 means do not use any PCA\")\n",
    "parser.add_argument('-lambdaF', \"--lambdaF\", type=float, default=0.0,\n",
    "                    help=\"Penalty term for user EMG input (user effort)\")\n",
    "parser.add_argument('-lambdaD', \"--lambdaD\", type=float, default=1e-3,\n",
    "                    help=\"Penalty term for the decoder norm (interface effort)\")\n",
    "parser.add_argument('-lambdaE', \"--lambdaE\", type=float, default=1e-4,\n",
    "                    help=\"Penalty term on performance error norm\")\n",
    "parser.add_argument('-starting_update', \"--starting_update\", type=int, default=0,\n",
    "                    help=\"Which update to start on (for CPHS Simulation). Use 0 or 10.\")\n",
    "parser.add_argument('-test_split_fraction', \"--test_split_fraction\", type=float, default=0.2,\n",
    "                    help=\"Fraction of data to use for testing\")\n",
    "parser.add_argument('-device_channels', \"--device_channels\", type=int, default=64,\n",
    "                    help=\"Number of recording channels with the used EMG device\")\n",
    "parser.add_argument('-dt', \"--dt\", type=float, default=1/60,\n",
    "                    help=\"Delta time, amount of time (sec?) between measurements\")\n",
    "parser.add_argument('-normalize_emg', \"--normalize_emg\", type=bool, default=False,\n",
    "                    help=\"Normalize the input EMG signals\")\n",
    "parser.add_argument('-normalize_V', \"--normalize_V\", type=bool, default=False,\n",
    "                    help=\"Normalize the V term in the cost function\")\n",
    "parser.add_argument('-local_round_threshold', \"--local_round_threshold\", type=int, default=50,\n",
    "                    help=\"Number of communication rounds per client until a client will advance to the next batch of streamed data\")\n",
    "parser.add_argument('-debug_mode', \"--debug_mode\", type=bool, default=False,\n",
    "                    help=\"In debug mode, the code is run to minimize overhead time in order to debug as fast as possible.  Namely, the data is held at the server to decrease init time, and communication delays are ignored.\")\n",
    "parser.add_argument('-condition_number', \"--condition_number\", type=int, default=1,\n",
    "                    help=\"Which condition number (trial) to train on\")\n",
    "parser.add_argument('-test_split_each_update', \"--test_split_each_update\", type=bool, default=False,\n",
    "                    help=\"Implement train/test split within each update or on the entire dataset\")\n",
    "parser.add_argument('-verbose', \"--verbose\", type=bool, default=False,\n",
    "                    help=\"Print out a bunch of extra stuff\")\n",
    "parser.add_argument('-slow_clients_bool', \"--slow_clients_bool\", type=bool, default=False,\n",
    "                    help=\"Control whether or not to have ANY slow clients\")\n",
    "parser.add_argument('-return_cost_func_comps', \"--return_cost_func_comps\", type=bool, default=False, #True\n",
    "                    help=\"Return Loss, Error, DTerm, FTerm from loss class\")\n",
    "parser.add_argument('-test_split_users', \"--test_split_users\", type=bool, default=False,\n",
    "                    help=\"Split testing data by holding out some users (fraction held out determined by test_split_fraction)\")\n",
    "    \n",
    "parser.add_argument('-t', \"--times\", type=int, default=1,\n",
    "                    help=\"Running times\")\n",
    "parser.add_argument('-ab', \"--auto_break\", type=bool, default=False)\n",
    "parser.add_argument('-dlg', \"--dlg_eval\", type=bool, default=False)\n",
    "parser.add_argument('-dlgg', \"--dlg_gap\", type=int, default=100)\n",
    "parser.add_argument('-bnpc', \"--batch_num_per_client\", type=int, default=2)  # Only used with DLG\n",
    "parser.add_argument('-eg', \"--eval_gap\", type=int, default=1,\n",
    "                    help=\"Rounds gap for evaluation\")\n",
    "parser.add_argument('-nnc', \"--num_new_clients\", type=int, default=0)\n",
    "\n",
    "# This one for sure breaks it\n",
    "#parser.add_argument('-fte', \"--fine_tuning_epoch\", type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0579f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parser.parse_args()\n",
    "args = parser.parse_known_args()\n",
    "\n",
    "args = args[0]\n",
    "args.fine_tuning_epoch=0\n",
    "dataset = 'cphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe479760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Running time: 0th =============\n",
      "Creating server and clients ...\n",
      "Linear(in_features=64, out_features=2, bias=True)\n",
      "Serveravg init(): set_slow_clients()\n",
      "Serveravg init(): set_clients()\n",
      "SBSC: iter 0\n",
      "SBSC: iter 1\n",
      "SBSC: iter 2\n",
      "SBSC: iter 3\n",
      "SBSC: iter 4\n",
      "SBSC: iter 5\n",
      "SBSC: iter 6\n",
      "SBSC: iter 7\n",
      "SBSC: iter 8\n",
      "SBSC: iter 9\n",
      "SBSC: iter 10\n",
      "SBSC: iter 11\n",
      "SBSC: iter 12\n",
      "SBSC: iter 13\n",
      "\n",
      "Join ratio / total clients: 0.2 / 14\n",
      "Finished creating server and clients.\n"
     ]
    }
   ],
   "source": [
    "time_list = []\n",
    "#reporter = MemReporter()\n",
    "model_str = args.model\n",
    "\n",
    "# Switched args.prev to 0 since it wasn't working\n",
    "#for i in range(0, args.times):\n",
    "print(f\"\\n============= Running time: {0}th =============\")\n",
    "print(\"Creating server and clients ...\")\n",
    "start = time.time()\n",
    "\n",
    "# Generate args.model\n",
    "args.model = torch.nn.Linear(args.pca_channels, 2)  #input_size, output_size\n",
    "\n",
    "print(args.model)\n",
    "\n",
    "# select algorithm\n",
    "if args.algorithm == \"FedAvg\":\n",
    "    server = FedAvg(args, 0)\n",
    "elif args.algorithm == \"Local\":\n",
    "    server = Local(args, 0)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "#server.train()\n",
    "\n",
    "#time_list.append(time.time()-start)\n",
    "#print(f\"\\nAverage time cost: {round(np.average(time_list), 2)}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68086a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------Round number: 0-------------\n",
      "Selected client IDs: [6, 11]\n"
     ]
    }
   ],
   "source": [
    "server.selected_clients = server.clients\n",
    "with torch.no_grad():\n",
    "    # subscript global_model with [0] if it is sequential instead of linear model --> does that return just the first layer then?\n",
    "    server.global_model.weight.fill_(0)\n",
    "\n",
    "#for i in range(self.global_rounds+1):\n",
    "if 0%server.eval_gap == 0:\n",
    "    print(f\"\\n-------------Round number: {0}-------------\")\n",
    "    if 0!=0:\n",
    "        print(\"\\nEvaluate personalized models\")\n",
    "        server.evaluate()\n",
    "\n",
    "        #print(f\"len: {len(self.rs_train_loss[-1])}\")\n",
    "        if type(server.rs_train_loss[-1]) in [int, float]:\n",
    "            print(f\"rs_train_loss: {server.rs_train_loss[-1]}\")\n",
    "        else:\n",
    "            print(f\"len: {len(server.rs_train_loss[-1])}\")\n",
    "        print()\n",
    "\n",
    "server.selected_clients = server.select_clients()\n",
    "print(f\"Selected client IDs: {[client.ID for client in server.selected_clients]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa18673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"CLIENT TRAINING\")\n",
    "#for client in server.selected_clients:\n",
    "#    client.train()\n",
    "#    print(f\"Client{client.ID} loss: {client.loss_log[-1]:0,.3f}\")\n",
    "\n",
    "my_client = server.selected_clients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc17c34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.local_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430d1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(self):\n",
    "trainloader = my_client.load_train_data()\n",
    "# self.model.to(self.device)\n",
    "my_client.model.train()\n",
    "\n",
    "# differential privacy\n",
    "#if self.privacy:\n",
    "#    self.model, self.optimizer, trainloader, privacy_engine = \\\n",
    "#        initialize_dp(self.model, self.optimizer, trainloader, self.dp_sigma)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "max_local_steps = my_client.local_epochs\n",
    "#if self.train_slow:\n",
    "#    max_local_steps = np.random.randint(1, max_local_steps // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24dde6",
   "metadata": {},
   "source": [
    "## Default Trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8f5c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(trainloader):\n",
    "    print(f\"Batch {i}: x has size {x.size()}; y has size {y.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0275b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "base_data_path = 'C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\personalization-privacy-risk\\\\Data\\\\Client_Specific_Files\\\\'\n",
    "client = clientAVG(server.args, \n",
    "                    ID=i, \n",
    "                    train_samples = base_data_path + \"UserID\" + str(i) + \"_TrainData_8by20770by64.npy\", \n",
    "                    test_samples = base_data_path + \"UserID\" + str(i) + \"_Labels_8by20770by2.npy\", \n",
    "                    train_slow=False, \n",
    "                    send_slow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca38014c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.test_split_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e03f657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.test_split_each_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "68e9ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(my_client.ID)\n",
    "my_client.condition_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7b806988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.42677364 14.68297396 14.68297396 16.38068939 16.38068939 16.38068939\n",
      " 16.59242395  9.08707751  9.08707751  9.08707751]\n",
      "[10.42677364 14.68297396 14.68297396 16.38068939 16.38068939 16.38068939\n",
      " 16.59242395  9.08707751  9.08707751  9.08707751]\n"
     ]
    }
   ],
   "source": [
    "# FROM CLIENTBASE.PY\n",
    "\n",
    "#def load_train_data(self, batch_size=None):\n",
    "batch_size=None\n",
    "# Load full client dataasets\n",
    "#if my_client.local_round == 0:\n",
    "\n",
    "###########################################################################################\n",
    "#self._load_train_data()   # Returns nothing, sets self variables\n",
    "# Load in client's data\n",
    "with open(my_client.samples_path, 'rb') as handle:\n",
    "    samples_npy = np.load(handle)\n",
    "    print(samples_npy[0,:10,0])\n",
    "with open(my_client.labels_path, 'rb') as handle:\n",
    "    labels_npy = np.load(handle)\n",
    "# Select for given condition #THIS IS THE ACTUAL TRAINING DATA AND LABELS FOR THE GIVEN TRIAL\n",
    "my_client.cond_samples_npy = samples_npy[my_client.condition_number,:,:]\n",
    "print(my_client.cond_samples_npy[:10,0])\n",
    "my_client.cond_labels_npy = labels_npy[my_client.condition_number,:,:]\n",
    "# Split data into train and test sets\n",
    "testsplit_upper_bound = round((1-my_client.test_split_fraction)*(my_client.cond_samples_npy.shape[0]))\n",
    "# Set the number of examples (used to be done on init) --> ... THIS IS ABOUT TRAIN/TEST SPLIT\n",
    "my_client.train_samples = testsplit_upper_bound\n",
    "my_client.test_samples = my_client.cond_samples_npy.shape[0] - testsplit_upper_bound\n",
    "train_test_update_number_split = min(my_client.update_ix, key=lambda x:abs(x-testsplit_upper_bound))\n",
    "my_client.max_training_update_upbound = my_client.update_ix.index(train_test_update_number_split)\n",
    "###########################################################################################\n",
    "\n",
    "# Why is this in local_round=0...\n",
    "#if my_client.current_update < my_client.max_training_update_upbound:\n",
    "#    my_client.update_lower_bound = my_client.update_ix[my_client.current_update]\n",
    "#    my_client.update_upper_bound = my_client.update_ix[my_client.current_update+1]\n",
    "#else:\n",
    "#    ...\n",
    "# I just added this, should really be idx bound not update bound...\n",
    "my_client.update_lower_bound = my_client.update_ix[my_client.current_update]\n",
    "my_client.update_upper_bound = my_client.update_ix[my_client.current_update+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf2cbe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update lower bound: 0\n",
      "update upper bound: 1200\n"
     ]
    }
   ],
   "source": [
    "print(f\"update lower bound: {my_client.update_lower_bound}\")\n",
    "print(f\"update upper bound: {my_client.update_upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d721c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.local_round += 1\n",
    "# Check if you need to advance the update\n",
    "# ---> THIS IMPLIES THAT I AM CREATING A NEW TRAINING LOADER FOR EACH UPDATE...\n",
    "# Uh why is 16 hardcoded...\n",
    "# This is the update logic\n",
    "if (my_client.local_round>1) and (my_client.current_update < 16) and (my_client.local_round%my_client.local_round_threshold==0):\n",
    "    my_client.current_update += 1\n",
    "    print(f\"Client{my_client.ID} advances to update {my_client.current_update}\")\n",
    "    # Slice the full client dataset based on the current update number\n",
    "    if my_client.current_update < my_client.max_training_update_upbound:\n",
    "        my_client.update_lower_bound = my_client.update_ix[my_client.current_update]\n",
    "        my_client.update_upper_bound = my_client.update_ix[my_client.current_update+1]\n",
    "    else:\n",
    "        my_client.update_lower_bound = my_client.max_training_update_upbound - 1\n",
    "        my_client.update_upper_bound = my_client.max_training_update_upbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae64ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Dataset Obj\n",
    "# Uhhhh is this creating a new one each time? As long as its not re-reading in the data it probably doesn't matter...\n",
    "#train_data = read_client_data(self.dataset, self.ID, self.current_update, is_train=True)  # Original code\n",
    "#CustomEMGDataset(emgs_block1[my_user][condition_idx,update_lower_bound:update_upper_bound,:], refs_block1[my_user][condition_idx,update_lower_bound:update_upper_bound,:])\n",
    "training_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:], my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:])\n",
    "X_data = torch.Tensor(training_dataset_obj['x']).type(torch.float32)\n",
    "y_data = torch.Tensor(training_dataset_obj['y']).type(torch.float32)\n",
    "training_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7b90c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:], my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edbc9978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70c65e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dacefc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(training_dataset_obj['x']).type(torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b13500a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_for_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "979ea185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_for_dataloader[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b2ad6",
   "metadata": {},
   "source": [
    "That all looks fine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd3136a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataloader\n",
    "if batch_size == None:\n",
    "    batch_size = my_client.batch_size\n",
    "trainloader = DataLoader(\n",
    "    dataset=training_data_for_dataloader,\n",
    "    batch_size=batch_size, \n",
    "    drop_last=False,  # Yah idk if this should be true or false or if it matters...\n",
    "    shuffle=False) \n",
    "#return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b47cacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_samples_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04f5035e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20770, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_labels_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9e53793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(trainloader):\n",
    "    print(f\"Batch {i}: x has size {x.size()}; y has size {y.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62c252de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18370"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200*15+370"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88fe7d",
   "metadata": {},
   "source": [
    "It shouldn't be going through all the batches at once right... it should only be going through the first updates worth...\n",
    "- There's 16 batches, but 18 updates... looks like it held the last few batches out for testing? \n",
    "- Don't really wanna test on the actual last batch\n",
    "- Also shouldn't the training updates by broken up perfectly and not with some leftover? Code isn't working as expected...\n",
    "- Does simulate data streaming have any affect? NO\n",
    "- Where was it in the code that I kept auto-remaking trainloaders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01a320d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(update_ix)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bc54b",
   "metadata": {},
   "source": [
    "Uhh is this code only used once lol\n",
    "- It's literaly only used in the client init..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fb0d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before this I need to run the INIT update segmentation code...\n",
    "#init_dl = self.load_train_data()\n",
    "#self.simulate_data_streaming(init_dl)\n",
    "# ^ This func sets F, V, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bdbdb",
   "metadata": {},
   "source": [
    "## Load Testing Data\n",
    "update setting code SHOULD NOT be in test data (unless each update has its own separate test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1307677",
   "metadata": {},
   "source": [
    "CORRECTION TO LOAD_TEST_DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f66ca6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED\n",
    "#def load_test_data(self, batch_size=None):\n",
    "batch_size=None\n",
    "# Make sure this runs AFTER load_train_data so the data is already loaded in\n",
    "if batch_size == None:\n",
    "    batch_size = my_client.batch_size\n",
    "\n",
    "##########################################################################\n",
    "# Idk if update_ix would need self or if it even exists within the object yet lmao\n",
    "my_client.test_split_idx = update_ix[my_client.max_training_update_upbound]\n",
    "##########################################################################\n",
    "    \n",
    "#test_data = read_client_data(self.dataset, self.ID, self.current_update, is_train=False)\n",
    "testing_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.test_split_idx:,:], my_client.cond_labels_npy[my_client.test_split_idx:,:])\n",
    "X_data = torch.Tensor(testing_dataset_obj['x']).type(torch.float32)\n",
    "y_data = torch.Tensor(testing_dataset_obj['y']).type(torch.float32)\n",
    "testing_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]\n",
    "\n",
    "correctedtestloader = DataLoader(\n",
    "    dataset=testing_data_for_dataloader,\n",
    "    batch_size=batch_size, \n",
    "    drop_last=False,  # Yah idk if this should be true or false or if it matters...\n",
    "    shuffle=False) \n",
    "#return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58241c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 1: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 2: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n",
      "Batch 3: x has size torch.Size([344, 64]); y has size torch.Size([344, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(correctedtestloader):\n",
    "    print(f\"Batch {i}: x has size {x.size()}; y has size {y.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f63f1",
   "metadata": {},
   "source": [
    "Seems like it fixed it... \n",
    "- Do I wanna just drop the last batch? It might mess things up since it's not the same size\n",
    "- Idk what would happen in real-time trials though if everything has to be set up into uniform sized buckets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070f532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "669e6645",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d82c48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting x, y back to training data!\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(trainloader):\n",
    "    print(\"Resetting x, y back to training data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b72bf210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54b0dca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afe96213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, pair 0 in traindl: x.size(): torch.Size([1200, 64])\n"
     ]
    }
   ],
   "source": [
    "# WHICH OF THESE LOOPS IS EQUIVALENT TO MY EPOCHS...\n",
    "running_num_samples = 0\n",
    "step = 0\n",
    "#for step in range(max_local_steps):  # I'm assuming this is gradient steps?... are local epochs the same as gd steps?\n",
    "#for i, (x, y) in enumerate(trainloader):  # This is all the data in a given batch, I think? Can I just kill this... PITA\n",
    "print(f\"Step {step}, pair {i} in traindl: x.size(): {x.size()}\")\n",
    "if type(x) == type([]):\n",
    "    x[0] = x[0].to(my_client.device)\n",
    "else:\n",
    "    x = x.to(my_client.device)\n",
    "y = y.to(my_client.device)\n",
    "#if self.train_slow:\n",
    "#    time.sleep(0.1 * np.abs(np.random.rand()))\n",
    "output = my_client.model(x)\n",
    "#print(f\"clientAVG ----> Training LOSS {i}\")  # What is this even tellimg me lol\n",
    "loss = my_client.loss(output, y, my_client.model)\n",
    "t1 = my_client.loss.term1_error.item()\n",
    "t2 = my_client.loss.term2_ld_decnorm.item()\n",
    "t3 = my_client.loss.term3_lf_emgnorm.item()\n",
    "if np.isnan(t1):\n",
    "    print(\"CLIENTAVG: Error term is None...\")\n",
    "    t1 = -1\n",
    "if np.isnan(t2):\n",
    "    print(\"CLIENTAVG: Decoder Effort term is None...\")\n",
    "    t2 = -1\n",
    "if np.isnan(t3):\n",
    "    print(\"CLIENTAVG: User Effort term is None...\")\n",
    "    t3 = -1\n",
    "#my_client.cost_func_comps_log.append((t1, t2, t3))\n",
    "\n",
    "#my_client.train_time_cost['num_rounds'] += 1\n",
    "#my_client.train_time_cost['total_cost'] += time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7dc74e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[  0.0307,   0.0888],\n",
      "        [  0.0307,   0.0888],\n",
      "        [ -0.3059,   0.6793],\n",
      "        ...,\n",
      "        [-16.1558,   0.5281],\n",
      "        [-20.8682,  -0.0383],\n",
      "        [-20.8682,  -0.0383]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "output - y: tensor([[ 3.0684e-02,  8.8819e-02],\n",
      "        [ 3.0684e-02,  8.8819e-02],\n",
      "        [-3.0591e-01,  6.7933e-01],\n",
      "        ...,\n",
      "        [-3.5156e+01, -9.4719e+00],\n",
      "        [-3.9868e+01, -1.0038e+01],\n",
      "        [-3.9868e+01, -1.0038e+01]], grad_fn=<SubBackward0>)\n",
      "\n",
      "sum of differences between output and y [sum over 1200 samples, 2 cols]: tensor([-21983.8281,  11366.1172], grad_fn=<AddBackward0>)\n",
      "t1: 65.34932708740234\n",
      "t2: 0.0006846926989965141\n"
     ]
    }
   ],
   "source": [
    "print(f\"output: {output}\")\n",
    "print()\n",
    "print(f\"output - y: {output - y}\")\n",
    "print()\n",
    "print(f\"sum of differences between output and y [sum over 1200 samples, 2 cols]: {sum(output - y)}\")\n",
    "print(f\"t1: {t1}\")\n",
    "print(f\"t2: {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c640b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight gradient is None...\n"
     ]
    }
   ],
   "source": [
    "weight_grad = my_client.model.weight.grad\n",
    "if weight_grad == None:\n",
    "    print(\"Weight gradient is None...\")\n",
    "    my_client.gradient_norm_log.append(-1)\n",
    "else:\n",
    "    #grad_norm = torch.linalg.norm(self.model.weight.grad, ord='fro')\n",
    "    grad_norm = np.linalg.norm(my_client.model.weight.grad.detach().numpy())\n",
    "    my_client.gradient_norm_log.append(grad_norm)\n",
    "#my_client.loss_log.append(loss.item())\n",
    "#running_num_samples += x.size(0)\n",
    "my_client.optimizer.zero_grad()\n",
    "loss.backward()\n",
    "my_client.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2b9afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[  0.0307,   0.0888],\n",
      "        [  0.0307,   0.0888],\n",
      "        [ -0.3059,   0.6793],\n",
      "        ...,\n",
      "        [-16.1558,   0.5281],\n",
      "        [-20.8682,  -0.0383],\n",
      "        [-20.8682,  -0.0383]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "output - y: tensor([[ 3.0684e-02,  8.8819e-02],\n",
      "        [ 3.0684e-02,  8.8819e-02],\n",
      "        [-3.0591e-01,  6.7933e-01],\n",
      "        ...,\n",
      "        [-3.5156e+01, -9.4719e+00],\n",
      "        [-3.9868e+01, -1.0038e+01],\n",
      "        [-3.9868e+01, -1.0038e+01]], grad_fn=<SubBackward0>)\n",
      "\n",
      "sum of differences between output and y [sum over 1200 samples, 2 cols]: tensor([-21983.8281,  11366.1172], grad_fn=<AddBackward0>)\n",
      "t1: 65.34932708740234\n",
      "t2: 0.0006846926989965141\n"
     ]
    }
   ],
   "source": [
    "print(f\"output: {output}\")\n",
    "print()\n",
    "print(f\"output - y: {output - y}\")\n",
    "print()\n",
    "print(f\"sum of differences between output and y [sum over 1200 samples, 2 cols]: {sum(output - y)}\")\n",
    "print(f\"t1: {t1}\")\n",
    "print(f\"t2: {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c684930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, pair 0 in traindl: x.size(): torch.Size([1200, 64])\n",
      "output: tensor([[ 5.5665e-02,  7.3136e-02],\n",
      "        [ 5.5665e-02,  7.3136e-02],\n",
      "        [ 7.6774e+01, -4.8346e+01],\n",
      "        ...,\n",
      "        [ 7.5202e+02, -4.6749e+02],\n",
      "        [ 9.3611e+02, -5.8086e+02],\n",
      "        [ 9.3611e+02, -5.8086e+02]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "output - y: tensor([[ 5.5665e-02,  7.3136e-02],\n",
      "        [ 5.5665e-02,  7.3136e-02],\n",
      "        [ 7.6774e+01, -4.8346e+01],\n",
      "        ...,\n",
      "        [ 7.3302e+02, -4.7749e+02],\n",
      "        [ 9.1711e+02, -5.9086e+02],\n",
      "        [ 9.1711e+02, -5.9086e+02]], grad_fn=<SubBackward0>)\n",
      "\n",
      "sum of differences between output and y [sum over 1200 samples, 2 cols]: tensor([1099143.0000, -702366.3750], grad_fn=<AddBackward0>)\n",
      "t1: 231167.40625\n",
      "t2: 0.036918941885232925\n"
     ]
    }
   ],
   "source": [
    "print(f\"Step {step}, pair {i} in traindl: x.size(): {x.size()}\")\n",
    "if type(x) == type([]):\n",
    "    x[0] = x[0].to(my_client.device)\n",
    "else:\n",
    "    x = x.to(my_client.device)\n",
    "y = y.to(my_client.device)\n",
    "#if self.train_slow:\n",
    "#    time.sleep(0.1 * np.abs(np.random.rand()))\n",
    "output = my_client.model(x)\n",
    "#print(f\"clientAVG ----> Training LOSS {i}\")  # What is this even tellimg me lol\n",
    "loss = my_client.loss(output, y, my_client.model)\n",
    "t1 = my_client.loss.term1_error.item()\n",
    "t2 = my_client.loss.term2_ld_decnorm.item()\n",
    "t3 = my_client.loss.term3_lf_emgnorm.item()\n",
    "if np.isnan(t1):\n",
    "    print(\"CLIENTAVG: Error term is None...\")\n",
    "    t1 = -1\n",
    "if np.isnan(t2):\n",
    "    print(\"CLIENTAVG: Decoder Effort term is None...\")\n",
    "    t2 = -1\n",
    "if np.isnan(t3):\n",
    "    print(\"CLIENTAVG: User Effort term is None...\")\n",
    "    t3 = -1\n",
    "    \n",
    "print(f\"output: {output}\")\n",
    "print()\n",
    "print(f\"output - y: {output - y}\")\n",
    "print()\n",
    "print(f\"sum of differences between output and y [sum over 1200 samples, 2 cols]: {sum(output - y)}\")\n",
    "print(f\"t1: {t1}\")\n",
    "print(f\"t2: {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57eb5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_grad = my_client.model.weight.grad\n",
    "if weight_grad == None:\n",
    "    print(\"Weight gradient is None...\")\n",
    "    my_client.gradient_norm_log.append(-1)\n",
    "else:\n",
    "    #grad_norm = torch.linalg.norm(self.model.weight.grad, ord='fro')\n",
    "    grad_norm = np.linalg.norm(my_client.model.weight.grad.detach().numpy())\n",
    "    my_client.gradient_norm_log.append(grad_norm)\n",
    "#my_client.loss_log.append(loss.item())\n",
    "#running_num_samples += x.size(0)\n",
    "my_client.optimizer.zero_grad()\n",
    "loss.backward()\n",
    "my_client.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8d11fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 5.5665e-02,  7.3136e-02],\n",
      "        [ 5.5665e-02,  7.3136e-02],\n",
      "        [ 7.6774e+01, -4.8346e+01],\n",
      "        ...,\n",
      "        [ 7.5202e+02, -4.6749e+02],\n",
      "        [ 9.3611e+02, -5.8086e+02],\n",
      "        [ 9.3611e+02, -5.8086e+02]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "output - y: tensor([[ 5.5665e-02,  7.3136e-02],\n",
      "        [ 5.5665e-02,  7.3136e-02],\n",
      "        [ 7.6774e+01, -4.8346e+01],\n",
      "        ...,\n",
      "        [ 7.3302e+02, -4.7749e+02],\n",
      "        [ 9.1711e+02, -5.9086e+02],\n",
      "        [ 9.1711e+02, -5.9086e+02]], grad_fn=<SubBackward0>)\n",
      "\n",
      "sum of differences between output and y [sum over 1200 samples, 2 cols]: tensor([1099143.0000, -702366.3750], grad_fn=<AddBackward0>)\n",
      "t1: 231167.40625\n",
      "t2: 0.036918941885232925\n"
     ]
    }
   ],
   "source": [
    "print(f\"output: {output}\")\n",
    "print()\n",
    "print(f\"output - y: {output - y}\")\n",
    "print()\n",
    "print(f\"sum of differences between output and y [sum over 1200 samples, 2 cols]: {sum(output - y)}\")\n",
    "print(f\"t1: {t1}\")\n",
    "print(f\"t2: {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2ced309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, pair 0 in traindl: x.size(): torch.Size([1200, 64])\n",
      "output: tensor([[-1.4554e+00,  1.0354e+00],\n",
      "        [-1.4554e+00,  1.0354e+00],\n",
      "        [-4.7664e+03,  3.0408e+03],\n",
      "        ...,\n",
      "        [-4.7783e+04,  3.0324e+04],\n",
      "        [-5.9583e+04,  3.7796e+04],\n",
      "        [-5.9583e+04,  3.7796e+04]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "output - y: tensor([[-1.4554e+00,  1.0354e+00],\n",
      "        [-1.4554e+00,  1.0354e+00],\n",
      "        [-4.7664e+03,  3.0408e+03],\n",
      "        ...,\n",
      "        [-4.7802e+04,  3.0314e+04],\n",
      "        [-5.9602e+04,  3.7786e+04],\n",
      "        [-5.9602e+04,  3.7786e+04]], grad_fn=<SubBackward0>)\n",
      "\n",
      "sum of differences between output and y [sum over 1200 samples, 2 cols]: tensor([-69182904.,  44109432.], grad_fn=<AddBackward0>)\n",
      "t1: 908618048.0\n",
      "t2: 142.64292907714844\n"
     ]
    }
   ],
   "source": [
    "print(f\"Step {step}, pair {i} in traindl: x.size(): {x.size()}\")\n",
    "if type(x) == type([]):\n",
    "    x[0] = x[0].to(my_client.device)\n",
    "else:\n",
    "    x = x.to(my_client.device)\n",
    "y = y.to(my_client.device)\n",
    "#if self.train_slow:\n",
    "#    time.sleep(0.1 * np.abs(np.random.rand()))\n",
    "output = my_client.model(x)\n",
    "#print(f\"clientAVG ----> Training LOSS {i}\")  # What is this even tellimg me lol\n",
    "loss = my_client.loss(output, y, my_client.model)\n",
    "t1 = my_client.loss.term1_error.item()\n",
    "t2 = my_client.loss.term2_ld_decnorm.item()\n",
    "t3 = my_client.loss.term3_lf_emgnorm.item()\n",
    "if np.isnan(t1):\n",
    "    print(\"CLIENTAVG: Error term is None...\")\n",
    "    t1 = -1\n",
    "if np.isnan(t2):\n",
    "    print(\"CLIENTAVG: Decoder Effort term is None...\")\n",
    "    t2 = -1\n",
    "if np.isnan(t3):\n",
    "    print(\"CLIENTAVG: User Effort term is None...\")\n",
    "    t3 = -1\n",
    "    \n",
    "weight_grad = my_client.model.weight.grad\n",
    "if weight_grad == None:\n",
    "    print(\"Weight gradient is None...\")\n",
    "    my_client.gradient_norm_log.append(-1)\n",
    "else:\n",
    "    grad_norm = np.linalg.norm(my_client.model.weight.grad.detach().numpy())\n",
    "    my_client.gradient_norm_log.append(grad_norm)\n",
    "my_client.optimizer.zero_grad()\n",
    "loss.backward()\n",
    "my_client.optimizer.step()\n",
    "    \n",
    "print(f\"output: {output}\")\n",
    "print()\n",
    "print(f\"output - y: {output - y}\")\n",
    "print()\n",
    "print(f\"sum of differences between output and y [sum over 1200 samples, 2 cols]: {sum(output - y)}\")\n",
    "print(f\"t1: {t1}\")\n",
    "print(f\"t2: {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fc765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "215b5a78",
   "metadata": {},
   "source": [
    "# ALTERNATIVE VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5af8829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 677238.75 \n",
      "Epoch: 1 | Loss: 103747800.0 \n",
      "Epoch: 2 | Loss: 829558685696.0 \n",
      "Epoch: 3 | Loss: 7786028383338496.0 \n",
      "Epoch: 4 | Loss: 7.361807371662577e+19 \n",
      "Epoch: 5 | Loss: 6.962956296650413e+23 \n",
      "Epoch: 6 | Loss: 6.585809935826766e+27 \n",
      "Epoch: 7 | Loss: 6.229092318903552e+31 \n",
      "Epoch: 8 | Loss: 5.891698516480374e+35 \n",
      "Epoch: 9 | Loss: inf \n"
     ]
    }
   ],
   "source": [
    "# our model\n",
    "model = torch.nn.Linear(64, 2)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "# Making LR tiny slows it down but it still blows up...\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for i, (x_data, y_data) in enumerate(trainloader):\n",
    "        # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_data)\n",
    "\n",
    "        # 2) Compute and print loss\n",
    "        loss = criterion(y_pred, y_data)\n",
    "        print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e3d0d3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 20770, 64)\n",
      "(8, 20770, 2)\n"
     ]
    }
   ],
   "source": [
    "#refs_block1, _, _, _, emgs_block1, _, _, _, _, _, _ = pickle.load(handle)\n",
    "print(emgs_block1[keys[0]].shape)\n",
    "print(refs_block1[keys[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "066b9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli0_cond1_samples = emgs_block1[keys[0]][0, :, :]\n",
    "cli0_cond1_labels = refs_block1[keys[0]][0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b3ad6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 26502288.0 \n",
      "Epoch: 1 | Loss: 3441264699637760.0 \n",
      "Epoch: 2 | Loss: 1.2555818147174948e+24 \n",
      "Epoch: 3 | Loss: 4.582583999763409e+32 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: nan \n"
     ]
    }
   ],
   "source": [
    "# our model\n",
    "model = torch.nn.Linear(64, 2)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)\n",
    "# Making LR tiny slows it down but it still blows up...\n",
    "\n",
    "x_data = torch.tensor(cli0_cond1_samples, dtype=torch.float32)\n",
    "y_data = torch.tensor(cli0_cond1_labels, dtype=torch.float32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    #for i, (x_data, y_data) in enumerate(trainloader):\n",
    "    \n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae254810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a44eeb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIENT0\n",
      "Epoch: 0 | Loss: 24290276.0 \n",
      "Epoch: 1 | Loss: 2936180303724544.0 \n",
      "Epoch: 2 | Loss: 1.0715106186900139e+24 \n",
      "Epoch: 3 | Loss: 3.910772509543822e+32 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT1\n",
      "Epoch: 0 | Loss: 31997452.0 \n",
      "Epoch: 1 | Loss: 7.60232833712128e+16 \n",
      "Epoch: 2 | Loss: 4.857631661008698e+26 \n",
      "Epoch: 3 | Loss: 3.1054302270356055e+36 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: nan \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT2\n",
      "Epoch: 0 | Loss: 16203730.0 \n",
      "Epoch: 1 | Loss: 3074357198848.0 \n",
      "Epoch: 2 | Loss: 5.72363532173679e+19 \n",
      "Epoch: 3 | Loss: 1.360310131660171e+27 \n",
      "Epoch: 4 | Loss: 3.2437940919801105e+34 \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: inf \n",
      "\n",
      "CLIENT3\n",
      "Epoch: 0 | Loss: 328153216.0 \n",
      "Epoch: 1 | Loss: 1.1020855405161585e+20 \n",
      "Epoch: 2 | Loss: 4.0373252821927207e+31 \n",
      "Epoch: 3 | Loss: inf \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: nan \n",
      "Epoch: 8 | Loss: nan \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT4\n",
      "Epoch: 0 | Loss: 28052824.0 \n",
      "Epoch: 1 | Loss: 631670973661184.0 \n",
      "Epoch: 2 | Loss: 3.138726114220637e+22 \n",
      "Epoch: 3 | Loss: 1.5597773997843653e+30 \n",
      "Epoch: 4 | Loss: 7.751259518034906e+37 \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: inf \n",
      "\n",
      "CLIENT5\n",
      "Epoch: 0 | Loss: 20859348.0 \n",
      "Epoch: 1 | Loss: 604112919986176.0 \n",
      "Epoch: 2 | Loss: 1.0774869314243315e+23 \n",
      "Epoch: 3 | Loss: 1.9286170103481843e+31 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT6\n",
      "Epoch: 0 | Loss: 18202284.0 \n",
      "Epoch: 1 | Loss: 557102489665536.0 \n",
      "Epoch: 2 | Loss: 2.6903297209211208e+23 \n",
      "Epoch: 3 | Loss: 1.3386601948785568e+32 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: inf \n",
      "\n",
      "CLIENT7\n",
      "Epoch: 0 | Loss: 34644544.0 \n",
      "Epoch: 1 | Loss: 2.590955986432819e+16 \n",
      "Epoch: 2 | Loss: 3.4128985293556695e+25 \n",
      "Epoch: 3 | Loss: 4.595242338995614e+34 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: nan \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT8\n",
      "Epoch: 0 | Loss: 19999928.0 \n",
      "Epoch: 1 | Loss: 1191114421829632.0 \n",
      "Epoch: 2 | Loss: 3.29187948631667e+23 \n",
      "Epoch: 3 | Loss: 9.107017120178305e+31 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT9\n",
      "Epoch: 0 | Loss: 16718649.0 \n",
      "Epoch: 1 | Loss: 56422922452992.0 \n",
      "Epoch: 2 | Loss: 8.591705597369091e+20 \n",
      "Epoch: 3 | Loss: 1.3083603224554186e+28 \n",
      "Epoch: 4 | Loss: 1.992399724206964e+35 \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: inf \n",
      "\n",
      "CLIENT10\n",
      "Epoch: 0 | Loss: 13025966.0 \n",
      "Epoch: 1 | Loss: 40913444864.0 \n",
      "Epoch: 2 | Loss: 2.681353408652247e+17 \n",
      "Epoch: 3 | Loss: 1.8085317593534113e+24 \n",
      "Epoch: 4 | Loss: 1.2198530583129619e+31 \n",
      "Epoch: 5 | Loss: 8.2278920872388e+37 \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: inf \n",
      "\n",
      "CLIENT11\n",
      "Epoch: 0 | Loss: 20566472.0 \n",
      "Epoch: 1 | Loss: 2143802592591872.0 \n",
      "Epoch: 2 | Loss: 5.8811859015484966e+23 \n",
      "Epoch: 3 | Loss: 1.6137064865195071e+32 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT12\n",
      "Epoch: 0 | Loss: 13989776.0 \n",
      "Epoch: 1 | Loss: 138237523787776.0 \n",
      "Epoch: 2 | Loss: 1.4797815065623291e+22 \n",
      "Epoch: 3 | Loss: 1.585129934263231e+30 \n",
      "Epoch: 4 | Loss: 1.6979804565479465e+38 \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: nan \n",
      "\n",
      "CLIENT13\n",
      "Epoch: 0 | Loss: 21966392.0 \n",
      "Epoch: 1 | Loss: 708645981519872.0 \n",
      "Epoch: 2 | Loss: 8.356634472728963e+23 \n",
      "Epoch: 3 | Loss: 1.0339552197195286e+33 \n",
      "Epoch: 4 | Loss: inf \n",
      "Epoch: 5 | Loss: inf \n",
      "Epoch: 6 | Loss: inf \n",
      "Epoch: 7 | Loss: inf \n",
      "Epoch: 8 | Loss: inf \n",
      "Epoch: 9 | Loss: nan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(keys)):\n",
    "    print(f\"CLIENT{i}\")\n",
    "\n",
    "    # our model\n",
    "    model = torch.nn.Linear(64, 2)\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)\n",
    "    # Making LR tiny slows it down but it still blows up...\n",
    "\n",
    "    x_data = torch.tensor(emgs_block1[keys[i]][0, :, :], dtype=torch.float32)\n",
    "    y_data = torch.tensor(refs_block1[keys[i]][0, :, :], dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        #for i, (x_data, y_data) in enumerate(trainloader):\n",
    "\n",
    "        # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_data)\n",
    "\n",
    "        # 2) Compute and print loss\n",
    "        loss = criterion(y_pred, y_data)\n",
    "        print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3155c5",
   "metadata": {},
   "source": [
    "I guess try and train an sklearn model on the data? No idea what's going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a79aa8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6645262b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06147221139504461"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.tensor(emgs_block1[keys[0]][0, :testsplit_upper_bound, :], dtype=torch.float32)\n",
    "y_data = torch.tensor(refs_block1[keys[0]][0, :testsplit_upper_bound, :], dtype=torch.float32)\n",
    "\n",
    "reg = LinearRegression().fit(x_data, y_data)\n",
    "reg.score(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "856ec4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.800249    0.23163907]\n",
      " [-2.800249    0.23163907]\n",
      " [-2.800249    0.23163907]]\n",
      "\n",
      "tensor([[-8.5753e-05, -5.6882e-04],\n",
      "        [-8.5753e-05, -5.6882e-04],\n",
      "        [-2.0100e-04, -1.3241e-03]])\n"
     ]
    }
   ],
   "source": [
    "print(reg.predict(x_data[:3,:]))\n",
    "print()\n",
    "print(y_data[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0f29449c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20770, 64])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04660061",
   "metadata": {},
   "source": [
    "Even linear regression using sklearn doesn't work?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92c1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509e04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6abdca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba21af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0132a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPHSLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1821dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import isnan\n",
    "\n",
    "class TroubleShooting_Loss(torch.nn.modules.loss._Loss):\n",
    "    def __init__(self, lambdaF=0, lambdaD=1e-3, lambdaE=1e-6, Nd=2, Ne=64, return_cost_func_comps=False, verbose=False, dt=1/60, normalize_V=False) -> None:\n",
    "        super().__init__()\n",
    "        self.lambdaF = lambdaF\n",
    "        self.lambdaD = lambdaD\n",
    "        self.lambdaE = lambdaE\n",
    "        self.Nd = Nd\n",
    "        self.Ne = Ne\n",
    "        # Don't use return_cost_func_comps since I don't think loss.item() will return a tuple, it only returns scalars AFAIK\n",
    "        self.dt = dt\n",
    "        self.normalize_V = normalize_V\n",
    "        self.verbose = verbose\n",
    "        self.term1_error = 0\n",
    "        self.term2_ld_decnorm = 0\n",
    "        self.term3_lf_emgnorm = 0\n",
    "\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = torch.transpose(outputs, 0, 1)\n",
    "        p_reference = torch.transpose(targets, 0, 1)\n",
    "        p_actual = torch.cumsum(outputs, dim=1)*self.dt  # Numerical integration of v_actual to get p_actual\n",
    "        self.V = (p_reference - p_actual)*self.dt\n",
    "        if self.normalize_V:\n",
    "            self.V = self.V/torch.linalg.norm(self.V, ord='fro')\n",
    "            assert (torch.linalg.norm(self.V, ord='fro')<1.2) and (torch.linalg.norm(self.V, ord='fro')>0.8)\n",
    "        Vplus = self.V[:,1:]\n",
    "        # Performance\n",
    "        return self.lambdaE*(torch.linalg.matrix_norm(outputs[:,:-1] - Vplus)**2)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e219ae14",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TroubleShooting_Loss' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m my_loss_func \u001b[38;5;241m=\u001b[39m TroubleShooting_Loss()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmy_loss_func\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TroubleShooting_Loss' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "my_loss_func = TroubleShooting_Loss()\n",
    "my_loss_func[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba743796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bbca359",
   "metadata": {},
   "source": [
    "# ITS GOTTA BE THE TRAININGLOADER..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "164b267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "880427ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data\\continuous_full_data_block1.pickle\", 'rb') as handle:\n",
    "    #refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1, Hs_block1, alphas_block1, pDs_block1, times_block1, conditions_block1 = pickle.load(handle)\n",
    "    refs_block1, _, _, _, emgs_block1, _, _, _, _, _, _ = pickle.load(handle)\n",
    "    \n",
    "keys = ['METACPHS_S106', 'METACPHS_S107', 'METACPHS_S108', 'METACPHS_S109', 'METACPHS_S110', 'METACPHS_S111', 'METACPHS_S112', 'METACPHS_S113', 'METACPHS_S114', 'METACPHS_S115', 'METACPHS_S116', 'METACPHS_S117', 'METACPHS_S118', 'METACPHS_S119']\n",
    "key_to_num = dict()\n",
    "num_to_key = dict()\n",
    "for idx, key in enumerate(keys):\n",
    "    key_to_num[key] = idx\n",
    "    num_to_key[idx] = key\n",
    "\n",
    "num_conds = 8\n",
    "num_channels = 64\n",
    "num_updates = 19\n",
    "cphs_starting_update = 10\n",
    "update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b99f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0017a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMG Input shape: (8, 20770, 64)\n",
      "2D Velocity Label shape: (8, 20770, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"EMG Input shape: {emgs_block1[keys[my_client_num]].shape}\")\n",
    "print(f\"2D Velocity Label shape: {refs_block1[keys[my_client_num]].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "929c89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Condition 1\")\n",
    "F1tens_full = torch.from_numpy(emgs_block1[keys[my_client_num]][0, :, :])\n",
    "PREF1tens_full = torch.from_numpy(refs_block1[keys[my_client_num]][0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b591e4",
   "metadata": {},
   "source": [
    "VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee5606ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM CLIENTBASE.PY\n",
    "\n",
    "#def load_train_data(self, batch_size=None):\n",
    "batch_size=None\n",
    "# Load full client dataasets\n",
    "###########################################################################################\n",
    "#self._load_train_data()   # Returns nothing, sets self variables\n",
    "# Load in client's data\n",
    "with open(my_client.samples_path, 'rb') as handle:\n",
    "    samples_npy = np.load(handle)\n",
    "with open(my_client.labels_path, 'rb') as handle:\n",
    "    labels_npy = np.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f556749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data\\Client_Specific_Files\\UserID6_TrainData_8by20770by64.npy\n"
     ]
    }
   ],
   "source": [
    "print(my_client.samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd378ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a95bf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20770, 64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emgs_block1[keys[my_client_num]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652361c",
   "metadata": {},
   "source": [
    "Check differences between samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9237bd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli6_cond0_npy = samples_npy[0]\n",
    "cli6_cond0_emg = emgs_block1[keys[my_client_num]][0]\n",
    "sum(sum(cli6_cond0_npy - cli6_cond0_emg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55167af8",
   "metadata": {},
   "source": [
    "Check differences between labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f591667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli6_cond0_labels_npy = labels_npy[0]\n",
    "cli6_cond0_ref = refs_block1[keys[my_client_num]][0]\n",
    "sum(sum(cli6_cond0_labels_npy - cli6_cond0_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535ce34",
   "metadata": {},
   "source": [
    "Do train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae13ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20770\n",
      "16616\n",
      "4154\n",
      "16826\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(cli6_cond0_npy.shape[0])\n",
    "testsplit_upper_bound = round((1-my_client.test_split_fraction)*(cli6_cond0_npy.shape[0]))\n",
    "# Set the number of examples (used to be done on init) --> ... THIS IS ABOUT TRAIN/TEST SPLIT\n",
    "my_client.train_samples = testsplit_upper_bound\n",
    "print(my_client.train_samples)\n",
    "my_client.test_samples = cli6_cond0_npy.shape[0] - testsplit_upper_bound\n",
    "print(my_client.test_samples)\n",
    "train_test_update_number_split = min(my_client.update_ix, key=lambda x:abs(x-testsplit_upper_bound))\n",
    "print(train_test_update_number_split)\n",
    "my_client.max_training_update_upbound = my_client.update_ix.index(train_test_update_number_split)\n",
    "print(my_client.max_training_update_upbound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3569b91",
   "metadata": {},
   "source": [
    "Check out train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d289026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb load_train_data(): Client6: Setting Training DataLoader\n"
     ]
    }
   ],
   "source": [
    "#def load_train_data(self, batch_size=None):\n",
    "    # Load full client dataasets\n",
    "    \n",
    "#_load_train_data()\n",
    "#############################################################################\n",
    "#print(f\"Client{self.ID} loading data file in [SHOULD ONLY RUN ONCE PER CLIENT]\")\n",
    "## Load in client's data\n",
    "#with open(self.samples_path, 'rb') as handle:\n",
    "#    samples_npy = np.load(handle)\n",
    "#with open(self.labels_path, 'rb') as handle:\n",
    "#    labels_npy = np.load(handle)\n",
    "## Select for given condition #THIS IS THE ACTUAL TRAINING DATA AND LABELS FOR THE GIVEN TRIAL\n",
    "#self.cond_samples_npy = samples_npy[self.condition_number,:,:]\n",
    "#self.cond_labels_npy = labels_npy[self.condition_number,:,:]\n",
    "## Split data into train and test sets\n",
    "#if self.test_split_users:\n",
    "#    # NOT FINISHED YET\n",
    "#    # Randomly pick the test_split_fraction of users to be completely held out of training to be used for testing\n",
    "#    num_test_users = round(len(self.clients)*self.test_split_fraction)\n",
    "#    # Pick/sample num_test_users from self.clients to be removed and put into self.testing_clients\n",
    "#    self.testing_clients = [self.clients.pop(random.randrange(len(self.clients))) for _ in range(num_test_users)]\n",
    "#    # Hmmm this requires a full rewrite of the code... \n",
    "#    raise ValueError(\"test_split_users is not fully supported yet\")\n",
    "#elif self.test_split_each_update:\n",
    "#    # Idk this might actually be supported just in a different function. I'm not sure. Don't plan on using it rn so who cares\n",
    "#    raise ValueError(\"test_split_each_update not supported yet.  Idk if this is necessary to add\")\n",
    "#else: \n",
    "#    testsplit_upper_bound = round((1-self.test_split_fraction)*(self.cond_samples_npy.shape[0]))\n",
    "## Set the number of examples (used to be done on init) --> ... THIS IS ABOUT TRAIN/TEST SPLIT\n",
    "#self.train_samples = testsplit_upper_bound\n",
    "#self.test_samples = self.cond_samples_npy.shape[0] - testsplit_upper_bound\n",
    "## The below gets stuck in the debugger and just keeps running until you step over\n",
    "#train_test_update_number_split = min(self.update_ix, key=lambda x:abs(x-testsplit_upper_bound))\n",
    "#self.max_training_update_upbound = self.update_ix.index(train_test_update_number_split)\n",
    "my_client.test_split_idx = my_client.update_ix[my_client.max_training_update_upbound]\n",
    "#############################################################################\n",
    "\n",
    "# Do I really want this here...\n",
    "my_client.local_round += 1\n",
    "# Check if you need to advance the update\n",
    "# ---> THIS IMPLIES THAT I AM CREATING A NEW TRAINING LOADER FOR EACH UPDATE... this is what I want actually I think\n",
    "if (my_client.local_round>1) and (my_client.current_update < 16) and (my_client.local_round%my_client.local_round_threshold==0):\n",
    "    my_client.current_update += 1\n",
    "    print(f\"Client{my_client.ID} advances to update {my_client.current_update}\")\n",
    "# Slice the full client dataset based on the current update number\n",
    "if my_client.current_update < my_client.max_training_update_upbound:\n",
    "    my_client.update_lower_bound = my_client.update_ix[my_client.current_update]\n",
    "    my_client.update_upper_bound = my_client.update_ix[my_client.current_update+1]\n",
    "else:\n",
    "    my_client.update_lower_bound = my_client.max_training_update_upbound - 1\n",
    "    my_client.update_upper_bound = my_client.max_training_update_upbound\n",
    "# Set the Dataset Obj\n",
    "# Uhhhh is this creating a new one each time? As long as its not re-reading in the data it probably doesn't matter...\n",
    "#train_data = read_client_data(self.dataset, self.ID, self.current_update, is_train=True)  # Original code\n",
    "#CustomEMGDataset(emgs_block1[my_user][condition_idx,update_lower_bound:update_upper_bound,:], refs_block1[my_user][condition_idx,update_lower_bound:update_upper_bound,:])\n",
    "training_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:], my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:])\n",
    "X_data = torch.Tensor(training_dataset_obj['x']).type(torch.float32)\n",
    "y_data = torch.Tensor(training_dataset_obj['y']).type(torch.float32)\n",
    "training_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]\n",
    "\n",
    "print(f\"cb load_train_data(): Client{my_client.ID}: Setting Training DataLoader\")\n",
    "# Set dataloader\n",
    "if batch_size == None:\n",
    "    batch_size = my_client.batch_size\n",
    "dl = DataLoader(\n",
    "    dataset=training_data_for_dataloader,\n",
    "    batch_size=batch_size, \n",
    "    drop_last=False,  # Yah idk if this should be true or false or if it matters...\n",
    "    shuffle=False) \n",
    "#return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e841751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 64])\n",
      "torch.Size([1200, 2])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dl:\n",
    "    print(x.size())\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a466927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.1929e+03, -2.5532e+03,  8.6848e+02, -2.1895e+03,  6.5569e+03,\n",
      "        -2.2461e+04,  1.7643e+03,  5.0437e+03,  6.9633e+03,  1.2043e+04,\n",
      "         7.5770e+03,  9.5396e+03,  1.1119e+04,  8.7130e+03,  5.3621e+03,\n",
      "         4.7259e+03, -3.0674e+03, -2.1818e+04, -7.4441e+03,  1.5363e+04,\n",
      "         1.2195e+03, -5.2106e+03, -4.0107e+03, -6.0993e+03, -4.4521e+03,\n",
      "        -1.8632e+03,  1.1350e+03, -4.3047e+02, -4.4105e+02, -1.7976e+03,\n",
      "        -2.1017e+03, -3.0254e+02,  1.4371e+03,  7.8531e+03,  9.2425e+03,\n",
      "         1.0661e+04,  1.3320e+04,  1.0254e+04,  1.0509e+04,  9.0886e+03,\n",
      "         1.0372e+04,  5.9883e+03,  3.3941e+03, -8.9814e+03, -7.4265e+03,\n",
      "         1.4346e+04,  1.4491e+04, -1.4238e+03, -2.1487e+03,  8.0973e+03,\n",
      "         1.1084e+04,  2.1744e+03, -6.2367e+02, -3.7227e+03, -7.5776e+03,\n",
      "        -4.4595e+03,  5.2394e+03,  5.2911e+03,  5.8159e+03,  5.0607e+03,\n",
      "         1.2666e+04,  1.5031e+04,  1.1500e+04,  5.1229e-02],\n",
      "       dtype=torch.float64)\n",
      "tensor(176112.3001, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#cli6_cond0_npy = samples_npy[0]\n",
    "#cli6_cond0_emg = emgs_block1[keys[my_client_num]][0]\n",
    "#sum(sum(cli6_cond0_npy - cli6_cond0_emg))\n",
    "\n",
    "print(sum(x - cli6_cond0_npy[:1200, :]))\n",
    "print(sum(sum(x - cli6_cond0_npy[:1200, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "20bc93fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.9795, 0.9795, 2.5663, 2.5663, 4.0318, 4.0318, 3.9936,\n",
       "        3.9936])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44dd3f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.42677364, 14.68297396, 14.68297396, 16.38068939, 16.38068939,\n",
       "       16.38068939, 16.59242395,  9.08707751,  9.08707751,  9.08707751])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli6_cond0_npy[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "803d28f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "510.56800753619746\n"
     ]
    }
   ],
   "source": [
    "print(sum(x[0,:]))\n",
    "print(sum(cli6_cond0_npy[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "54116f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2140.2960,   311.4015], dtype=torch.float64)\n",
      "tensor(-1828.8945, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#cli6_cond0_labels_npy = labels_npy[0]\n",
    "#cli6_cond0_ref = refs_block1[keys[my_client_num]][0]\n",
    "#sum(sum(cli6_cond0_labels_npy - cli6_cond0_ref))\n",
    "\n",
    "print(sum(y - cli6_cond0_labels_npy[:1200, :]))\n",
    "print(sum(sum(y - cli6_cond0_labels_npy[:1200, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa5ad1",
   "metadata": {},
   "source": [
    "Investing training_data_for_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8ce2b2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_for_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029750c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:], my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:])\n",
    "#X_data = torch.Tensor(training_dataset_obj['x']).type(torch.float32)\n",
    "#y_data = torch.Tensor(training_dataset_obj['y']).type(torch.float32)\n",
    "#training_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0742f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 64])\n",
      "torch.Size([1200, 64])\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.9795, 0.9795, 2.5663, 2.5663, 4.0318, 4.0318, 3.9936,\n",
      "        3.9936])\n",
      "tensor([0.0000, 0.0000, 0.9795, 0.9795, 2.5663, 2.5663, 4.0318, 4.0318, 3.9936,\n",
      "        3.9936])\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset_obj['x'].shape)\n",
    "print(X_data.shape)\n",
    "print()\n",
    "print(training_dataset_obj['x'][:10,0])\n",
    "print(X_data[:10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c34a61",
   "metadata": {},
   "source": [
    "Note taht while X_data and training_dataset_obj['x'] are consistent, they already match the x found in the dataloader and NOT the training data from my npy file... Is my npy file wrong??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30f50b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.update_lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bc2a538a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.update_upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "80f9e873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.97946232, 0.97946232, 2.5663038 ,\n",
       "       2.5663038 , 4.03178426, 4.03178426, 3.99362317, 3.99362317])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.cond_samples_npy[:10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93fc2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_obj = CustomEMGDataset(my_client.cond_samples_npy[my_client.update_lower_bound:my_client.update_upper_bound,:], my_client.cond_labels_npy[my_client.update_lower_bound:my_client.update_upper_bound,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e49ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970669a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c0389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b6f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445918f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f13839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "27065553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200, 2])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_obj['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbaae5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data_for_dataloader = [(x, y) for x, y in zip(X_data, y_data)]\n",
    "for x,y in X_data, y_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7f9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7fa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a2726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed746e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224d6c7f",
   "metadata": {},
   "source": [
    "# CONTINUED CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb599a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "\n",
    "if self.auto_break and self.check_done(acc_lss=[self.rs_test_acc], top_cnt=self.top_cnt):\n",
    "    print(\"Breaking\")\n",
    "    break\n",
    "\n",
    "self.evaluate(train=False, test=True)\n",
    "print(\"\\nBest Loss.\")\n",
    "print(min(self.rs_test_loss))\n",
    "\n",
    "for idx, client in enumerate(self.clients):\n",
    "    #self.cost_func_comps_dict[idx] = client.cost_func_comps_log\n",
    "    #self.gradient_dict[idx] = client.gradient_norm_log\n",
    "    self.cost_func_comps_log.append(client.cost_func_comps_log)\n",
    "    self.gradient_norm_log.append(client.gradient_norm_log)\n",
    "\n",
    "self.save_results(save_cost_func_comps=True, save_gradient=True)\n",
    "model_path = os.path.join(\"models\", self.dataset)\n",
    "model_path = os.path.join(model_path, \"Local\")\n",
    "for client in self.clients:\n",
    "    client.save_item(client.model, 'local_client_model', item_path=model_path)\n",
    "# No idea where this global model is coming from? Why did they save it...\n",
    "self.save_global_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d56043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24caebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7747f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3967eff",
   "metadata": {},
   "source": [
    "## From NB 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training_loop(self):\n",
    "    self.simulate_data_stream()\n",
    "    self.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_stream(self, streaming_method=True):\n",
    "    if streaming_method:\n",
    "        streaming_method = self.data_stream\n",
    "    need_to_advance=True\n",
    "    self.current_round += 1\n",
    "    if self.current_update==16:  #17: previously 17 but the last update is super short so I cut it out\n",
    "        #print(\"Maxxed out your update (you are on update 18), continuing training on last update only\")\n",
    "        # Probably ought to track that we maxed out --> LOG SYSTEM\n",
    "        # We are stopping an update early, so use -3/-2 and not -2/-1 (the last update)\n",
    "        lower_bound = (update_ix[-3] + update_ix[-2])//2  #Use only the second half of each update\n",
    "        upper_bound = update_ix[-2]\n",
    "        self.learning_batch = upper_bound - lower_bound\n",
    "    elif streaming_method=='streaming':\n",
    "        # If we pass threshold, move on to the next update\n",
    "        if self.current_round%self.local_round_threshold==0:\n",
    "            self.current_update += 1\n",
    "\n",
    "            self.update_transition_log.append(self.latest_global_round)\n",
    "            if self.verbose==True and self.ID==1:\n",
    "                print(f\"Client {self.ID}: New update after lrt passed: (new update, current global round, current local round): {self.current_update, self.latest_global_round, self.current_round}\")\n",
    "                print()\n",
    "\n",
    "            # Using only the second half of each update for co-adaptivity reasons\n",
    "            lower_bound = (update_ix[self.current_update] + update_ix[self.current_update+1])//2  \n",
    "            upper_bound = update_ix[self.current_update+1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "        elif self.current_round>2:\n",
    "            # This is the base case\n",
    "            # The update number didn't change so we don't need to overwrite everything with the same data\n",
    "            need_to_advance = False\n",
    "        else:\n",
    "            # This is for the init case (current round is 0 or 1)\n",
    "            # need_to_advance is true, so we overwrite s and such... this is fine \n",
    "            lower_bound = (update_ix[self.current_update] + update_ix[self.current_update+1])//2  \n",
    "            upper_bound = update_ix[self.current_update+1]\n",
    "            self.learning_batch = upper_bound - lower_bound\n",
    "\n",
    "        self.current_update += 1\n",
    "    else:\n",
    "        raise ValueError(f'streaming_method (\"{streaming_method}\") not recognized: this data streaming functionality is not supported')\n",
    "\n",
    "    if need_to_advance:\n",
    "        s_temp = self.training_data[lower_bound:upper_bound,:]\n",
    "        # First, normalize the entire s matrix\n",
    "        if self.normalize_EMG:\n",
    "            s_normed = s_temp/np.amax(s_temp)\n",
    "        else:\n",
    "            s_normed = s_temp\n",
    "        # Now do PCA unless it is set to 64 (AKA the default num channels i.e. no reduction)\n",
    "        # Also probably ought to find a global transform if possible so I don't recompute it every time...\n",
    "        if self.PCA_comps!=self.pca_channel_default:  \n",
    "            pca = PCA(n_components=self.PCA_comps)\n",
    "            s_normed = pca.fit_transform(s_normed)\n",
    "        s = np.transpose(s_normed)\n",
    "        self.F = s[:,:-1] # note: truncate F for estimate_decoder\n",
    "        v_actual = self.w@s\n",
    "        p_actual = np.cumsum(v_actual, axis=1)*self.dt  # Numerical integration of v_actual to get p_actual\n",
    "        p_reference = np.transpose(self.labels[lower_bound:upper_bound,:])\n",
    "\n",
    "        #####################################################################\n",
    "        # Add the boundary conditions code here\n",
    "        if self.use_zvel:\n",
    "            # Maneeshika code\n",
    "            p_ref_lim = self.labels[lower_bound:upper_bound,:]\n",
    "            if self.current_round<2:\n",
    "                self.vel_est = np.zeros_like((p_ref_lim))\n",
    "                self.pos_est = np.zeros_like((p_ref_lim))\n",
    "                self.int_vel_est = np.zeros_like((p_ref_lim))\n",
    "                self.vel_est[0] = self.w@s[:,0]  # Translated from: Ds_fixed@emg_tr[0]\n",
    "                self.pos_est[0] = [0, 0]\n",
    "            else:\n",
    "                prev_vel_est = self.vel_est[-1]\n",
    "                prev_pos_est = self.pos_est[-1]\n",
    "\n",
    "                self.vel_est = np.zeros_like((p_ref_lim))\n",
    "                self.pos_est = np.zeros_like((p_ref_lim))\n",
    "                self.int_vel_est = np.zeros_like((p_ref_lim))\n",
    "\n",
    "                self.vel_est[0] = prev_vel_est\n",
    "                self.pos_est[0] = prev_pos_est\n",
    "            for tt in range(1, s.shape[1]):\n",
    "                # Note this does not keep track of actual updates, only the range of 1 to s.shape[1] (1202ish)\n",
    "                vel_plus = self.w@s[:,tt]  # Translated from: Ds_fixed@emg_tr[tt]\n",
    "                p_plus = self.pos_est[tt-1, :] + (self.vel_est[tt-1, :]*self.dt)\n",
    "                # These are just correctives, such that vel_plus can get bounded\n",
    "                # x-coordinate\n",
    "                if abs(p_plus[0]) > 36:  # 36 hardcoded from earlier works\n",
    "                    p_plus[0] = self.pos_est[tt-1, 0]\n",
    "                    vel_plus[0] = 0\n",
    "                    self.hit_bound += 1 # update hit_bound counter\n",
    "                if abs(p_plus[1]) > 24:  # 24 hardcoded from earlier works\n",
    "                    p_plus[1] = self.pos_est[tt-1, 1]\n",
    "                    vel_plus[1] = 0\n",
    "                    self.hit_bound += 1 # update hit_bound counter\n",
    "                if self.hit_bound > 200:  # 200 hardcoded from earlier works\n",
    "                    p_plus[0] = 0\n",
    "                    vel_plus[0] = 0\n",
    "                    p_plus[1] = 0\n",
    "                    vel_plus[1] = 0\n",
    "                    self.hit_bound = 0\n",
    "                # now update velocity and position\n",
    "                self.vel_est[tt] = vel_plus\n",
    "                self.pos_est[tt] = p_plus\n",
    "                # calculate intended velocity\n",
    "                self.int_vel_est[tt] = calculate_intended_vels(p_ref_lim[tt], p_plus, 1/self.dt)\n",
    "\n",
    "            self.V = np.transpose(self.int_vel_est[:tt+1])\n",
    "            #print(f\"V.shape: {self.V.shape}\")\n",
    "        else:\n",
    "            # Original code\n",
    "            self.V = (p_reference - p_actual)*self.dt\n",
    "\n",
    "        \n",
    "def train_given_model_1_comm_round(self, model, which):\n",
    "    '''This can be used for training the in ML pipeline but principally is for local-finetuning (eg training a model after it as completed its global training pipeline).'''\n",
    "\n",
    "    D_0 = copy.deepcopy(self.w_prev)\n",
    "    # Set the w_prev equal to the current w:\n",
    "    self.w_prev = copy.deepcopy(model)\n",
    "    if self.global_method in [\"FedAvg\", \"NoFL\", \"FedAvgSB\", \"Per-FedAvg\", \"Per-FedAvg FO\", \"Per-FedAvg HF\"]:\n",
    "        for i in range(self.num_steps):\n",
    "            if self.normalize_dec:\n",
    "                model /= np.amax(model)\n",
    "\n",
    "            if self.method=='EtaGradStep':\n",
    "                model = self.train_eta_gradstep(model, self.eta, self.F, model, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, PCA_comps=self.PCA_comps)\n",
    "            elif self.method=='EtaScipyMinStep':\n",
    "                model = self.train_eta_scipyminstep(self.w, self.eta, self.F, model, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, D_0, self.verbose, PCA_comps=self.PCA_comps)\n",
    "            elif self.method=='FullScipyMinStep':\n",
    "                model = self.train_eta_scipyminstep(model, self.eta, self.F, model, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, D_0, self.verbose, PCA_comps=self.PCA_comps, full=True)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported method\")\n",
    "\n",
    "            #if self.mix_in_each_steps:\n",
    "            #    raise(\"mix_in_each_steps=True: Functionality not yet supported\")\n",
    "            #    self.mixed_w = self.smoothbatch*self.w + ((1 - self.smoothbatch)*self.mixed_w)\n",
    "\n",
    "        if self.normalize_dec:\n",
    "            model /= np.amax(model)\n",
    "\n",
    "    if which!=None:\n",
    "        return model, self.eval_model(which)\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(self, which):\n",
    "    if which=='local':\n",
    "        my_dec = self.w\n",
    "        my_V = self.V\n",
    "    else:\n",
    "        raise ValueError(\"Please set <which> to either local or global\")\n",
    "    # Just did this so we wouldn't have the 14 decimals points it always tries to give\n",
    "    if self.round2int:\n",
    "        temp = np.ceil(cost_l2(self.F, my_dec, self.H, my_V, self.learning_batch, self.alphaF, self.alphaD))\n",
    "        # Setting to int is just to catch overflow errors\n",
    "        # For RT considerations, ints are also generally ints cheaper than floats...\n",
    "        out = int(temp)\n",
    "    else:\n",
    "        temp = cost_l2(self.F, my_dec, self.H, my_V, self.learning_batch, self.alphaF, self.alphaD, Ne=self.PCA_comps)\n",
    "        out = round(temp, 3)\n",
    "    return out\n",
    "\n",
    "def test_inference(self, test_current_dec=True):\n",
    "    ''' No training / optimization, this just tests the fed in dec '''\n",
    "\n",
    "    if test_current_dec==True:\n",
    "        test_dec = self.w\n",
    "    else:\n",
    "        #test_dec is whatever you input, presumably a matrix... probably should check\n",
    "        test_dec = test_current_dec\n",
    "        if np.prod(test_dec.shape)!=(self.PCA_comps*2):\n",
    "            raise ValueError(f\"Unexpected size of test_current_dec: {np.prod(test_dec.shape)} vs {self.PCA_comps*2} expected\")\n",
    "\n",
    "    # This sets FVD using the full client dataset\n",
    "    # Since we aren't doing any optimization then it shouldn't matter if we use updates or not...\n",
    "    simulate_data_stream(streaming_method='full_data')\n",
    "    # Evaluate cost\n",
    "    temp = cost_l2(self.F, test_dec, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, Ne=self.PCA_comps)\n",
    "    dec_cost = round(temp, 3)\n",
    "    # Also want to see actual output \n",
    "    # This might be the cost and not the actual position...\n",
    "    D_reshaped = np.reshape(test_dec,(2,self.PCA_comps))\n",
    "    dec_pos = D_reshaped@self.F + self.H@self.V[:,:-1] - self.V[:,1:]\n",
    "    return dec_cost, dec_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb82a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dec66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a92f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a563c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [(1,2,3), (3,2,4), (1,1,1), (234,345,2345345), (34132412413,1,1324)]\n",
    "my_list2 = [(1,2,3), (3,2,4), [], (234,345,2345345), (34132412413,1,1324)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_data_path = 'C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\personalization-privacy-risk\\\\Data\\\\Client_Specific_Files\\\\'\n",
    "\n",
    "with h5py.File('C:\\\\Users\\\\kdmen\\\\Downloads\\\\hdf5_test1.h5', 'w') as hf:\n",
    "    #hf.create_dataset('gradient_norm_log', data=self.gradient_norm_log)\n",
    "    #hf.create_dataset('my_list', my_list)\n",
    "    G1 = hf.create_group('by_client')\n",
    "    for i in range(len(my_list)):\n",
    "        name_str = 'ClientID' + str(i)\n",
    "        G1.create_dataset(name_str, data=my_list[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('C:\\\\Users\\\\kdmen\\\\Downloads\\\\hdf5_test1.h5', 'r') as hf:\n",
    "    base_items = list(hf.items())\n",
    "    print(base_items)\n",
    "    print()\n",
    "    G1 = hf.get('by_client')\n",
    "    print(G1)\n",
    "    print()\n",
    "    G1_items = list(G1.items())\n",
    "    print(G1_items)\n",
    "    print()\n",
    "    print(type(G1_items[0]))\n",
    "    print()\n",
    "    print(G1_items[0])\n",
    "    print()\n",
    "    print(np.array(G1.get('ClientID0')))\n",
    "    print(type(np.array(G1.get('ClientID0'))))\n",
    "    print(len(np.array(G1.get('ClientID0'))))\n",
    "    print(np.array(G1.get('ClientID0'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_func_comps_log = [[], [(66.26913452148438, 0.0006768530583940446, 0.0)], [(2.9157848358154297, 0.0006768530583940446, 0.0), (60.782691955566406, 0.0007280856370925903, 0.0)], [], [(20.13532257080078, 0.0006768530583940446, 0.0)], [(51.6102180480957, 0.0006768530583940446, 0.0), (79754.7734375, 0.018196912482380867, 0.0)], [], [], [], [(2.3531734943389893, 0.0006768530583940446, 0.0), (138.36375427246094, 0.0007864069775678217, 0.0), (13262.6416015625, 0.012903410941362381, 0.0)], [(68.56493377685547, 0.0006768530583940446, 0.0), (296026.5625, 0.04193491116166115, 0.0)], [(61.63677978515625, 0.0006768530583940446, 0.0)], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('C:\\\\Users\\\\kdmen\\\\Downloads\\\\hdf5_cfc_log.h5', 'w') as hf:\n",
    "    G1 = hf.create_group('by_client')\n",
    "    for idx, list_of_tuples in enumerate(cost_func_comps_log):\n",
    "        name_str = 'ClientID' + str(idx)\n",
    "        print(name_str)\n",
    "        G1.create_dataset(name_str, data=list_of_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9246a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573c28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45456773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
