{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4addba",
   "metadata": {},
   "source": [
    "> Uh I guess I should import the code from clientbase and maybe from NB202 or the server to set up a few clients, load in their data, print it, compare it to doing it manually like in torch_linregr.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c17da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from flcore.pflniid_utils.data_utils import read_client_data\n",
    "from utils.custom_loss_class import CPHSLoss\n",
    "from utils.emg_dataset_class import *\n",
    "\n",
    "import time\n",
    "from flcore.pflniid_utils.privacy import *\n",
    "\n",
    "from flcore.clients.clientbase import Client\n",
    "from flcore.clients.clientavg import clientAVG\n",
    "from flcore.servers.serverbase import Server\n",
    "from flcore.servers.serveravg import FedAvg\n",
    "from flcore.servers.serverlocal import Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa59cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b415cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# general\n",
    "parser.add_argument('-go', \"--goal\", type=str, default=\"test\", \n",
    "                    help=\"The goal for this experiment\")\n",
    "parser.add_argument('-dev', \"--device\", type=str, default=\"cpu\",  # KAI: Changed the default to cpu\n",
    "                    choices=[\"cpu\", \"cuda\"])\n",
    "parser.add_argument('-did', \"--device_id\", type=str, default=\"0\")\n",
    "parser.add_argument('-data', \"--dataset\", type=str, default=\"cphs\")  # KAI: Changed the default to cphs (from mnist)\n",
    "#parser.add_argument('-nb', \"--num_classes\", type=int, default=10)  # Not doing classification...\n",
    "parser.add_argument('-m', \"--model\", type=str, default=\"LinearRegression\")  # KAI: Changed the default to Linear Regression\n",
    "parser.add_argument('-lbs', \"--batch_size\", type=int, default=1200)  # Setting it to a full update would be 1300ish... how many batches does it run? In one epoch? Not even sure where that is set\n",
    "# The 1300 and the batch size are 2 separate things...\n",
    "# I want to restrict the given dataset to just the 1300, but then iterate in batches... or do I since we don't have that much data and can probably just use all the data at once? Make batch size match the update size? ...\n",
    "parser.add_argument('-lr', \"--local_learning_rate\", type=float, default=0.005,\n",
    "                    help=\"Local learning rate\")\n",
    "parser.add_argument('-ld', \"--learning_rate_decay\", type=bool, default=False)\n",
    "parser.add_argument('-ldg', \"--learning_rate_decay_gamma\", type=float, default=0.99)\n",
    "parser.add_argument('-gr', \"--global_rounds\", type=int, default=250)  # KAI: Switched to 250 down from 2000\n",
    "parser.add_argument('-ls', \"--local_epochs\", type=int, default=1, \n",
    "                    help=\"Multiple update steps in one local epoch.\")  # KAI: I think it was 1 originally.  I'm gonna keep it there.  Does this mean I can set batchsize to 1300 and cook?Is my setup capable or running multiple epochs? Implicitly I was doing 1 epoch before, using the full update data I believe...\n",
    "parser.add_argument('-algo', \"--algorithm\", type=str, default=\"FedAvg\")\n",
    "parser.add_argument('-jr', \"--join_ratio\", type=float, default=0.2,\n",
    "                    help=\"Ratio of clients per round\")\n",
    "parser.add_argument('-rjr', \"--random_join_ratio\", type=bool, default=False,\n",
    "                    help=\"Random ratio of clients per round\")\n",
    "parser.add_argument('-nc', \"--num_clients\", type=int, default=14,\n",
    "                    help=\"Total number of clients\")\n",
    "parser.add_argument('-dp', \"--privacy\", type=bool, default=False,\n",
    "                    help=\"differential privacy\")\n",
    "parser.add_argument('-dps', \"--dp_sigma\", type=float, default=0.0)\n",
    "parser.add_argument('-sfn', \"--save_folder_name\", type=str, default='items')\n",
    "\n",
    "# SECTION: practical\n",
    "parser.add_argument('-cdr', \"--client_drop_rate\", type=float, default=0.0,\n",
    "                    help=\"Rate for clients that train but drop out\")\n",
    "parser.add_argument('-tsr', \"--train_slow_rate\", type=float, default=0.0,\n",
    "                    help=\"The rate for slow clients when training locally\")\n",
    "parser.add_argument('-ssr', \"--send_slow_rate\", type=float, default=0.0,\n",
    "                    help=\"The rate for slow clients when sending global model\")\n",
    "parser.add_argument('-ts', \"--time_select\", type=bool, default=False,\n",
    "                    help=\"Whether to group and select clients at each round according to time cost\")\n",
    "parser.add_argument('-tth', \"--time_threthold\", type=float, default=10000,\n",
    "                    help=\"The threthold for droping slow clients\")\n",
    "\n",
    "# SECTION: Kai's additional args\n",
    "parser.add_argument('-pca_channels', \"--pca_channels\", type=int, default=64,\n",
    "                    help=\"Number of principal components. 64 means do not use any PCA\")\n",
    "parser.add_argument('-lambdaF', \"--lambdaF\", type=float, default=0.0,\n",
    "                    help=\"Penalty term for user EMG input (user effort)\")\n",
    "parser.add_argument('-lambdaD', \"--lambdaD\", type=float, default=1e-3,\n",
    "                    help=\"Penalty term for the decoder norm (interface effort)\")\n",
    "parser.add_argument('-lambdaE', \"--lambdaE\", type=float, default=1e-4,\n",
    "                    help=\"Penalty term on performance error norm\")\n",
    "parser.add_argument('-starting_update', \"--starting_update\", type=int, default=0,\n",
    "                    help=\"Which update to start on (for CPHS Simulation). Use 0 or 10.\")\n",
    "parser.add_argument('-test_split_fraction', \"--test_split_fraction\", type=float, default=0.2,\n",
    "                    help=\"Fraction of data to use for testing\")\n",
    "parser.add_argument('-device_channels', \"--device_channels\", type=int, default=64,\n",
    "                    help=\"Number of recording channels with the used EMG device\")\n",
    "parser.add_argument('-dt', \"--dt\", type=float, default=1/60,\n",
    "                    help=\"Delta time, amount of time (sec?) between measurements\")\n",
    "parser.add_argument('-normalize_emg', \"--normalize_emg\", type=bool, default=False,\n",
    "                    help=\"Normalize the input EMG signals\")\n",
    "parser.add_argument('-normalize_V', \"--normalize_V\", type=bool, default=False,\n",
    "                    help=\"Normalize the V term in the cost function\")\n",
    "parser.add_argument('-local_round_threshold', \"--local_round_threshold\", type=int, default=50,\n",
    "                    help=\"Number of communication rounds per client until a client will advance to the next batch of streamed data\")\n",
    "parser.add_argument('-debug_mode', \"--debug_mode\", type=bool, default=False,\n",
    "                    help=\"In debug mode, the code is run to minimize overhead time in order to debug as fast as possible.  Namely, the data is held at the server to decrease init time, and communication delays are ignored.\")\n",
    "parser.add_argument('-condition_number', \"--condition_number\", type=int, default=1,\n",
    "                    help=\"Which condition number (trial) to train on\")\n",
    "parser.add_argument('-test_split_each_update', \"--test_split_each_update\", type=bool, default=False,\n",
    "                    help=\"Implement train/test split within each update or on the entire dataset\")\n",
    "parser.add_argument('-verbose', \"--verbose\", type=bool, default=False,\n",
    "                    help=\"Print out a bunch of extra stuff\")\n",
    "parser.add_argument('-slow_clients_bool', \"--slow_clients_bool\", type=bool, default=False,\n",
    "                    help=\"Control whether or not to have ANY slow clients\")\n",
    "parser.add_argument('-return_cost_func_comps', \"--return_cost_func_comps\", type=bool, default=False, #True\n",
    "                    help=\"Return Loss, Error, DTerm, FTerm from loss class\")\n",
    "parser.add_argument('-test_split_users', \"--test_split_users\", type=bool, default=False,\n",
    "                    help=\"Split testing data by holding out some users (fraction held out determined by test_split_fraction)\")\n",
    "    \n",
    "parser.add_argument('-t', \"--times\", type=int, default=1,\n",
    "                    help=\"Running times\")\n",
    "parser.add_argument('-ab', \"--auto_break\", type=bool, default=False)\n",
    "parser.add_argument('-dlg', \"--dlg_eval\", type=bool, default=False)\n",
    "parser.add_argument('-dlgg', \"--dlg_gap\", type=int, default=100)\n",
    "parser.add_argument('-bnpc', \"--batch_num_per_client\", type=int, default=2)  # Only used with DLG\n",
    "parser.add_argument('-eg', \"--eval_gap\", type=int, default=1,\n",
    "                    help=\"Rounds gap for evaluation\")\n",
    "parser.add_argument('-nnc', \"--num_new_clients\", type=int, default=0)\n",
    "\n",
    "# This one for sure breaks it\n",
    "#parser.add_argument('-fte', \"--fine_tuning_epoch\", type=int, default=0)\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args = parser.parse_known_args()\n",
    "\n",
    "args = args[0]\n",
    "args.fine_tuning_epoch=0\n",
    "dataset = 'cphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9219c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Running time: 0th =============\n",
      "Creating server and clients ...\n",
      "Linear(in_features=64, out_features=2, bias=True)\n",
      "Serveravg init(): set_slow_clients()\n",
      "Serveravg init(): set_clients()\n",
      "SBSC: iter 0\n",
      "SBSC: iter 1\n",
      "SBSC: iter 2\n",
      "SBSC: iter 3\n",
      "SBSC: iter 4\n",
      "SBSC: iter 5\n",
      "SBSC: iter 6\n",
      "SBSC: iter 7\n",
      "SBSC: iter 8\n",
      "SBSC: iter 9\n",
      "SBSC: iter 10\n",
      "SBSC: iter 11\n",
      "SBSC: iter 12\n",
      "SBSC: iter 13\n",
      "\n",
      "Join ratio / total clients: 0.2 / 14\n",
      "Finished creating server and clients.\n",
      "\n",
      "-------------Round number: 0-------------\n",
      "Selected client IDs: [9, 12]\n"
     ]
    }
   ],
   "source": [
    "time_list = []\n",
    "#reporter = MemReporter()\n",
    "model_str = args.model\n",
    "\n",
    "# Switched args.prev to 0 since it wasn't working\n",
    "#for i in range(0, args.times):\n",
    "print(f\"\\n============= Running time: {0}th =============\")\n",
    "print(\"Creating server and clients ...\")\n",
    "start = time.time()\n",
    "\n",
    "# Generate args.model\n",
    "args.model = torch.nn.Linear(args.pca_channels, 2)  #input_size, output_size\n",
    "\n",
    "print(args.model)\n",
    "\n",
    "# select algorithm\n",
    "if args.algorithm == \"FedAvg\":\n",
    "    server = FedAvg(args, 0)\n",
    "elif args.algorithm == \"Local\":\n",
    "    server = Local(args, 0)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "#server.train()\n",
    "\n",
    "#time_list.append(time.time()-start)\n",
    "#print(f\"\\nAverage time cost: {round(np.average(time_list), 2)}s.\")\n",
    "\n",
    "server.selected_clients = server.clients\n",
    "with torch.no_grad():\n",
    "    # subscript global_model with [0] if it is sequential instead of linear model --> does that return just the first layer then?\n",
    "    server.global_model.weight.fill_(0)\n",
    "\n",
    "#for i in range(self.global_rounds+1):\n",
    "if 0%server.eval_gap == 0:\n",
    "    print(f\"\\n-------------Round number: {0}-------------\")\n",
    "    if 0!=0:\n",
    "        print(\"\\nEvaluate personalized models\")\n",
    "        server.evaluate()\n",
    "\n",
    "        #print(f\"len: {len(self.rs_train_loss[-1])}\")\n",
    "        if type(server.rs_train_loss[-1]) in [int, float]:\n",
    "            print(f\"rs_train_loss: {server.rs_train_loss[-1]}\")\n",
    "        else:\n",
    "            print(f\"len: {len(server.rs_train_loss[-1])}\")\n",
    "        print()\n",
    "\n",
    "server.selected_clients = server.select_clients()\n",
    "print(f\"Selected client IDs: {[client.ID for client in server.selected_clients]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee8dd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_client: <flcore.clients.clientavg.clientAVG object at 0x000001CB2BA90700>\n"
     ]
    }
   ],
   "source": [
    "#print(\"CLIENT TRAINING\")\n",
    "#for client in server.selected_clients:\n",
    "#    client.train()\n",
    "#    print(f\"Client{client.ID} loss: {client.loss_log[-1]:0,.3f}\")\n",
    "\n",
    "my_client = server.selected_clients[0]\n",
    "print(f\"my_client: {my_client}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9205bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(self):\n",
    "trainloader = my_client.load_train_data()\n",
    "# self.model.to(self.device)\n",
    "my_client.model.train()\n",
    "\n",
    "# differential privacy\n",
    "#if self.privacy:\n",
    "#    self.model, self.optimizer, trainloader, privacy_engine = \\\n",
    "#        initialize_dp(self.model, self.optimizer, trainloader, self.dp_sigma)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "max_local_steps = my_client.local_epochs\n",
    "#if self.train_slow:\n",
    "#    max_local_steps = np.random.randint(1, max_local_steps // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f650c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: x has size torch.Size([1200, 64]); y has size torch.Size([1200, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(trainloader):\n",
    "    print(f\"Batch {i}: x has size {x.size()}; y has size {y.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM CLIENTBASE.PY\n",
    "\n",
    "'''\n",
    "def simulate_data_streaming(self, dl, test_data=False):\n",
    "    it = iter(dl)\n",
    "    s0 = it.__next__()\n",
    "    # self.max_training_update_upbound\n",
    "    # I think this is wrong, implies it never advances the update? Didnt I handle that somewhere else tho...\n",
    "    # Uhhh this is super wrong, should only be testing one update at a time???\n",
    "    #s_temp = s0[0][0:self.update_ix[1],:]\n",
    "    #p_reference = torch.transpose(s0[1][0:self.update_ix[1],:], 0, 1)\n",
    "    lb = self.update_ix[self.current_update]\n",
    "    ub = self.update_ix[self.current_update+1]\n",
    "    s_temp = s0[0][lb:ub,:]\n",
    "    p_reference = torch.transpose(s0[1][lb:ub,:], 0, 1)\n",
    "\n",
    "    # First, normalize the entire s matrix\n",
    "    if self.normalize_emg:\n",
    "        s_normed = s_temp / torch.linalg.norm(s_temp, ord='fro')\n",
    "        assert (torch.linalg.norm(s_normed, ord='fro')<1.2) and (torch.linalg.norm(s_normed, ord='fro')>0.8)\n",
    "    else:\n",
    "        s_normed = s_temp\n",
    "    # Apply PCA if applicable\n",
    "    if self.pca_channels!=self.device_channels:  # 64 is the number of channels present on the recording armband\n",
    "        pca = PCA(n_components=self.pca_channels)\n",
    "        s = torch.transpose(torch.tensor(pca.fit_transform(s_normed), dtype=torch.float32), 0, 1)\n",
    "    else:\n",
    "        s = torch.transpose(s_normed, 0, 1)\n",
    "\n",
    "    self.F = s[:,:-1]\n",
    "    v_actual =  torch.matmul(self.model.weight, s)\n",
    "    p_actual = torch.cumsum(v_actual, dim=1)*self.dt  # Numerical integration of v_actual to get p_actual\n",
    "    self.V = (p_reference - p_actual)*self.dt\n",
    "    if self.normalize_V:\n",
    "        self.V = self.V/torch.linalg.norm(self.V, ord='fro')\n",
    "        assert (torch.linalg.norm(self.V, ord='fro')<1.2) and (torch.linalg.norm(self.V, ord='fro')>0.8)\n",
    "    self.Y = p_reference[:, :-1]  # To match the input\n",
    "'''\n",
    "0\n",
    "\n",
    "#my_client.simulate_data_streaming(trainloader)\n",
    "it = iter(dl)\n",
    "s0 = it.__next__()\n",
    "# self.max_training_update_upbound\n",
    "# I think this is wrong, implies it never advances the update? Didnt I handle that somewhere else tho...\n",
    "# Uhhh this is super wrong, should only be testing one update at a time???\n",
    "#s_temp = s0[0][0:self.update_ix[1],:]\n",
    "#p_reference = torch.transpose(s0[1][0:self.update_ix[1],:], 0, 1)\n",
    "lb = update_ix[10]\n",
    "ub = update_ix[11]\n",
    "s_temp = s0[0][lb:ub,:]\n",
    "p_reference = torch.transpose(s0[1][lb:ub,:], 0, 1)\n",
    "\n",
    "# First, normalize the entire s matrix\n",
    "s_normed = s_temp\n",
    "# Apply PCA if applicable\n",
    "s = torch.transpose(s_normed, 0, 1)\n",
    "\n",
    "F = s[:,:-1]\n",
    "# Hmmm I think this line is doing my output=model(x)...\n",
    "v_actual =  torch.matmul(my_client.model.weight, s)\n",
    "p_actual = torch.cumsum(v_actual, dim=1)/60  # Numerical integration of v_actual to get p_actual\n",
    "V = (p_reference - p_actual)/60\n",
    "y = p_reference[:, :-1]  # To match the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_client.simulate_data_streaming(trainloader)\n",
    "it = iter(dl)\n",
    "s0 = it.__next__()\n",
    "# self.max_training_update_upbound\n",
    "# I think this is wrong, implies it never advances the update? Didnt I handle that somewhere else tho...\n",
    "# Uhhh this is super wrong, should only be testing one update at a time???\n",
    "#s_temp = s0[0][0:self.update_ix[1],:]\n",
    "#p_reference = torch.transpose(s0[1][0:self.update_ix[1],:], 0, 1)\n",
    "lb = update_ix[10]\n",
    "ub = update_ix[11]\n",
    "s_temp = s0[0][lb:ub,:]\n",
    "p_reference = torch.transpose(s0[1][lb:ub,:], 0, 1)\n",
    "\n",
    "# First, normalize the entire s matrix\n",
    "s_normed = s_temp\n",
    "# Apply PCA if applicable\n",
    "s = torch.transpose(s_normed, 0, 1)\n",
    "\n",
    "F = s[:,:-1]\n",
    "# Hmmm I think this line is doing my output=model(x)...\n",
    "v_actual =  my_client.model(torch.transpose(F, 0, 1))\n",
    "p_actual = torch.cumsum(v_actual, dim=1)/60  # Numerical integration of v_actual to get p_actual\n",
    "V = (p_reference - p_actual)/60\n",
    "y = p_reference[:, :-1]  # To match the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = CPHSLoss2(lambdaF=my_client.lambdaF, lambdaD=my_client.lambdaD, lambdaE=my_client.lambdaE)\n",
    "if v_actual.shape[0]!=y.shape[0]:\n",
    "    ty_pred = torch.transpose(y_pred, 0, 1)\n",
    "else:\n",
    "    ty_pred = y_pred\n",
    "t2 = lambdasFDE[1]*(torch.linalg.matrix_norm((my_client.model.weight))**2)\n",
    "t3 = lambdasFDE[0]*(torch.linalg.matrix_norm((F))**2)\n",
    "loss = loss_func(ty_pred, y) + t2 + t3\n",
    "print(f\"t2: {}\")\n",
    "print(f\"t3: {}\")\n",
    "print(f\"{}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d6c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c255e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefa581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7aac2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
