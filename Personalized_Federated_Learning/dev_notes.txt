Explore effect of changing JR

Reparam lambdas?
Reparam learning rate?
Reparam batch size? --> 1202 is an awkward size... what to do about it? 
Reparam number of epochs?
Explore join ratio...
Explore why learning rate is so high... are model weights really high?
Use lr decay?
What does a high lr do to FL?


JOIN RATIO SHOULD HAVE NO EFFECT FOR LOCAL ALGO!!! ALL CLIENTS RUN AT ONCE!!!

Plot JR effects
Look at model weights (are they huge? Justify high learning rate?)
Vary learning rate
Vary lambdas? Turn back on lambda abort... grid/random search?