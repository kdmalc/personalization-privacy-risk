{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027922ee-0419-41d0-a60a-445dc2952e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7517b-27fb-492e-bc8a-8edc17cc6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 64  # Number of channels\n",
    "hidden_size = 128  # Size of the hidden state in LSTM\n",
    "output_size = 2 # x,y vel predictions\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "mybatchsize = 32\n",
    "\n",
    "lambdaD = 1e-3\n",
    "lambdaE = 1e-4\n",
    "lambdaF = 0.0\n",
    "starting_update = 10\n",
    "final_update = 18\n",
    "cond_num = 1\n",
    "\n",
    "update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35120f70-ffcd-4919-be71-5f195b822725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        #out = self.fc(out[:, -1, :])  # Take the output of the last time step\n",
    "        out = self.fc(out)  # Predictions for all time steps\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb9fe7-5cf5-4d3a-9b98-f73a71c4e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        #return (self.data.shape[-1])\n",
    "        return (self.data.shape[1]) # This only works for 2D inputs I think...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming 'data' and 'labels' are lists of numpy arrays\n",
    "        sample_data = self.data[:,idx]\n",
    "        sample_labels = self.labels[:,idx]\n",
    "        # You can apply any custom logic here based on the specific requirements of your task\n",
    "        # Convert to PyTorch tensors\n",
    "        sample_data = torch.Tensor(sample_data)\n",
    "        sample_labels = torch.Tensor(sample_labels)\n",
    "        return sample_data, sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70513e9a-b4ba-408d-aa96-2a0e402f7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "print(\"Loading Data\")\n",
    "input_data = None\n",
    "target_data = None\n",
    "data_path = r\"C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\Client_Specific_Files\"\n",
    "for i in range(14):\n",
    "    datafile = \"UserID\" + str(i) + \"_TrainData_8by20770by64.npy\"\n",
    "    full_data = np.load(data_path+\"\\\\\"+datafile)\n",
    "    cond_data = full_data[cond_num-1, update_ix[starting_update]:update_ix[final_update], :]\n",
    "    data = np.transpose(cond_data)\n",
    "    if input_data is None:\n",
    "        input_data = data\n",
    "    else:\n",
    "        input_data = np.vstack((input_data, data))\n",
    "\n",
    "    labelfile = \"UserID\" + str(i) + \"_Labels_8by20770by2.npy\"\n",
    "    full_data = np.load(data_path+\"\\\\\"+labelfile)\n",
    "    cond_data = full_data[cond_num-1, update_ix[starting_update]:update_ix[final_update], :]\n",
    "    data = np.transpose(cond_data)\n",
    "    if target_data is None:\n",
    "        target_data = data\n",
    "    else:\n",
    "        target_data = np.vstack((target_data, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ff97c-f462-4bd2-985f-c850e3551679",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_idx = ceil(input_data.shape[1]*.8)\n",
    "testing_inputs = torch.tensor(input_data[:, test_split_idx:], dtype=torch.float)\n",
    "testing_targets = torch.tensor(target_data[:, test_split_idx:], dtype=torch.float)\n",
    "training_inputs = torch.tensor(input_data[:, :test_split_idx], dtype=torch.float)\n",
    "training_targets = torch.tensor(target_data[:, :test_split_idx], dtype=torch.float)\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de239f31-8b47-4685-bfd2-3b5fdf814509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to DataLoader\n",
    "print(\"Create custom datasets\")\n",
    "train_dataset = CustomTimeSeriesDataset(training_inputs, training_targets)\n",
    "trainloader = DataLoader(train_dataset, batch_size=mybatchsize, shuffle=False)\n",
    "test_dataset = CustomTimeSeriesDataset(testing_inputs, testing_targets)\n",
    "testloader = DataLoader(test_dataset, batch_size=mybatchsize, shuffle=False)\n",
    "print(\"Datasets and dataloaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79e9b6-1400-4894-b500-653207e861fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "print(\"Create LSTM Model\")\n",
    "model = TimeSeriesLSTM(input_size, hidden_size, output_size, num_layers)\n",
    "print(\"Model instantiated!\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e09aad-5360-4683-bc1e-130643b93c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = []\n",
    "test_log = []\n",
    "\n",
    "print_boolean = True\n",
    "# Training the model\n",
    "print(\"Train model\")\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in trainloader:\n",
    "        \n",
    "        total_elements = inputs.numel()\n",
    "        new_first_dim = total_elements // (mybatchsize * input_size)\n",
    "        # Calculate the remainder\n",
    "        remainder = total_elements % (mybatchsize * input_size)\n",
    "        # Optionally, you can pad or trim the tensor to make it evenly divisible\n",
    "        if remainder != 0:\n",
    "            # Pad or trim the tensor to make it evenly divisible\n",
    "            inputs = inputs.view(-1)[:new_first_dim * mybatchsize * input_size]\n",
    "        # Reshape the tensor\n",
    "        input_reshaped = inputs.view(new_first_dim, mybatchsize, input_size)\n",
    "\n",
    "        total_elements = targets.numel()\n",
    "        new_first_dim = total_elements // (mybatchsize * input_size)\n",
    "        # Calculate the remainder\n",
    "        remainder = total_elements % (mybatchsize * input_size)\n",
    "        # Optionally, you can pad or trim the tensor to make it evenly divisible\n",
    "        if remainder != 0:\n",
    "            # Pad or trim the tensor to make it evenly divisible\n",
    "            targets = targets.view(-1)[:new_first_dim * mybatchsize * input_size]\n",
    "        # Reshape the tensor\n",
    "        targets_reshaped = targets.view(new_first_dim, mybatchsize, input_size)\n",
    "\n",
    "        #######################################################\n",
    "        # Reshape to (N, batch_size, input_size) with batch_size = 32\n",
    "        #input_reshaped = inputs.view(-1, mybatchsize, input_size)\n",
    "        #targets_reshaped = targets.transpose(0, 1)  # Didnt fix it... need to have an output size of 2...\n",
    "        #targets_reshaped = targets.view(-1, mybatchsize, 2)\n",
    "        #######################################################\n",
    "\n",
    "        if epoch==0 and print_boolean==True:\n",
    "            print_boolean = False\n",
    "            print(\"Original size of inputs:\", inputs.size())\n",
    "            print(\"Reshaped size of inputs:\", input_reshaped.size())\n",
    "\n",
    "        ##########################################################################################\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_reshaped)\n",
    "        #loss = criterion(outputs, targets)\n",
    "        t1 = lambdaE*criterion(outputs, targets_reshaped)\n",
    "        # Initialize a variable to accumulate the norms\n",
    "        running_weights_norm = 0.0\n",
    "        # Iterate through the parameters and calculate the norm\n",
    "        for param in model.parameters():\n",
    "            # Uhh make sure this is only norming the weights and not other params (bias or something idk)\n",
    "            running_weights_norm += torch.norm(param)\n",
    "        t2 = lambdaD*(running_weights_norm**2)\n",
    "        t3 = lambdaF*(torch.linalg.matrix_norm(inputs)**2)\n",
    "        loss = t1 + t2 + t3\n",
    "        train_log.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test the current model each epoch\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_targets in testloader:\n",
    "            model.eval()\n",
    "\n",
    "            test_inputs_reshaped = test_inputs.view(-1, mybatchsize, input_size)\n",
    "            test_targets = test_targets.transpose(0, 1)\n",
    "\n",
    "            prediction = model(test_inputs_reshaped)\n",
    "            t1 = lambdaE*criterion(prediction, test_targets)\n",
    "            t2 = lambdaD*(torch.linalg.matrix_norm((model.weight))**2)\n",
    "            t3 = lambdaF*(torch.linalg.matrix_norm(test_inputs)**2)\n",
    "            test_loss = t1 + t2 + t3\n",
    "            test_log.append(test_loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3d8e0-38bc-4333-b426-81f75663ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plot training and testing logs!\")\n",
    "plt.plot(range(len(train_log)), train_log, label=\"Train\")\n",
    "plt.plot(range(len(test_log)), test_log, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Per Epoch\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1ac30-dd55-4115-93f2-5d2c6b1a358e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
