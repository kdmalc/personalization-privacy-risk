{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dd81df-49af-48ba-9459-f36b18e7645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dbaabe-5ea5-44ac-892e-da1069d72182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Data loaded!\n",
      "Create custom datasets\n",
      "Datasets and dataloaders created!\n"
     ]
    }
   ],
   "source": [
    "B, T, D = 14, 20770, 64\n",
    "cond_num = 1\n",
    "update_ix = np.load('C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\update_ix.npy')\n",
    "starting_update = 10\n",
    "final_update = 17\n",
    "batch_size = 14\n",
    "num_tensor_dims = 3\n",
    "num_users = 14\n",
    "\n",
    "class CustomTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels, num_dims):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num_dims = num_dims\n",
    "        assert ((self.num_dims == 2) or (self.num_dims == 3))\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.num_dims==2:\n",
    "            return self.data.shape[1] # Return the number of observations (sequence length)\n",
    "        elif self.num_dims==3:\n",
    "            return self.data.shape[0] # Return the number of sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.num_dims==2:\n",
    "            sample_data = torch.Tensor(self.data[:,idx])\n",
    "            sample_labels = torch.Tensor(self.labels[:,idx])\n",
    "            return sample_data, sample_labels\n",
    "        elif self.num_dims==3:\n",
    "            sample_data = torch.Tensor(self.data[idx])\n",
    "            sample_labels = torch.Tensor(self.labels[idx])\n",
    "            return sample_data, sample_labels\n",
    "\n",
    "# Load Data\n",
    "print(\"Loading Data\")\n",
    "input_data = None\n",
    "target_data = None\n",
    "data_path = r\"C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\Client_Specific_Files\"\n",
    "for i in range(num_users):\n",
    "    datafile = \"UserID\" + str(i) + \"_TrainData_8by20770by64.npy\"\n",
    "    full_data = np.load(data_path+\"\\\\\"+datafile)\n",
    "    cond_data = full_data[cond_num-1, update_ix[starting_update]:update_ix[final_update], :]\n",
    "    data = np.transpose(cond_data)\n",
    "    if input_data is None:\n",
    "        input_data = data\n",
    "    else:\n",
    "        input_data = np.vstack((input_data, data))\n",
    "\n",
    "    labelfile = \"UserID\" + str(i) + \"_Labels_8by20770by2.npy\"\n",
    "    full_data = np.load(data_path+\"\\\\\"+labelfile)\n",
    "    cond_data = full_data[cond_num-1, update_ix[starting_update]:update_ix[final_update], :]\n",
    "    data = np.transpose(cond_data)\n",
    "    if target_data is None:\n",
    "        target_data = data\n",
    "    else:\n",
    "        target_data = np.vstack((target_data, data))\n",
    "\n",
    "#################\n",
    "\n",
    "test_split_idx = ceil(input_data.shape[1]*.8)\n",
    "test_data = torch.tensor(input_data[:, test_split_idx:], dtype=torch.float)\n",
    "test_labels = torch.tensor(target_data[:, test_split_idx:], dtype=torch.float)\n",
    "train_data = torch.tensor(input_data[:, :test_split_idx], dtype=torch.float)\n",
    "train_labels = torch.tensor(target_data[:, :test_split_idx], dtype=torch.float)\n",
    "print(\"Data loaded!\")\n",
    "\n",
    "if num_tensor_dims == 3:\n",
    "    # Calculate the size for the reshaped tensor\n",
    "    test_data_new_size = (num_users, -1, test_data.shape[1])\n",
    "    test_labels_new_size = (num_users, -1, test_labels.shape[1])\n",
    "    train_data_new_size = (num_users, -1, train_data.shape[1])\n",
    "    train_labels_new_size = (num_users, -1, train_labels.shape[1])\n",
    "    # Reshape the tensor\n",
    "    test_data = test_data.view(*test_data_new_size)\n",
    "    test_labels = test_labels.view(*test_labels_new_size)\n",
    "    train_data = train_data.view(*train_data_new_size)\n",
    "    train_labels = train_labels.view(*train_labels_new_size)\n",
    "\n",
    "#################\n",
    "\n",
    "# Convert data to DataLoader\n",
    "print(\"Create custom datasets\")\n",
    "train_dataset = CustomTimeSeriesDataset(train_data, train_labels, 3)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = CustomTimeSeriesDataset(test_data, test_labels, 3)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "inference_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(\"Datasets and dataloaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a50b45-19f8-4418-a867-37ac65c2e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 64, 6732])\n",
      "torch.Size([14, 2, 6732])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ddf65ca-d304-400f-bef2-d2458868a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Data Shape: torch.Size([14, 64, 6732]), Labels Shape: torch.Size([14, 2, 6732])\n",
      "1\n",
      "\n",
      "Batch 1 - Data Shape: torch.Size([14, 64, 1682]), Labels Shape: torch.Size([14, 2, 1682])\n",
      "1\n",
      "\n",
      "Batch 1 - Data Shape: torch.Size([1, 64, 1682]), Labels Shape: torch.Size([1, 2, 1682])\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "train_i = 0\n",
    "for i, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    if i==0:\n",
    "        print(f\"Batch {i + 1} - Data Shape: {batch_data.shape}, Labels Shape: {batch_labels.shape}\")\n",
    "    train_i += 1\n",
    "print(train_i)\n",
    "\n",
    "print()\n",
    "\n",
    "test_i = 0\n",
    "for i, (batch_data, batch_labels) in enumerate(test_loader):\n",
    "    if i==0:\n",
    "        print(f\"Batch {i + 1} - Data Shape: {batch_data.shape}, Labels Shape: {batch_labels.shape}\")\n",
    "    test_i += 1\n",
    "print(test_i)\n",
    "\n",
    "print()\n",
    "\n",
    "inf_i = 0\n",
    "for i, (batch_data, batch_labels) in enumerate(inference_loader):\n",
    "    if i==0:\n",
    "        print(f\"Batch {i + 1} - Data Shape: {batch_data.shape}, Labels Shape: {batch_labels.shape}\")\n",
    "    inf_i += 1\n",
    "print(inf_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d65c51e4-6fd4-400f-adf1-7324a255e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = D  # Number of features in the input data\n",
    "output_size = 2  # Number of dimensions in the output labels\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6819921-346b-4978-b493-f44495c0302f",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53180989-cbac-4477-ac8c-0336d7d918a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_loss_class import CPHSLoss\n",
    "\n",
    "lambdaF=0\n",
    "lambdaD=1e-3\n",
    "lambdaE=1e-4\n",
    "criterion = CPHSLoss(lambdaF=lambdaF, lambdaD=lambdaD, lambdaE=lambdaE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a036db74-ac80-4829-b3ac-79ef2c6e39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(input_tensor):\n",
    "    '''Normalizes a tensor of any dimensions. Goal is to have inputs on the range 0-1, NOT a norm of 1.'''\n",
    "    \n",
    "    # Compute min and max values across all dimensions\n",
    "    min_values = torch.min(input_tensor)\n",
    "    max_values = torch.max(input_tensor)\n",
    "\n",
    "    return (input_tensor - min_values) / (max_values - min_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7d9b7-287a-4a95-8f3a-ef978dd1a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def simulate_data_streaming_xy(self, x, y, test_data=False):\n",
    "s_temp = x\n",
    "p_reference = torch.transpose(y, 0, 1)\n",
    "\n",
    "s_normed = normalize_tensor(s_temp)\n",
    "p_reference = normalize_tensor(p_reference)\n",
    "\n",
    "self.F = s[:,:-1]\n",
    "v_actual =  torch.matmul(self.model.weight, s)\n",
    "# Numerical integration of v_actual to get p_actual\n",
    "p_actual = torch.cumsum(v_actual, dim=1)*self.dt\n",
    "# I don't think I actually use V later on\n",
    "self.V = (p_reference - p_actual)*self.dt\n",
    "self.y_ref = p_reference[:, :-1]  # To match the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925728a-2693-4dc0-80de-ad9633a11312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b57176-51e3-4d89-baf5-a3c666dd02b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ded3d0b-af20-4c4a-85bf-356e1277de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: 17071574.0\n",
      "t2: 0.0035664208699017763\n",
      "Epoch [1/10], Loss: 17071574.0000\n",
      "t1: 1.5797770973257662e+19\n",
      "t2: 164185152.0\n",
      "Epoch [2/10], Loss: 15797770973257662464.0000\n",
      "t1: 1.462421364381834e+31\n",
      "t2: 1.519892218509179e+20\n",
      "Epoch [3/10], Loss: 14624213643818340766111200444416.0000\n",
      "t1: inf\n",
      "t2: 1.4070001911769784e+32\n",
      "Epoch [4/10], Loss: inf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "EPOCH_LOSS IS inf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(epoch_loss) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(epoch_loss):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m#print(f\"t1: {t1}\")\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m#print(f\"t2: {t2}\")\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH_LOSS IS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mValueError\u001b[0m: EPOCH_LOSS IS inf"
     ]
    }
   ],
   "source": [
    "# Define a simple linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Initialize the weights using a specific initialization method\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressionModel(input_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_loss_log = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        # Flatten the input data\n",
    "        batch_data = batch_data.view(-1, input_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_data)\n",
    "\n",
    "        # LOSS\n",
    "        # L2 regularization term\n",
    "        l2_loss = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_loss += torch.norm(param, p=2)\n",
    "        t1 = criterion(outputs, batch_labels.view(-1, output_size))\n",
    "        t2 = lambdaD*(l2_loss**2)\n",
    "        print(f\"t1: {t1}\")\n",
    "        print(f\"t2: {t2}\")\n",
    "        #t3 = lambdaF*(torch.linalg.matrix_norm((F))**2)\n",
    "        t3 = 0 \n",
    "        loss = t1 + t2 + t3\n",
    "        epoch_loss += loss.item()\n",
    "        if np.isnan(epoch_loss) or np.isinf(epoch_loss):\n",
    "            #print(f\"t1: {t1}\")\n",
    "            #print(f\"t2: {t2}\")\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "            raise ValueError(f\"EPOCH_LOSS IS {epoch_loss}\")\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log average loss acros all batches for the epoch\n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_log.append(epoch_loss)\n",
    "\n",
    "    #if epoch%10==0:\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plot the training loss over epochs\n",
    "plt.plot(range(1, num_epochs+1), train_loss_log, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a85f260-3856-4ce9-a149-d1d2c5e9f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-9.2567e+16, -9.2485e+16, -9.2420e+16, -9.2475e+16, -9.2543e+16,\n",
      "         -9.2378e+16, -9.2185e+16, -9.2169e+16, -9.2342e+16, -9.2212e+16,\n",
      "         -9.2146e+16, -9.2154e+16, -9.2317e+16, -9.2170e+16, -9.1953e+16,\n",
      "         -9.2124e+16, -9.2307e+16, -9.2237e+16, -9.2069e+16, -9.2113e+16,\n",
      "         -9.2331e+16, -9.2218e+16, -9.2080e+16, -9.2279e+16, -9.2498e+16,\n",
      "         -9.2406e+16, -9.2327e+16, -9.2354e+16, -9.2485e+16, -9.2339e+16,\n",
      "         -9.2186e+16, -9.2288e+16, -9.2506e+16, -9.2349e+16, -9.2212e+16,\n",
      "         -9.2209e+16, -9.2205e+16, -9.2072e+16, -9.2013e+16, -9.2104e+16,\n",
      "         -9.2198e+16, -9.1965e+16, -9.1908e+16, -9.2107e+16, -9.2192e+16,\n",
      "         -9.2101e+16, -9.1954e+16, -9.2125e+16, -9.2321e+16, -9.2205e+16,\n",
      "         -9.2172e+16, -9.2085e+16, -9.2131e+16, -9.1977e+16, -9.1976e+16,\n",
      "         -9.2197e+16, -9.2401e+16, -9.2230e+16, -9.2114e+16, -9.2253e+16,\n",
      "         -9.2428e+16, -9.2281e+16, -9.2206e+16, -9.2217e+16],\n",
      "        [ 7.5466e+16,  7.5400e+16,  7.5347e+16,  7.5392e+16,  7.5447e+16,\n",
      "          7.5313e+16,  7.5155e+16,  7.5143e+16,  7.5285e+16,  7.5177e+16,\n",
      "          7.5123e+16,  7.5130e+16,  7.5263e+16,  7.5143e+16,  7.4967e+16,\n",
      "          7.5106e+16,  7.5255e+16,  7.5197e+16,  7.5060e+16,  7.5096e+16,\n",
      "          7.5274e+16,  7.5182e+16,  7.5069e+16,  7.5231e+16,  7.5410e+16,\n",
      "          7.5335e+16,  7.5271e+16,  7.5292e+16,  7.5399e+16,  7.5279e+16,\n",
      "          7.5156e+16,  7.5239e+16,  7.5417e+16,  7.5289e+16,  7.5176e+16,\n",
      "          7.5174e+16,  7.5171e+16,  7.5063e+16,  7.5015e+16,  7.5088e+16,\n",
      "          7.5166e+16,  7.4976e+16,  7.4931e+16,  7.5093e+16,  7.5161e+16,\n",
      "          7.5087e+16,  7.4968e+16,  7.5106e+16,  7.5266e+16,  7.5171e+16,\n",
      "          7.5145e+16,  7.5073e+16,  7.5111e+16,  7.4985e+16,  7.4984e+16,\n",
      "          7.5165e+16,  7.5332e+16,  7.5193e+16,  7.5097e+16,  7.5209e+16,\n",
      "          7.5352e+16,  7.5234e+16,  7.5172e+16,  7.5181e+16]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.6599e+15,  2.9838e+15], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c136f6-419a-437a-8077-4d4d5d3d0e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9057e+20,  1.5536e+20],\n",
       "        [-1.8936e+20,  1.5438e+20],\n",
       "        [-1.8660e+20,  1.5213e+20],\n",
       "        ...,\n",
       "        [-1.5690e+20,  1.2792e+20],\n",
       "        [-1.2507e+20,  1.0197e+20],\n",
       "        [-1.7177e+20,  1.4004e+20]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d25bf649-90e9-4535-bf49-0474540a9faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.6089,   5.6089,   2.9489,  ...,  -3.9273,  -3.9273,  -3.9273],\n",
       "         [  5.3464,   5.3464,   5.3166,  ...,   7.0196,   7.0196,   7.0196]],\n",
       "\n",
       "        [[-15.4448, -15.4448, -10.8032,  ...,  21.1627,  21.7610,  21.7610],\n",
       "         [-21.7431, -21.7431, -19.9325,  ...,  14.5263,  14.7706,  14.7706]],\n",
       "\n",
       "        [[  7.7023,   7.7023,   5.9858,  ...,  -3.1351,  -3.9392,  -3.9392],\n",
       "         [ -7.7380,  -7.7380,  -9.0258,  ...,  10.2470,  11.1555,  11.1555]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 34.4756,  34.6800,  35.0770,  ..., -18.0856, -18.0856, -18.5382],\n",
       "         [ 14.9993,  15.8593,  18.6275,  ...,  -4.2904,  -4.2904,  -4.4546]],\n",
       "\n",
       "        [[-33.0132, -33.4118, -35.9532,  ...,   6.0055,   6.1073,   6.3316],\n",
       "         [ 14.1637,  14.1922,  14.1270,  ...,  -0.2625,  -0.2113,  -0.0874]],\n",
       "\n",
       "        [[-18.4834, -18.6501, -19.7785,  ...,  12.1055,  11.4284,  11.0979],\n",
       "         [-17.5188, -17.8982, -20.2428,  ...,  21.1333,  20.7342,  20.5140]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1fd39-9509-4079-b2be-7c08f0ad41c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec63f81-cf86-4083-8f00-b4d704bc0b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a007a0d-3a73-4402-b81a-c054cc9997a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f38b5-6a10-4c86-a738-bdd0a59a96f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48580af0-5b31-47af-860e-f742798a3bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c1608-5d1d-43a4-8db1-276b24fdbafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb0ae6-008e-4ad2-9f90-d0c8db23c705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1fceb-0546-41f2-982a-96fd9732a849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327cb155-992b-4a3d-bcad-ec95a537580c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac646f-021f-4043-a6be-413802c4dfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28985911-0894-4e69-abd1-64b66ded8eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562aea9-0c80-4708-9a9b-825b89cf6982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379caba7-0bf5-494a-b066-577c3f7b934a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672290c-bfa7-45b3-8642-af1e1d7a3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        print(batch_data.shape)\n",
    "        batch_data = batch_data.view(-1, input_size)\n",
    "        print(batch_data.shape)\n",
    "        outputs = model(batch_data)\n",
    "        print(outputs.shape)\n",
    "        total_loss += criterion(outputs, batch_labels.view(-1, output_size)).item()\n",
    "        print()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd900cd-9125-447d-bb20-4c30cea6dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(outputs[:,0])\n",
    "plt.plot(batch_labels.view(-1, output_size)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfded667-93a5-4c97-9dba-53ce54bea938",
   "metadata": {},
   "source": [
    "Single at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1dac67-608a-4dac-b79f-d34525a8f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "outputs_log = []\n",
    "labels_log = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        print(f\"batch_data.shape: {batch_data.shape}\")\n",
    "        for user_num in range(batch_data.shape[0]):\n",
    "            user1_seq_data = batch_data[user_num,:,:]\n",
    "            user1_seq_labels = batch_labels[user_num,:,:]\n",
    "            print(f\"user1_seq_data.shape: {user1_seq_data.shape}\")\n",
    "            iter_i = 0\n",
    "            for (one_seq, one_seq_label) in zip(user1_seq_data, user1_seq_labels):\n",
    "                iter_i += 1\n",
    "                outputs = model(one_seq)\n",
    "                outputs_log.append(outputs[0].item())\n",
    "                labels_log.append(one_seq_label[0].item())\n",
    "                if iter_i%1000==0:\n",
    "                    #print(f\"outputs.shape: {outputs.shape}\")\n",
    "                    print(f\"{outputs}, {one_seq_label}\")\n",
    "                total_loss += criterion(outputs, one_seq_label).item()\n",
    "            print()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3576e-b8f9-42ca-aaed-7d20483664eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(outputs_log)\n",
    "plt.plot(labels_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aead6d0-67a1-4250-95eb-079c4fedfc3c",
   "metadata": {},
   "source": [
    "> Above, I show that you get the same outputs when you use the same model, whether or not you compute all the data at once (eg the default) or doing it one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0c59e-27f9-4379-ba43-e590147ad1e9",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001b81b-05ee-4672-89f4-23383e407c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_size = D  # Number of features in the input data\n",
    "#output_size = 2  # Number of dimensions in the output labels\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b74a8-1461-41b2-ba7c-cad2f267cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # RNN input: (batch_size, seq_len, input_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(rnn_out)\n",
    "        return output\n",
    "\n",
    "# Initialize the RNN model\n",
    "rnn_model = RNNModel(D, hidden_size, 2)  # Change output_size to 2\n",
    "\n",
    "# Define loss function and optimizer for the RNN model\n",
    "#rnn_criterion = nn.MSELoss()\n",
    "rnn_criterion = CPHSLoss(lambdaF=lambdaF, lambdaD=lambdaD, lambdaE=lambdaE)\n",
    "rnn_optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop for the RNN model\n",
    "rnn_train_losses = []\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    rnn_epoch_losses = []\n",
    "    batch_counter = 0\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        #print(f\"Pre-reshape batch_data size: {batch_data.shape}\")\n",
    "        # Reshape the input data to (batch_size, seq_len, input_size)\n",
    "        #batch_data = batch_data.permute(0, 2, 1)\n",
    "        #print(f\"Post-reshape batch_data size: {batch_data.shape}\")\n",
    "\n",
    "        # Make sure the input size matches the RNN input size\n",
    "        assert batch_data.size(-1) == D, f\"Expected input size {D}, got {batch_data.size(-1)}\"\n",
    "\n",
    "        # Forward pass\n",
    "        rnn_outputs = rnn_model(batch_data)\n",
    "\n",
    "        if epoch==0:\n",
    "            print(f\"Batch {batch_counter}. batch_data size: {batch_data.shape}. batch_labels size: {batch_labels.shape}. rnn_outputs size: {rnn_outputs.shape}.\")\n",
    "        batch_counter += 1\n",
    "        \n",
    "        rnn_loss = rnn_criterion(rnn_outputs, batch_labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        rnn_optimizer.zero_grad()\n",
    "        rnn_loss.backward()\n",
    "        rnn_optimizer.step()\n",
    "\n",
    "        rnn_epoch_losses.append(rnn_loss.item())\n",
    "\n",
    "    average_rnn_epoch_loss = sum(rnn_epoch_losses) / len(rnn_epoch_losses)\n",
    "    rnn_train_losses.append(average_rnn_epoch_loss)\n",
    "    if epoch%10==0:\n",
    "        print(f'RNN Epoch [{epoch+1}/{num_epochs}], Loss: {average_rnn_epoch_loss:.4f}')\n",
    "\n",
    "# Plot the training loss for the RNN model over epochs\n",
    "plt.plot(range(1, num_epochs+1), rnn_train_losses, label='RNN Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('RNN Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d69c8-0014-4065-8893-625c0cb5403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN LOADER\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dim0 = 0\n",
    "    dim1 = 0\n",
    "    dim2 = 0\n",
    "    batch_counter = 0\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        print(batch_counter)\n",
    "        batch_counter += 1\n",
    "        print(f\"Pre-reshape batch_data size: {batch_data.shape}\")\n",
    "        print(f\"Pre-reshape batch_labels size: {batch_labels.shape}\")\n",
    "        dim0 += batch_data.shape[0]\n",
    "        dim1 += batch_data.shape[1]\n",
    "        dim2 += batch_data.shape[2]\n",
    "        \n",
    "        print()\n",
    "    print(f\"Summed dims: ({dim0}, {dim1}, {dim2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724efa91-39d0-43a3-b103-731acc2bcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST LOADER\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dim0 = 0\n",
    "    dim1 = 0\n",
    "    dim2 = 0\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        print(f\"Pre-reshape batch_data size: {batch_data.shape}\")\n",
    "        print(f\"Pre-reshape batch_labels size: {batch_labels.shape}\")\n",
    "        dim0 += batch_data.shape[0]\n",
    "        dim1 += batch_data.shape[1]\n",
    "        dim2 += batch_data.shape[2]\n",
    "        \n",
    "        print()\n",
    "    print(f\"Summed dims: ({dim0}, {dim1}, {dim2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b44aef-d41c-4882-ab53-8f2e04f6ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        print(f\"batch_data size: {batch_data.shape}\")\n",
    "        print(f\"batch_labels size: {batch_labels.shape}\")\n",
    "        outputs = rnn_model(batch_data)\n",
    "        #reshaped_batch_labels = batch_labels.view(-1, output_size)\n",
    "        print(f\"outputs size: {outputs.shape}\")\n",
    "        #print(f\"reshaped_batch_labels size: {reshaped_batch_labels.shape}\")\n",
    "        total_loss += criterion(outputs, batch_labels).item()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45321a1-f30d-43a4-80f5-84ed8b262835",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3def81-a3f8-4ad1-bedd-f31004663173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM input: (batch_size, seq_len, input_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Take the last time step's output\n",
    "        last_output = lstm_out#[:, , :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "# Initialize the LSTM model\n",
    "hidden_size = 64\n",
    "lstm_model = LSTMModel(D, hidden_size, output_size)  # Change input_size to D\n",
    "\n",
    "# Define loss function and optimizer for the LSTM model\n",
    "#lstm_criterion = nn.MSELoss()\n",
    "lstm_criterion = CPHSLoss(lambdaF=lambdaF, lambdaD=lambdaD, lambdaE=lambdaE)\n",
    "lstm_optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for the LSTM model\n",
    "lstm_train_losses = []\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_epoch_losses = []\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        # Reshape the input data to (batch_size, seq_len, input_size)\n",
    "        #batch_data = batch_data.view(batch_data.size(0), -1, D)\n",
    "\n",
    "        # Make sure the input size matches the LSTM input size\n",
    "        assert batch_data.size(-1) == D, f\"Expected input size {D}, got {batch_data.size(-1)}\"\n",
    "\n",
    "        # Forward pass\n",
    "        lstm_outputs = lstm_model(batch_data)\n",
    "        lstm_loss = lstm_criterion(lstm_outputs, batch_labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        lstm_optimizer.zero_grad()\n",
    "        lstm_loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "        lstm_epoch_losses.append(lstm_loss.item())\n",
    "\n",
    "    average_lstm_epoch_loss = sum(lstm_epoch_losses) / len(lstm_epoch_losses)\n",
    "    lstm_train_losses.append(average_lstm_epoch_loss)\n",
    "    if epoch%10==0:\n",
    "        (f'LSTM Epoch [{epoch+1}/{num_epochs}], Loss: {average_lstm_epoch_loss:.4f}')\n",
    "\n",
    "# Plot the training loss for the LSTM model over epochs\n",
    "plt.plot(range(1, num_epochs+1), lstm_train_losses, label='LSTM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LSTM Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3bb2c-c44e-48a8-bba1-1cf1d041243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_data = batch_data.view(-1, input_size)\n",
    "        outputs = model(batch_data)\n",
    "        total_loss += criterion(outputs, batch_labels.view(-1, output_size)).item()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18660697-f743-4538-aa0b-f6ef5539fb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6bbb1-eafb-40a2-b856-4550c1de9743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdeb157-1a27-4168-98ff-2ec1d51270dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
