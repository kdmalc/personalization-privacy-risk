{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f846e1-aaf7-4563-9e2d-20a93c98e5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4a1772-6a67-4220-add0-56ec55a40edc",
   "metadata": {},
   "source": [
    "## ELLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2384f77-1e51-4b33-b6ed-ed8dd1432f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Repo (8 years ago!): https://github.com/paulruvolo/ELLA\n",
    "\n",
    "# ELLA.py converted to Python 3, PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ELLA(nn.Module):\n",
    "    def __init__(self, d, k, base_learner, base_learner_kwargs={}, mu=1, lam=1, k_init=False):\n",
    "        super(ELLA, self).__init__()\n",
    "        self.d = d\n",
    "        self.k = k\n",
    "        self.L = nn.Parameter(torch.randn(d, k))\n",
    "        self.A = torch.zeros((d * k, d * k), dtype=torch.float64)\n",
    "        self.b = torch.zeros((d * k, 1), dtype=torch.float64)\n",
    "        self.S = torch.zeros((k, 0), dtype=torch.float64)\n",
    "        self.T = 0\n",
    "        self.mu = mu\n",
    "        self.lam = lam\n",
    "        self.k_init = k_init\n",
    "\n",
    "        if base_learner in [nn.Linear, nn.Ridge]:\n",
    "            self.perf_metric = nn.functional.mse_loss\n",
    "        elif base_learner == nn.LogisticRegression:\n",
    "            self.perf_metric = nn.functional.binary_cross_entropy\n",
    "        else:\n",
    "            raise Exception(\"Unsupported Base Learner\")\n",
    "\n",
    "        self.base_learner = base_learner\n",
    "        self.base_learner_kwargs = base_learner_kwargs\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def fit(self, X, y, task_id):\n",
    "        self.T += 1\n",
    "        single_task_model = self.base_learner(self.d, 1, bias=False, **self.base_learner_kwargs).double()\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        optimizer = optim.SGD(single_task_model.parameters(), lr=0.01)\n",
    "\n",
    "        X_tensor = Variable(torch.from_numpy(X).double())\n",
    "        y_tensor = Variable(torch.from_numpy(y).double())\n",
    "\n",
    "        for epoch in range(100):  # adjust the number of epochs as needed\n",
    "            optimizer.zero_grad()\n",
    "            outputs = single_task_model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        D_t = self.get_hessian(single_task_model, X_tensor, y_tensor)\n",
    "        D_t_sqrt = torch.matrix_sqrt(D_t)\n",
    "        theta_t = single_task_model.weight.data.t()\n",
    "\n",
    "        sparse_encode = nn.functional.lasso(torch.mm(D_t_sqrt, self.L),\n",
    "                                            torch.mm(D_t_sqrt, theta_t.t()),\n",
    "                                            alpha=self.mu / (X.shape[0] * 2.0),\n",
    "                                            fit_intercept=False)\n",
    "        sparse_coeffs = sparse_encode.coef_\n",
    "\n",
    "        self.S = torch.cat((self.S, sparse_coeffs.t()))\n",
    "\n",
    "        self.A += torch.kron(self.S[:, task_id].view(-1, 1), self.S[:, task_id].view(1, -1)) * D_t\n",
    "        self.b += torch.kron(self.S[:, task_id].t(), theta_t.t() @ D_t).t()\n",
    "        L_vectorized = torch.inverse(self.A / self.T + self.lam * torch.eye(self.d * self.k, self.d * self.k,\n",
    "                                                                             dtype=torch.float64)) @ self.b / self.T\n",
    "        self.L.data = L_vectorized.view(self.k, self.d).t()\n",
    "        self.revive_dead_components()\n",
    "\n",
    "    def revive_dead_components(self):\n",
    "        for i, val in enumerate(torch.sum(self.L, dim=0)):\n",
    "            if abs(val) < 1e-8:\n",
    "                self.L[:, i] = torch.randn(self.d, dtype=torch.float64)\n",
    "\n",
    "    def predict(self, X, task_id):\n",
    "        if self.base_learner == nn.Linear or self.base_learner == nn.Ridge:\n",
    "            return X @ self.L @ self.S[:, task_id]\n",
    "        elif self.base_learner == nn.LogisticRegression:\n",
    "            return 1. / (1.0 + torch.exp(-X @ self.L @ self.S[:, task_id])) > 0.5\n",
    "\n",
    "    def score(self, X, y, task_id):\n",
    "        return self.perf_metric(self.predict(X, task_id), y)\n",
    "\n",
    "    def get_hessian(self, model, X, y):\n",
    "        theta_t = model.weight.data.t()\n",
    "        if self.base_learner == nn.Linear:\n",
    "            return X.t() @ X / (2.0 * X.shape[0])\n",
    "        elif self.base_learner == nn.Ridge:\n",
    "            return X.t() @ X / (2.0 * X.shape[0]) + model.weight_decay * torch.eye(self.d, dtype=torch.float64)\n",
    "        elif self.base_learner == nn.LogisticRegression:\n",
    "            preds = 1. / (1.0 + torch.exp(-X @ theta_t.t()))\n",
    "            base = preds * (1 - preds)\n",
    "            hessian = (base.view(1, -1) * X).t() @ X / (2.0 * X.shape[0])\n",
    "            return hessian + torch.eye(self.d, dtype=torch.float64) / (2.0 * model.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce90f560-9eae-4f71-a88d-7cfb187a8a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8d4621-bf3e-4e40-aa86-e3bda8871d8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Ridge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     29\u001b[0m noise_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.1\u001b[39m\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mELLA\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRidge\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m S_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(k,T)\n\u001b[0;32m     34\u001b[0m L_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(d,k)\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mELLA.__init__\u001b[1;34m(self, d, k, base_learner, base_learner_kwargs, mu, lam, k_init)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m=\u001b[39m lam\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_init \u001b[38;5;241m=\u001b[39m k_init\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_learner \u001b[38;5;129;01min\u001b[39;00m [nn\u001b[38;5;241m.\u001b[39mLinear, \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRidge\u001b[49m]:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperf_metric \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m base_learner \u001b[38;5;241m==\u001b[39m nn\u001b[38;5;241m.\u001b[39mLogisticRegression:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Ridge'"
     ]
    }
   ],
   "source": [
    "# ELLA.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def multi_task_train_test_split(Xs,Ys,train_size=0.5):\n",
    "    Xs_train = []\n",
    "    Ys_train = []\n",
    "    Xs_test = []\n",
    "    Ys_test = []\n",
    "    for t in range(len(Xs)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xs[t], np.squeeze(Ys[t]), train_size=train_size)\n",
    "        Xs_train.append(X_train)\n",
    "        Xs_test.append(X_test)\n",
    "        Ys_train.append(y_train)\n",
    "        Ys_test.append(y_test)\n",
    "    return Xs_train, Xs_test, Ys_train, Ys_test\n",
    "\n",
    "#from ELLA import ELLA\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "from scipy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "T = 20\n",
    "d = 10\n",
    "n = 100\n",
    "k = 5\n",
    "noise_var = .1\n",
    "\n",
    "model = ELLA(d,k,Ridge,mu=1,lam=10**-5)\n",
    "\n",
    "S_true = np.random.randn(k,T)\n",
    "L_true = np.random.randn(d,k)\n",
    "w_true = L_true.dot(S_true)\n",
    "\n",
    "# make sure to add a bias term (it is not done automatically)\n",
    "Xs = [np.hstack((np.random.randn(n,d-1), np.ones((n,1)))) for i in range(T)]\n",
    "# generate the synthetic labels\n",
    "Ys = [Xs[i].dot(w_true[:,i]) + noise_var*np.random.randn(n,) for i in range(T)]\n",
    "# break into train and test sets\n",
    "Xs_train, Xs_test, Ys_train, Ys_test = multi_task_train_test_split(Xs,Ys,train_size=0.5)\n",
    "\n",
    "for t in range(T):\n",
    "    model.fit(Xs_train[t], Ys_train[t], t)\n",
    "#print \"Average explained variance score\", np.mean([model.score(Xs_test[t], Ys_test[t], t) for t in range(T)])\n",
    "print(\"Average explained variance score\", np.mean([model.score(Xs_test[t], Ys_test[t], t) for t in range(T)]))\n",
    "\n",
    "# Try out a classification problem\n",
    "Ys_binarized_train = [Ys_train[i] > 0 for i in range(T)]\n",
    "Ys_binarized_test = [Ys_test[i] > 0 for i in range(T)]\n",
    "\n",
    "model = ELLA(d,k,LogisticRegression,mu=1,lam=10**-5)\n",
    "for t in range(T):\n",
    "    model.fit(Xs_train[t], Ys_binarized_train[t], t)\n",
    "\n",
    "#print \"Average classification accuracy\", np.mean([model.score(Xs_test[t], Ys_binarized_test[t], t) for t in range(T)])\n",
    "print(\"Average classification accuracy\", np.mean([model.score(Xs_test[t], Ys_binarized_test[t], t) for t in range(T)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3c34e-905e-4a69-a2df-4bf198750e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = loadmat('landminedata.mat')\n",
    "\n",
    "Xs_lm = []\n",
    "Ys_lm = []\n",
    "for t in range(data['feature'].shape[1]):\n",
    "    X_t = data['feature'][0,t]\n",
    "    Xs_lm.append(np.hstack((X_t,np.ones((X_t.shape[0],1)))))\n",
    "    Ys_lm.append(data['label'][0,t] == 1.0)\n",
    "\n",
    "d = Xs_lm[0].shape[1]\n",
    "k = 1\n",
    "\n",
    "Xs_lm_train, Xs_lm_test, Ys_lm_train, Ys_lm_test = multi_task_train_test_split(Xs_lm,Ys_lm,train_size=0.5)\n",
    "model = ELLA(d,k,LogisticRegression,{'C':10**0},mu=1,lam=10**-5)\n",
    "for t in range(T):\n",
    "    model.fit(Xs_lm_train[t], Ys_lm_train[t], t)\n",
    "\n",
    "print model.S    \n",
    "\n",
    "print \"Average AUC:\", np.mean([roc_auc_score(Ys_lm_test[t],\n",
    "                                             model.predict_logprobs(Xs_lm_test[t], t))\n",
    "                               for t in range(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b0063-9a40-4022-b2bb-8ccff5e2110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde1c8e-5a82-47d1-8e45-824c49168dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456a377-5a08-4d59-93cf-e8134881d56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1faf41d3-3d34-4600-a05a-4e2c01142f6c",
   "metadata": {},
   "source": [
    "## Elastic Weight Consolidation Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511adf7-ff2f-46f4-a0b9-13f134578d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the model architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "model = RegressionModel(input_dim=10, output_dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model using EWC\n",
    "fisher_matrices = []\n",
    "for name, param in model.named_parameters():\n",
    "    fisher_matrices.append(torch.zeros_like(param))\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(X_train, y_train)):\n",
    "        inputs, labels = data\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute the Fisher information matrix for each parameter\n",
    "        loss.backward()\n",
    "        for j, (name, param) in enumerate(model.named_parameters()):\n",
    "            fisher_matrices[j] += (param.grad.detach() ** 2) / len(X_train)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(X_train)}\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for i, data in enumerate(zip(X_test, y_test)):\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        print(f\"Test Loss: {test_loss/len(X_test)}\")\n",
    "\n",
    "    # Apply EWC to the model\n",
    "    for j, (name, param) in enumerate(model.named_parameters()):\n",
    "        penalty = 0.0\n",
    "        for k in range(epoch):\n",
    "            old_param = deepcopy(model).state_dict()[name]\n",
    "            old_fisher = fisher_matrices[j]\n",
    "            new_fisher = torch.zeros_like(old_fisher)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            new_fisher += (param.grad.detach() ** 2) / len(X_train)\n",
    "            penalty += ((old_param - param) ** 2) * (old_fisher + new_fisher)\n",
    "        param.grad += penalty\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090019f9-bc38-439e-9d1f-eb22523e3ebc",
   "metadata": {},
   "source": [
    "# EWC, LwF, and IMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691275f6-6df0-42b0-80ed-c343d55acba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the model architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "model = RegressionModel(input_dim=10, output_dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Select the lifelong learning method to use\n",
    "lifelong_learning_method = \"EWC\"  # Change this to \"LwF\" or \"IMM\" to use a different method\n",
    "\n",
    "if lifelong_learning_method == \"EWC\":\n",
    "    # Train the model using Elastic Weight Consolidation\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(zip(X_train, y_train)):\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute the Fisher information matrix for each parameter\n",
    "            loss.backward()\n",
    "            for j, (name, param) in enumerate(model.named_parameters()):\n",
    "                fisher_matrices[j] += (param.grad.detach() ** 2) / len(X_train)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {running_loss/len(X_train)}\")\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for i, data in enumerate(zip(X_test, y_test)):\n",
    "                inputs, labels = data\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "                labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            print(f\"Test Loss: {test_loss/len(X_test)}\")\n",
    "\n",
    "        # Apply EWC to the model\n",
    "        for j, (name, param) in enumerate(model.named_parameters()):\n",
    "            penalty = 0.0\n",
    "            for k in range(epoch):\n",
    "                old_param = deepcopy(model).state_dict()[name]\n",
    "                old_fisher = fisher_matrices[j]\n",
    "                new_fisher = torch.zeros_like(old_fisher)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                new_fisher += (param.grad.detach() ** 2) / len(X_train)\n",
    "                penalty += ((old_param - param) ** 2) * (old_fisher + new_fisher)\n",
    "            param.grad += penalty\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "elif lifelong_learning_method == \"LwF\":\n",
    "    # Train the model using Learning without Forgetting\n",
    "    old_params = [param.clone() for param in model.parameters()]\n",
    "    alpha = [1.0 for _ in range(len(old_params))]\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(zip(X_train, y_train)):\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Add the penalty term to the gradient\n",
    "            loss.backward()\n",
    "            for j, (name, param) in enumerate(model.named_parameters()):\n",
    "                param.grad += (param.data - old_params[j].data) / alpha[j]\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {running_loss/len(X_train)}\")\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        #...\n",
    "\n",
    "        # Save the current model parameters for the next epoch\n",
    "        old_params = [param.clone() for param in model.parameters()]\n",
    "        alpha = [1.0 for _ in range(len(old_params))]\n",
    "\n",
    "elif lifelong_learning_method == \"IMM\":\n",
    "    # Train the model using Incremental Moment Matching\n",
    "    old_params = [param.clone() for param in model.parameters()]\n",
    "    alpha = [1.0 for _ in range(len(old_params))]\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(zip(X_train, y_train)):\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Add the penalty term to the gradient\n",
    "            loss.backward()\n",
    "            for j, (name, param) in enumerate(model.named_parameters()):\n",
    "                param.grad += (param.data - old_params[j].data) / alpha[j]\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {running_loss/len(X_train)}\")\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for i, data in enumerate(zip(X_test, y_test)):\n",
    "                inputs, labels = data\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "                labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            print(f\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28302f8f-d898-4686-a526-fb9761b1977a",
   "metadata": {},
   "source": [
    "## Combining All Three Into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add31b8e-0af0-4604-a895-961929e138bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the model architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfaf126-f59d-439a-96f2-57de2b913b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "model = RegressionModel(input_dim=10, output_dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Select the lifelong learning method to use\n",
    "lifelong_learning_method = \"EWC\"  # Change this to \"LwF\" or \"IMM\" to use a different method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d96b0-8a91-4af9-96cf-2c2c382fec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lifelong_learning_method == \"LwF\" or lifelong_learning_method == \"IMM\":\n",
    "    # Train the model using Learning without Forgetting / Incremental Moment Matching\n",
    "    old_params = [param.clone() for param in model.parameters()]\n",
    "    alpha = [1.0 for _ in range(len(old_params))]\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(X_train, y_train)):\n",
    "        inputs, labels = data\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        if lifelong_learning_method == \"EWC\":\n",
    "            # Compute the Fisher information matrix for each parameter\n",
    "            for j, (name, param) in enumerate(model.named_parameters()):\n",
    "                fisher_matrices[j] += (param.grad.detach() ** 2) / len(X_train)\n",
    "        elif lifelong_learning_method == \"LwF\" or lifelong_learning_method == \"IMM\":\n",
    "            # Add the penalty term to the gradient\n",
    "            for j, (name, param) in enumerate(model.named_parameters()):\n",
    "                param.grad += (param.data - old_params[j].data) / alpha[j]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(X_train)}\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for i, data in enumerate(zip(X_test, y_test)):\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        print(f\"Test Loss: {test_loss/len(X_test)}\")\n",
    "\n",
    "    \n",
    "    if lifelong_learning_method == \"EWC\":\n",
    "        # Apply EWC to the model\n",
    "        for j, (name, param) in enumerate(model.named_parameters()):\n",
    "            penalty = 0.0\n",
    "            for k in range(epoch):\n",
    "                old_param = deepcopy(model).state_dict()[name]\n",
    "                old_fisher = fisher_matrices[j]\n",
    "                new_fisher = torch.zeros_like(old_fisher)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                new_fisher += (param.grad.detach() ** 2) / len(X_train)\n",
    "                penalty += ((old_param - param) ** 2) * (old_fisher + new_fisher)\n",
    "            param.grad += penalty\n",
    "        optimizer.step()\n",
    "    elif lifelong_learning_method == \"LwF\":\n",
    "        # Save the current model parameters for the next epoch\n",
    "        old_params = [param.clone() for param in model.parameters()]\n",
    "        alpha = [1.0 for _ in range(len(old_params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024aba46-d7bf-4653-a17c-7aaa6ce915a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526164c-175c-46bf-beb2-daa057579aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef382542-8d24-4cf3-906a-d12ee39b1fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607261bf-17c5-43ca-a3f7-fe2dce9138cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the model architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "model = RegressionModel(input_dim=10, output_dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the penalty term for IMM\n",
    "penalty = 0.1\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, optimizer, criterion, X_train, y_train, X_test, y_test, penalty=None):\n",
    "    old_params = [param.clone() for param in model.parameters()]\n",
    "    alpha = [1.0 for _ in range(len(old_params))]\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(zip(X_train, y_train)):\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Add the penalty term to the loss function\n",
    "            if penalty is not None:\n",
    "                for j, (name, param) in enumerate(model.named_parameters()):\n",
    "                    loss += penalty * ((param - old_params[j]) ** 2).sum()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {running_loss/len(X_train)}\")\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for i, data in enumerate(zip(X_test, y_test)):\n",
    "                inputs, labels = data\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "                labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            print(f\"Test Loss: {test_loss/len(X_test)}\")\n",
    "\n",
    "        # Save the current model parameters for the next epoch\n",
    "        old_params = [param.clone() for param in model.parameters()]\n",
    "        alpha = [1.0 for _ in range(len(old_params))]\n",
    "\n",
    "# Train the model using LwF\n",
    "train_model(model, optimizer, criterion, X_train, y_train, X_test, y_test, penalty=None)\n",
    "\n",
    "# Train the model using IMM\n",
    "train_model(model, optimizer, criterion, X_train, y_train, X_test, y_test, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74286-1331-4f60-8180-62cc69a2d943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe8e9ae-6721-43db-9f01-01d9d7d95153",
   "metadata": {},
   "source": [
    "BingAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c0967-ac53-40e8-a654-fd8b2efb3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Code for using Incremental Moment Matching\n",
    "def imm(model_old, model_new, alpha=0.5):\n",
    "    for param_old, param_new in zip(model_old.parameters(), model_new.parameters()):\n",
    "        param_new.data.copy_(alpha * param_old.data + (1 - alpha) * param_new.data)\n",
    "\n",
    "# Defining the model architecture\n",
    "class LifelongRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=100):\n",
    "        super(LifelongRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Use Incremental Moment Matching\n",
    "    imm(model, new_model)\n",
    "\n",
    "    # Update the old model with the new one\n",
    "    model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b3be8-d700-429d-ae05-5fdffdfe6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Defining the model architecture\n",
    "class LifelongRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=100):\n",
    "        super(LifelongRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model on the first task\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Train the model on a new task\n",
    "new_model = LifelongRegressor(input_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(new_model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(new_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = new_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Compute the distillation loss\n",
    "        if epoch > 0:\n",
    "            old_outputs = model(inputs)\n",
    "            distillation_loss = criterion(outputs[:, :num_outputs], old_outputs.detach())\n",
    "            loss += alpha * distillation_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update the old model with the new one\n",
    "    model.load_state_dict(new_model.state_dict())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
