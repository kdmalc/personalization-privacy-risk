{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f27ea73-e5e5-44b0-8f28-5fb19dd51faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "#from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d2dba-bb1f-4894-a0ad-576bcb01af7c",
   "metadata": {},
   "source": [
    "> All the copy and pasted imports since Jupyter doesnt treat the environment as a package..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f15cbf-126e-407b-a24d-1697ff714564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase:\n",
    "    def __init__(self, ID, w, opt_method, smoothbatch_lr=1, alphaF=0.0, alphaE=1e-6, alphaD=1e-4, verbose=False, starting_update=9, PCA_comps=64, current_round=0, num_clients=14, log_init=0):\n",
    "        # Not input\n",
    "        self.num_updates = 19\n",
    "        self.starting_update=starting_update\n",
    "        self.update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n",
    "        self.id2color = {0:'lightcoral', 1:'maroon', 2:'chocolate', 3:'darkorange', 4:'gold', 5:'olive', 6:'olivedrab', \n",
    "                7:'lawngreen', 8:'aquamarine', 9:'deepskyblue', 10:'steelblue', 11:'violet', 12:'darkorchid', 13:'deeppink'}\n",
    "        \n",
    "        self.type = 'BaseClass'\n",
    "        self.ID = ID\n",
    "        self.PCA_comps = PCA_comps\n",
    "        self.pca_channel_default = 64  # When PCA_comps equals this, DONT DO PCA\n",
    "        if w.shape!=(2, self.PCA_comps):\n",
    "            #print(f\"Class BaseModel: Overwrote the provided init decoder: {w.shape} --> {(2, self.PCA_comps)}\")\n",
    "            self.w = np.random.rand(2, self.PCA_comps)\n",
    "        else:\n",
    "            self.w = w\n",
    "        self.w_prev = copy.deepcopy(self.w)\n",
    "        self.dec_log = [copy.deepcopy(self.w)]\n",
    "        self.w_prev = copy.deepcopy(self.w)\n",
    "        self.num_clients = num_clients\n",
    "        self.log_init = log_init\n",
    "\n",
    "        self.alphaF = alphaF\n",
    "        self.alphaE = alphaE\n",
    "        self.alphaD = alphaD\n",
    "\n",
    "        self.local_train_error_log = []\n",
    "        self.global_train_error_log = []\n",
    "        self.local_test_error_log = []\n",
    "        self.global_test_error_log = []\n",
    "        \n",
    "        self.opt_method = opt_method.upper()\n",
    "        self.current_round = current_round\n",
    "        self.verbose = verbose\n",
    "        self.smoothbatch_lr = smoothbatch_lr\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"{self.type}{self.ID}\"\n",
    "    \n",
    "    def display_info(self): \n",
    "        return f\"{self.type} model: {self.ID}\\nCurrent Round: {self.current_round}\\nOptimization Method: {self.opt_method}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67393983-82f9-4aa3-a4fb-a605a0d8df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server(ModelBase):\n",
    "    def __init__(self, ID, D0, opt_method, global_method, all_clients, smoothbatch_lr=0.75, C=0.35, test_split_type='kfoldcv', \n",
    "                 num_kfolds=5, test_split_frac=0.3, current_round=0, PCA_comps=64, verbose=False, validate_memory_IDs=True, save_client_loss_logs=True, \n",
    "                 sequential=False, current_datatime=\"Overwritten\"):\n",
    "        super().__init__(ID, D0, opt_method, smoothbatch_lr=smoothbatch_lr, current_round=current_round, PCA_comps=PCA_comps, \n",
    "                         verbose=verbose, num_clients=14, log_init=0)\n",
    "        \n",
    "        self.type = 'Server'\n",
    "        self.save_client_loss_logs = save_client_loss_logs\n",
    "        self.sequential = sequential\n",
    "\n",
    "        # CLIENT LISTS!\n",
    "        self.num_avail_clients = 0\n",
    "        self.available_clients_lst = [0]*len(all_clients)  # Why is this not just an empty list...\n",
    "        # ^ This should really be: [cli for cli in all_clients if cli.availability==1]\n",
    "        self.num_chosen_clients = 0\n",
    "        self.chosen_clients_lst = [0]*len(all_clients)  # Why is this not just an empty list...\n",
    "        self.all_clients = all_clients  # ALL TRAIN AND VAL AND STATIC/UNAVAILABLE/QUEUED CLIENTS ARE PASSED IN HERE!!!\n",
    "        self.num_total_clients = len(self.all_clients)\n",
    "        # THE BELOW SHOULD NOT CHANGE DURING THE RUN!\n",
    "        ## Maybe train_clients can/should change, to reflect availability in the seq case? ...\n",
    "        self.train_clients = [cli for cli in self.all_clients if cli.val_set is False]\n",
    "        self.val_clients = [cli for cli in self.all_clients if cli.val_set is True]\n",
    "        self.num_train_clients = len(self.train_clients)\n",
    "        self.num_test_clients = len(self.val_clients)\n",
    "        # SET AVAILABLE CLIENT LIST\n",
    "        self.set_available_clients_list()\n",
    "        # Init all train clients with simulate_streaming so they have self.s, self.F, and self.p_reference\n",
    "        for client in self.train_clients:\n",
    "            client.simulate_data_stream()\n",
    "\n",
    "        self.global_method = global_method.upper()\n",
    "        print(f\"Running the {self.global_method} algorithm as the global method!\")\n",
    "        self.C = C  # Fraction of clients to use each round\n",
    "\n",
    "        # TESTING\n",
    "        self.test_split_type = test_split_type.upper()\n",
    "        self.num_kfolds = num_kfolds\n",
    "        self.test_split_frac = test_split_frac\n",
    "\n",
    "        # SAVE FILE RELATED\n",
    "        # Get the directory of the current script\n",
    "        self.script_directory = os.path.dirname(os.path.abspath(__file__))  # This returns the path to serverbase... so don't index the end of the path\n",
    "        # Relative path to results dir\n",
    "        self.result_path = \"\\\\results\\\\\"\n",
    "        if current_datatime is None:\n",
    "            self.set_save_filename()\n",
    "\n",
    "        # EVALUATE INIT MODEL AND SAVE THAT LOSS\n",
    "        ## See if this fixes each algo having different init losses...\n",
    "        for client_idx, my_client in enumerate(self.train_clients): # Eg all train-able clients (no witheld val clients from kfoldcv)\n",
    "            # test_metrics for all clients\n",
    "            local_test_loss, _ = my_client.test_metrics(my_client.w, 'local')\n",
    "            local_train_loss, _ = my_client.train_metrics(my_client.w, 'local')\n",
    "            if self.global_method!='NOFL':\n",
    "                global_test_loss, _ = my_client.test_metrics(self.w, 'global')\n",
    "                global_train_loss, _ = my_client.train_metrics(self.w, 'global')\n",
    "            else:\n",
    "                global_test_loss = 0\n",
    "                global_train_loss = 0\n",
    "\n",
    "            if client_idx!=0:\n",
    "                running_global_test_loss += global_test_loss\n",
    "                running_local_test_loss += local_test_loss\n",
    "                running_global_train_loss += global_train_loss\n",
    "                running_local_train_loss += local_train_loss\n",
    "            else:\n",
    "                running_global_test_loss = global_test_loss\n",
    "                running_local_test_loss = local_test_loss\n",
    "                running_global_train_loss = global_train_loss\n",
    "                running_local_train_loss = local_train_loss\n",
    "        self.local_test_error_log.append(running_local_test_loss / len(self.train_clients))\n",
    "        self.local_train_error_log.append(running_local_train_loss / len(self.train_clients))\n",
    "        if self.global_method!='NOFL':\n",
    "            self.global_test_error_log.append(running_global_test_loss / len(self.train_clients))\n",
    "            self.global_train_error_log.append(running_global_train_loss / len(self.train_clients))\n",
    "        \n",
    "\n",
    "    def set_save_filename(self, current_datetime=None):\n",
    "        if current_datetime is None:\n",
    "            # get current date and time\n",
    "            current_datetime = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
    "\n",
    "        # convert datetime obj to string\n",
    "        self.str_current_datetime = str(current_datetime)\n",
    "        # Specify the relative path from the script's directory\n",
    "        self.relative_path = self.result_path + self.str_current_datetime + \"_\" + self.global_method\n",
    "        # Combine the script's directory and the relative path to get the full path\n",
    "        self.trial_result_path = self.script_directory + self.relative_path\n",
    "        self.h5_file_path = os.path.join(self.trial_result_path, f\"{self.opt_method}_{self.global_method}\")\n",
    "        self.paramtxt_file_path = os.path.join(self.trial_result_path, \"param_log.txt\")\n",
    "        if not os.path.exists(self.trial_result_path):\n",
    "            os.makedirs(self.trial_result_path)\n",
    "\n",
    "                \n",
    "    # Main Loop\n",
    "    def execute_FL_loop(self):\n",
    "        # Update global round number\n",
    "        self.current_round += 1\n",
    "        \n",
    "        if self.global_method=='FEDAVG' or 'PFA' in self.global_method:\n",
    "            # Choose fraction C of available clients\n",
    "            self.set_available_clients_list()\n",
    "            self.choose_clients()\n",
    "            for my_client in self.all_clients:  \n",
    "                if my_client.val_set==True:\n",
    "                    # This is a val client, so don't log anything\n",
    "                    # Could be included in train_metrics but doesnt need to be\n",
    "                    continue\n",
    "                elif my_client.availability==False:\n",
    "                    raise ValueError(\"Sequential case not implemented yet\")\n",
    "                elif my_client not in self.chosen_clients_lst:\n",
    "                    # Just not getting trained this round\n",
    "                    continue\n",
    "                # THESE ARE THE CLIENTS WHICH ACTUALLY GET TRAINED!\n",
    "                my_client.latest_global_round = self.current_round          \n",
    "                # Send those clients the current global model\n",
    "                my_client.global_w = copy.deepcopy(self.w)\n",
    "                my_client.execute_training_loop()\n",
    "            # AGGREGATION\n",
    "            self.agg_local_weights()  # This func sets self.w, eg the new decoder\n",
    "            # GLOBAL SmoothBatch\n",
    "            self.w = self.smoothbatch_lr*self.w_prev + ((1 - self.smoothbatch_lr)*self.w)\n",
    "        elif self.global_method=='NOFL':\n",
    "            # TODO: Is NoFL just supposed to be the Local CPHS sims... if so this is fine I think \n",
    "            for my_client in self.all_clients:  \n",
    "                if my_client.val_set==True:\n",
    "                    # This is a val client, so don't log anything\n",
    "                    # Could be included in train_metrics but doesnt need to be\n",
    "                    continue\n",
    "                elif my_client.availability==False:\n",
    "                    raise ValueError(\"Sequential case not implemented yet\")\n",
    "                # THESE ARE THE CLIENTS WHICH ACTUALLY GET TRAINED!\n",
    "                my_client.latest_global_round = self.current_round          \n",
    "                my_client.execute_training_loop()\n",
    "        else:\n",
    "            raise('Method not currently supported, please reset method to FedAvg')\n",
    "        \n",
    "        # Save the new decoder to the log\n",
    "        self.dec_log.append(copy.deepcopy(self.w))\n",
    "        # Run train_metrics and test_metrics to log performance on training/testing data\n",
    "        for client_idx, my_client in enumerate(self.train_clients): # Eg all train-able clients (no witheld val clients from kfoldcv)\n",
    "            # Reset all clients so no one is chosen for the next round\n",
    "            my_client.chosen_status = 0\n",
    "            \n",
    "            # test_metrics for all clients\n",
    "            local_test_loss, _ = my_client.test_metrics(my_client.w, 'local')\n",
    "            local_train_loss, _ = my_client.train_metrics(my_client.w, 'local')\n",
    "            if self.global_method=='FEDAVG' or 'PFA' in self.global_method:\n",
    "                global_test_loss, _ = my_client.test_metrics(self.w, 'global')\n",
    "                global_train_loss, _ = my_client.train_metrics(self.w, 'global')\n",
    "            elif self.global_method=='NOFL':\n",
    "                global_test_loss = 0\n",
    "                global_train_loss = 0\n",
    "            \n",
    "            if client_idx!=0:\n",
    "                running_global_test_loss += global_test_loss\n",
    "                running_local_test_loss += local_test_loss\n",
    "\n",
    "                running_global_train_loss += global_train_loss\n",
    "                running_local_train_loss += local_train_loss\n",
    "            else:\n",
    "                running_global_test_loss = global_test_loss\n",
    "                running_local_test_loss = local_test_loss\n",
    "\n",
    "                running_global_train_loss = global_train_loss\n",
    "                running_local_train_loss = local_train_loss\n",
    "\n",
    "        # SERVER AVERAGE PERFORMANCE\n",
    "        # Divide by the number of clients to get average loss per client\n",
    "        ## For local, having the individual client logs would be better but they would probably get averaged anyways when plotting so\n",
    "        self.local_test_error_log.append(running_local_test_loss / len(self.train_clients))\n",
    "        self.local_train_error_log.append(running_local_train_loss / len(self.train_clients))\n",
    "        if self.global_method!='NOFL':\n",
    "            self.global_test_error_log.append(running_global_test_loss / len(self.train_clients))\n",
    "            self.global_train_error_log.append(running_global_train_loss / len(self.train_clients))\n",
    "            \n",
    "            \n",
    "    def set_available_clients_list(self):\n",
    "        # TODO come back to this depending on how available_clients_full_idx_lst shakes out...\n",
    "        self.available_clients_full_idx_lst = [0]*len(self.train_clients)\n",
    "        for idx, my_client in enumerate(self.train_clients):\n",
    "            if my_client.availability:\n",
    "                self.available_clients_full_idx_lst[idx] = my_client\n",
    "        # cli can be 0 if that client is not available this round... --> I think self.train_clients would need to be self.all_clients above tho...\n",
    "        self.available_clients_lst = [cli for cli in self.available_clients_full_idx_lst if cli != 0]\n",
    "        self.num_avail_clients = len(self.available_clients_lst)\n",
    "    \n",
    "\n",
    "    def choose_clients(self):\n",
    "        # Check what client are available this round\n",
    "        self.set_available_clients_list()\n",
    "        # Now choose frac C clients from the resulting available clients\n",
    "        if self.num_avail_clients > 0:\n",
    "            self.num_chosen_clients = int(np.ceil(self.num_avail_clients*self.C))\n",
    "            if self.num_chosen_clients<1:\n",
    "                raise ValueError(f\"ERROR: Chose {self.num_chosen_clients} clients for some reason, must choose more than 1\")\n",
    "            # Right now it chooses 2 at random: 14*.1=1.4 --> 2\n",
    "            self.chosen_clients_lst = random.sample(self.available_clients_lst, len(self.available_clients_lst))[:self.num_chosen_clients]\n",
    "            for my_client in self.chosen_clients_lst:\n",
    "                my_client.chosen_status = 1\n",
    "        else:\n",
    "            raise(f\"ERROR: Number of available clients must be greater than 0: {self.num_avail_clients}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca80c71-4f45-4efa-aae1-aa9fdb00ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\kdmen\\Desktop\\Research\\Data\\CPHS_EMG'\n",
    "model_saving_dir = r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\PythonVersion\\PythonSimsRevamp\\models\"\n",
    "cond0_filename = r'\\cond0_dict_list.p'\n",
    "all_decs_init_filename = r'\\all_decs_init.p'\n",
    "nofl_decs_filename = r'\\nofl_decs.p'\n",
    "id2color = {0:'lightcoral', 1:'maroon', 2:'chocolate', 3:'darkorange', 4:'gold', 5:'olive', 6:'olivedrab', \n",
    "            7:'lawngreen', 8:'aquamarine', 9:'deepskyblue', 10:'steelblue', 11:'violet', 12:'darkorchid', 13:'deeppink'}\n",
    "implemented_client_training_methods = ['GD', 'FullScipyMin', 'MaxIterScipyMin']\n",
    "NUM_USERS = 14\n",
    "# For exclusion when plotting later on\n",
    "#bad_nodes = [] #[1,3,13]\n",
    "D_0 = np.random.rand(2,64)\n",
    "num_updates = 18\n",
    "step_indices = list(range(num_updates))\n",
    "\n",
    "MAX_ITER=None  # For MAXITERSCIPYMIN. Use FULLSCIPYMIN for complete minimization, otherwise stay with 1\n",
    "\n",
    "COLORS_LST = ['red', 'blue', 'magenta', 'orange', 'darkviolet', 'lime']\n",
    "ALPHA = 0.7\n",
    "\n",
    "# get current date and time\n",
    "CURRENT_DATETIME = str(datetime.now().strftime(\"%m-%d_%H-%M\"))\n",
    "\n",
    "STARTING_UPDATE=10\n",
    "DATA_STREAM='streaming'\n",
    "NUM_KFOLDS=5\n",
    "USE_HITBOUNDS = False\n",
    "PLOT_EACH_FOLD = False\n",
    "USE_KFOLDCV = True\n",
    "TEST_SPLIT_TYPE = 'KFOLDCV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e7ff44-251e-4218-bb86-d44ba7cc69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_cost_l2(F, D, V, alphaD=1e-4, alphaE=1e-6, \n",
    "                     Nd=2, Ne=64, flatten=True):\n",
    "    D = np.reshape(D,(Nd, Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    if flatten:\n",
    "        return (2*(D@F - Vplus)@F.T*(alphaE) + 2*alphaD*D ).flatten()\n",
    "    else:\n",
    "        return 2*(D@F - Vplus)@F.T*(alphaE) + 2*alphaD*D \n",
    "\n",
    "def cost_l2(F, D, V, alphaD=1e-4, alphaE=1e-6, Nd=2, Ne=64, return_cost_func_comps=False):\n",
    "    D = np.reshape(D,(Nd,Ne))\n",
    "    Vplus = V[:,1:]\n",
    "    # Performance\n",
    "    term1 = alphaE*(np.linalg.norm((D@F - Vplus))**2)\n",
    "    # D Norm (Decoder Effort)\n",
    "    term2 = alphaD*(np.linalg.norm(D)**2)\n",
    "    # F Norm (User Effort)\n",
    "    #term3 = alphaF*(np.linalg.norm(F)**2)\n",
    "    if return_cost_func_comps:\n",
    "        return (term1 + term2), term1, term2\n",
    "    else:\n",
    "        return (term1 + term2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f21dd-96a0-4a2f-9c20-f2987e9c118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_METHOD = \"NOFL\"  #FedAvg #PFAFO_GDLS #NOFL\n",
    "OPT_METHOD = 'FULLSCIPYMIN' if GLOBAL_METHOD==\"NOFL\" else 'GDLS'\n",
    "GLOBAL_ROUNDS = 12 if GLOBAL_METHOD==\"NOFL\" else 250\n",
    "LOCAL_ROUND_THRESHOLD = 1 if GLOBAL_METHOD==\"NOFL\" else 20\n",
    "NUM_STEPS = 3  # This is basically just local_epochs. Num_grad_steps\n",
    "BETA=0.01  # Not used with GDLS? Only pertains to PFA regardless\n",
    "LR=1  # Not used with GDLS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f24c7e-aa75-49ca-9f97-02e763e62b75",
   "metadata": {},
   "source": [
    "## CROSS-SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0eff1a-1985-4a99-932c-dd237678a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIO = \"CROSS\"\n",
    "\n",
    "# THIS K FOLD SCHEME IS ONLY FOR CROSS-SUBJECT ANALYSIS!!!\n",
    "# Define number of folds\n",
    "kf = KFold(n_splits=NUM_KFOLDS)\n",
    "# Assuming cond0_training_and_labels_lst is a list of labels for 14 clients\n",
    "user_ids = list(range(14))\n",
    "folds = list(kf.split(user_ids))\n",
    "\n",
    "for fold_idx, (train_ids, test_ids) in enumerate(folds):\n",
    "    print(f\"Fold {fold_idx+1}/{NUM_KFOLDS}\")\n",
    "    print(f\"{len(train_ids)} Train_IDs: {train_ids}\")\n",
    "    print(f\"{len(test_ids)} Test_IDs: {test_ids}\")\n",
    "    \n",
    "    # Initialize clients for training\n",
    "    train_clients = [Client(i, copy.deepcopy(D_0), OPT_METHOD, cond0_training_and_labels_lst[i], DATA_STREAM,\n",
    "                            beta=BETA, scenario=SCENARIO, local_round_threshold=LOCAL_ROUND_THRESHOLD, lr=LR, current_fold=fold_idx, num_kfolds=NUM_KFOLDS, global_method=GLOBAL_METHOD, max_iter=MAX_ITER, \n",
    "                            num_steps=NUM_STEPS, use_zvel=USE_HITBOUNDS, test_split_type=TEST_SPLIT_TYPE) for i in train_ids]\n",
    "    # Initialize clients for testing\n",
    "    test_clients = [Client(i, copy.deepcopy(D_0), OPT_METHOD, cond0_training_and_labels_lst[i], DATA_STREAM,\n",
    "                           beta=BETA, scenario=SCENARIO, local_round_threshold=LOCAL_ROUND_THRESHOLD, lr=LR, current_fold=fold_idx, availability=False, val_set=True, num_kfolds=NUM_KFOLDS, global_method=GLOBAL_METHOD, max_iter=MAX_ITER, \n",
    "                           num_steps=NUM_STEPS, use_zvel=USE_HITBOUNDS, test_split_type=TEST_SPLIT_TYPE) for i in test_ids]\n",
    "\n",
    "    testing_datasets_lst = []\n",
    "    for test_cli in test_clients:\n",
    "        testing_datasets_lst.append(test_cli.get_testing_dataset())\n",
    "    for train_cli in train_clients:\n",
    "        train_cli.set_testset(testing_datasets_lst)\n",
    "\n",
    "    full_client_lst = train_clients+test_clients\n",
    "\n",
    "    for cli in train_clients:\n",
    "        for model in my_testing_models:\n",
    "            test_loss, vel_error, dec_error = cli.test_metrics(model, which=\"global\", return_cost_func_comps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1db67-1fe3-468b-a3d6-2e614e22116c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceabed0b-4eb1-4838-923b-293e18596bbb",
   "metadata": {},
   "source": [
    "## INTRA-SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bfa8d-2fc1-4109-9474-144cb2265fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIO = \"INTRA\"\n",
    "for fold_idx in range(NUM_KFOLDS):\n",
    "    print(f\"Fold {fold_idx+1}/{NUM_KFOLDS}\")\n",
    "    # Initialize clients for training\n",
    "    full_client_lst = [Client(i, copy.deepcopy(D_0), OPT_METHOD, cond0_training_and_labels_lst[i], DATA_STREAM, \n",
    "                            scenario=SCENARIO, local_round_threshold=LOCAL_ROUND_THRESHOLD, current_fold=fold_idx, global_method=GLOBAL_METHOD, max_iter=MAX_ITER, \n",
    "                            num_steps=NUM_STEPS, use_zvel=USE_HITBOUNDS, test_split_type=TEST_SPLIT_TYPE) for i in range(NUM_USERS)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
