{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89f9c39b-5e49-4097-b4ef-97fbd2832a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import h5py\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1725c-7d47-4a1b-9b0d-0f3600660e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_num_kfolds = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5d89b-14a8-455b-8913-55c31f6005b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTRA\n",
    "'09-19_22-56_NOFL'\n",
    "'09-19_23-04_FEDAVG' \n",
    "'09-19_23-05_PFAFO_GDLS'\n",
    "\n",
    "# CROSS\n",
    "'09-19_23-07_PFAFO_GDLS'\n",
    "'09-19_23-25_FEDAVG'\n",
    "'09-19_23-30_NOFL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e115b-b345-4598-8e48-1636927a8bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04eb05-5e3b-4ff5-ae9d-ca0313647d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81b73345-c201-4d65-a2f7-f1e91ff4942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['client_local_model_log', 'client_local_test_log', 'global_dec_log', 'global_test_error_log', 'global_train_error_log', 'gradient_norm_lists_by_client', 'local_test_error_log', 'local_train_error_log']\n"
     ]
    }
   ],
   "source": [
    "# CROSS\n",
    "\n",
    "cv_results_path = r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\PythonVersion\\PythonSimsRevamp\\results\\FinalSims\"\n",
    "nofl_filename = \"FULLSCIPYMIN_NOFL_KFold0.h5\" \n",
    "fedavg_filename = \"GDLS_FEDAVG_KFold0.h5\" \n",
    "\n",
    "directory_str = \"09-19_23-25_FEDAVG\"\n",
    "h5_path = os.path.join(cv_results_path, directory_str, fedavg_filename)\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    a_group_key = list(f.keys())#[0]\n",
    "    print(a_group_key)\n",
    "\n",
    "    local_test_error_log = f['local_test_error_log'][()]\n",
    "    local_train_error_log = f['local_train_error_log'][()]\n",
    "    \n",
    "    client_local_model_log_keys = list(f['client_local_model_log'].keys())\n",
    "    #print(\"Keys in 'client_local_test_log':\", client_local_model_log_keys)\n",
    "    client_local_model_log_data = {}\n",
    "    for key in client_local_model_log_keys:\n",
    "        try:\n",
    "            client_local_model_log_data[key] = f['client_local_model_log'][key][:]\n",
    "        except KeyError:\n",
    "            pass\n",
    "            \n",
    "    client_local_test_log_keys = list(f['client_local_test_log'].keys())\n",
    "    # Iterate over the keys and extract the corresponding data\n",
    "    client_local_test_log_data = {}\n",
    "    for key in client_local_test_log_keys:\n",
    "        client_local_test_log_data[key] = f['client_local_test_log'][key][:]\n",
    "\n",
    "    client_gradnorm_log_keys = list(f['gradient_norm_lists_by_client'].keys())\n",
    "    # Iterate over the keys and extract the corresponding data\n",
    "    client_gradnorm_log_data = {}\n",
    "    for key in client_gradnorm_log_keys:\n",
    "        client_gradnorm_log_data[key] = f['gradient_norm_lists_by_client'][key][:]\n",
    "\n",
    "    if \"FEDAVG\" in directory_str:\n",
    "        global_dec_log = f['global_dec_log'][()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56f8f5bf-918e-45cc-a7bc-c97de12271f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S0_client_local_model_log',\n",
       " 'S10_client_local_model_log',\n",
       " 'S11_client_local_model_log',\n",
       " 'S12_client_local_model_log',\n",
       " 'S13_client_local_model_log',\n",
       " 'S1_client_local_model_log',\n",
       " 'S2_client_local_model_log',\n",
       " 'S3_client_local_model_log',\n",
       " 'S4_client_local_model_log',\n",
       " 'S5_client_local_model_log',\n",
       " 'S6_client_local_model_log',\n",
       " 'S7_client_local_model_log',\n",
       " 'S8_client_local_model_log',\n",
       " 'S9_client_local_model_log']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_local_model_log_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef80738a-3542-4ff8-b977-cca829207177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 64)\n",
      "(105, 2, 64)\n",
      "(105, 2, 64)\n",
      "(114, 2, 64)\n",
      "(104, 2, 64)\n",
      "(1, 2, 64)\n",
      "(106, 2, 64)\n",
      "(103, 2, 64)\n",
      "(104, 2, 64)\n",
      "(106, 2, 64)\n",
      "(101, 2, 64)\n",
      "(104, 2, 64)\n",
      "(105, 2, 64)\n",
      "(105, 2, 64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_CLIENTS):\n",
    "    print(client_local_model_log_data[client_local_model_log_keys[i]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcded4bb-1ca0-4142-b692-f2f2e7b81d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 2, 64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_dec_log.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c6a78-c098-4024-993a-827f1bd013f8",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8514079-83c9-4608-9bbe-017da5954073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fcf9ee2-c870-4885-8173-d4fcc1a9429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "path = r'C:\\Users\\kdmen\\Desktop\\Research\\Data\\CPHS_EMG'\n",
    "model_saving_dir = r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\PythonVersion\\PythonSimsRevamp\\models\"\n",
    "cond0_filename = r'\\cond0_dict_list.p'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "with open(path+cond0_filename, 'rb') as fp:\n",
    "    cond0_training_and_labels_lst = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0c1ff2-8cbc-493b-a849-957d29ad6b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 1202 2404 3606 4808 6010 7212]\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 14\n",
    "update_ix = [0,  1200,  2402,  3604,  4806,  6008,  7210,  8412,  9614, 10816, 12018, 13220, 14422, 15624, 16826, 18028, 19230, 20432, 20769]\n",
    "STARTING_UPDATE = 10\n",
    "starting_update_ix = update_ix[STARTING_UPDATE]\n",
    "final_usable_update = 17\n",
    "final_usable_update_ix = update_ix[final_usable_update]\n",
    "adj_update_ix = np.array(update_ix[STARTING_UPDATE:final_usable_update]) - starting_update_ix\n",
    "print(adj_update_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3d0a541-1ced-4d55-b04d-6c2747a4a9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_data = dict()\n",
    "client_labels = dict()\n",
    "for i in range(NUM_CLIENTS):\n",
    "    client_data[i] = cond0_training_and_labels_lst[i]['training'][starting_update_ix:final_usable_update_ix, :]\n",
    "    client_labels[i] = cond0_training_and_labels_lst[i]['labels'][starting_update_ix:final_usable_update_ix, :]\n",
    "\n",
    "upper_bound = self.local_dataset.shape[0]\n",
    "lower_bound = 0\n",
    "\n",
    "def simulate_data_stream(self):\n",
    "        need_to_advance=True\n",
    "        if (self.current_update>=self.final_usable_update_ix-1) or (self.train_update_ix is not None and self.current_train_update>=(len(self.train_update_ix)-1)):\n",
    "            self.logger = \"UNNAMED\"\n",
    "            lower_bound = self.update_ix[16] - self.update_ix[self.starting_update]\n",
    "            upper_bound = self.update_ix[17] - self.update_ix[self.starting_update]\n",
    "            need_to_advance=False\n",
    "        elif streaming_method=='streaming':\n",
    "            self.logger = \"streaming\"\n",
    "            if self.scenario==\"CROSS\":\n",
    "                # If we pass threshold, move on to the next update\n",
    "                if self.current_round>2 and self.current_round%self.local_round_threshold==0:\n",
    "                    self.current_update += 1\n",
    "                    self.update_transition_log.append(self.latest_global_round)\n",
    "                    need_to_advance = True\n",
    "                else:\n",
    "                    need_to_advance = False\n",
    "\n",
    "        if need_to_advance:\n",
    "            if self.scenario==\"CROSS\":\n",
    "                lower_bound = (self.update_ix[self.current_update]) - self.update_ix[self.starting_update]\n",
    "                upper_bound = self.update_ix[self.current_update+1] - self.update_ix[self.starting_update]\n",
    "\n",
    "                if \"PFA\" in self.global_method:\n",
    "                    mid_point = (lower_bound+upper_bound)//2\n",
    "                    s_temp2 = copy.deepcopy(self.training_data[mid_point:upper_bound,:])\n",
    "                    self.p_reference2 = copy.deepcopy(np.transpose(self.training_labels[mid_point:upper_bound,:]))\n",
    "                s_temp = copy.deepcopy(self.training_data[lower_bound:upper_bound,:])\n",
    "                self.p_reference = copy.deepcopy(np.transpose(self.training_labels[lower_bound:upper_bound,:]))\n",
    "                \n",
    "            # First, normalize the entire s matrix\n",
    "            if self.normalize_EMG:\n",
    "                s_normed = s_temp/np.amax(s_temp)\n",
    "            else:\n",
    "                s_normed = s_temp\n",
    "            # Now do PCA unless it is set to 64 (AKA the default num channels i.e. no reduction)\n",
    "            # Also probably ought to find a global transform if possible so I don't recompute it every time...\n",
    "            if self.PCA_comps!=self.pca_channel_default:  \n",
    "                pca = PCA(n_components=self.PCA_comps)\n",
    "                s_normed = pca.fit_transform(s_normed)\n",
    "            self.s = np.transpose(s_normed)\n",
    "            #print(f\"Client{self.ID} TRAINING DATA UPDATE: Mean, var, and norm: {np.mean(self.s), np.var(self.s), np.linalg.norm(self.s)}\")\n",
    "            self.F = self.s[:,:-1] # note: truncate F for estimate_decoder\n",
    "            v_actual = self.w@self.s\n",
    "            p_actual = np.cumsum(v_actual, axis=1)*self.dt  # Numerical integration of v_actual to get p_actual\n",
    "            if \"PFA\" in self.global_method:\n",
    "                if self.normalize_EMG:\n",
    "                    s_normed2 = s_temp2/np.amax(s_temp2)\n",
    "                else:\n",
    "                    s_normed2 = s_temp2\n",
    "                if self.PCA_comps!=self.pca_channel_default:  \n",
    "                    pca = PCA(n_components=self.PCA_comps)\n",
    "                    s_normed2 = pca.fit_transform(s_normed2)\n",
    "                self.s2 = np.transpose(s_normed2)\n",
    "                self.F2 = self.s2[:,:-1] # note: truncate F for estimate_decoder\n",
    "                v_actual2 = self.w@self.s2\n",
    "                p_actual2 = np.cumsum(v_actual2, axis=1)*self.dt  # Numerical integration of v_actual to get p_actual\n",
    "\n",
    "\n",
    "    \n",
    "if PFA:\n",
    "    mid_point = (lower_bound+upper_bound)//2\n",
    "\n",
    "    s_temp2 = client_data[i][mid_point:upper_bound, :]\n",
    "    #p_ref_lim2 = self.local_labelset[mid_point:upper_bound, :]\n",
    "    #p_reference2 = np.transpose(p_ref_lim2)\n",
    "    s2 = np.transpose(s_temp2/np.amax(s_temp2))\n",
    "    F2 = s2[:,:-1] # note: truncate F for estimate_decoder\n",
    "else:\n",
    "    mid_point = upper_bound\n",
    "\n",
    "s_temp = client_data[i][lower_bound:mid_point, :]\n",
    "#p_ref_lim = self.local_labelset[lower_bound:mid_point, :]\n",
    "#p_reference = np.transpose(p_ref_lim)\n",
    "s = np.transpose(s_temp/np.amax(s_temp))\n",
    "F = s[:,:-1] # note: truncate F for estimate_decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b484da-9eb0-45b0-84df-519c444a1a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7094b-1ac7-4543-9f8b-036b5b77384f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ebacf-abc7-49d1-a109-5455a0126501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Dimensionality reduction of training data using PCA\n",
    "# Reduce X_train to 2 dimensions to match model weights shape\n",
    "pca = PCA(n_components=2)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# Step 2: Compute mutual information between reduced training data and local model weights\n",
    "mi_local = []\n",
    "for i in range(client_local_model_weights.shape[1]):\n",
    "    mi_local.append(mutual_info_regression(X_train_reduced, client_local_model_weights[:, i]))\n",
    "\n",
    "# Step 3: Compute mutual information between reduced training data and global model weights\n",
    "mi_global = []\n",
    "for i in range(global_model_weights.shape[1]):\n",
    "    mi_global.append(mutual_info_regression(X_train_reduced, global_model_weights[:, i]))\n",
    "\n",
    "# Output mutual information scores\n",
    "print(\"Mutual Information between X_train and local model weights:\", mi_local)\n",
    "print(\"Mutual Information between X_train and global model weights:\", mi_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0dedb4-4d99-43a6-890b-00cb7fa55780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy training data (64 examples, 1202 features)\n",
    "X_train = np.random.rand(64, 1202)\n",
    "\n",
    "# Dummy local model weights (64 examples, 2 features)\n",
    "client_local_model_weights = np.random.rand(64, 2)\n",
    "\n",
    "# Dummy global model weights (64 examples, 2 features)\n",
    "global_model_weights = np.random.rand(64, 2)\n",
    "\n",
    "# Step 1: Compute summary statistics for each example in X_train\n",
    "# Let's use mean and variance as our summary statistics\n",
    "X_train_summary = np.vstack([np.mean(X_train, axis=1), np.var(X_train, axis=1)]).T  # Shape: (64, 2)\n",
    "\n",
    "# Step 2: Compute mutual information between summary statistics and local model weights\n",
    "mi_local = []\n",
    "for i in range(client_local_model_weights.shape[1]):\n",
    "    mi_local.append(mutual_info_regression(X_train_summary, client_local_model_weights[:, i]))\n",
    "\n",
    "# Step 3: Compute mutual information between summary statistics and global model weights\n",
    "mi_global = []\n",
    "for i in range(global_model_weights.shape[1]):\n",
    "    mi_global.append(mutual_info_regression(X_train_summary, global_model_weights[:, i]))\n",
    "\n",
    "# Output mutual information scores\n",
    "print(\"Mutual Information (summary stats) between X_train and local model weights:\", mi_local)\n",
    "print(\"Mutual Information (summary stats) between X_train and global model weights:\", mi_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebb69b-8ddc-49d8-97f0-7a1146034da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute entropy using KDE\n",
    "def kde_entropy(X, bandwidth=1.0):\n",
    "    kde = KernelDensity(bandwidth=bandwidth)\n",
    "    kde.fit(X)\n",
    "    log_density = kde.score_samples(X)\n",
    "    return -np.mean(log_density)\n",
    "\n",
    "# Helper function to compute mutual information using KDE\n",
    "def kde_mutual_information(X, Y, bandwidth=1.0):\n",
    "    # Joint entropy H(X,Y)\n",
    "    XY = np.hstack([X, Y])\n",
    "    joint_entropy = kde_entropy(XY, bandwidth)\n",
    "    \n",
    "    # Marginal entropies H(X) and H(Y)\n",
    "    entropy_X = kde_entropy(X, bandwidth)\n",
    "    entropy_Y = kde_entropy(Y, bandwidth)\n",
    "    \n",
    "    # MI = H(X) + H(Y) - H(X,Y)\n",
    "    mutual_info = entropy_X + entropy_Y - joint_entropy\n",
    "    return mutual_info\n",
    "\n",
    "# Dummy training data (64 examples, 1202 features)\n",
    "X_train = np.random.rand(64, 1202)\n",
    "\n",
    "# Dummy local model weights (64 examples, 2 features)\n",
    "client_local_model_weights = np.random.rand(64, 2)\n",
    "\n",
    "# Dummy global model weights (64 examples, 2 features)\n",
    "global_model_weights = np.random.rand(64, 2)\n",
    "\n",
    "# Step 1: Reduce the training data dimensionality using PCA to match model weights (optional, depending on your scenario)\n",
    "# Alternatively, you can use summary statistics, but let's use PCA here for KDE method\n",
    "pca = PCA(n_components=2)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# Step 2: Compute KDE-based mutual information between reduced training data and local model weights\n",
    "mi_local_kde = kde_mutual_information(X_train_reduced, client_local_model_weights)\n",
    "\n",
    "# Step 3: Compute KDE-based mutual information between reduced training data and global model weights\n",
    "mi_global_kde = kde_mutual_information(X_train_reduced, global_model_weights)\n",
    "\n",
    "# Output mutual information scores\n",
    "print(\"KDE-based Mutual Information between X_train and local model weights:\", mi_local_kde)\n",
    "print(\"KDE-based Mutual Information between X_train and global model weights:\", mi_global_kde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2f56e-d181-4bb9-a485-2e5c0e2e9131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce341bb6-e799-4763-82fb-d651d203e115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0734616-314c-4731-be30-e0fa61a2b13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de3260-3034-414e-86e0-2442da04b5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907a136-a301-4d6e-b5a4-4b9ec536c5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef4b2c-450a-4506-a007-acd1b73a2cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc3aca-f616-4cdc-8b23-b7995e551729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708fe28-3c9a-47c6-b4b5-c15e64dc40d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3e433-005d-402e-9e77-b5eb4a3ff979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba679ad2-5110-4dbf-b582-14600af1b93b",
   "metadata": {},
   "source": [
    "## Membership Inference Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384b54b5-1032-4eee-b91b-ba208a86f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9693d93a-b6a9-4cf5-9366-b56e5e646349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (0, 6),\n",
       " (0, 7),\n",
       " (0, 8),\n",
       " (0, 9),\n",
       " (0, 10),\n",
       " (0, 11),\n",
       " (0, 12),\n",
       " (0, 13),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (1, 5),\n",
       " (1, 6),\n",
       " (1, 7),\n",
       " (1, 8),\n",
       " (1, 9),\n",
       " (1, 10),\n",
       " (1, 11),\n",
       " (1, 12),\n",
       " (1, 13),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (2, 5),\n",
       " (2, 6),\n",
       " (2, 7),\n",
       " (2, 8),\n",
       " (2, 9),\n",
       " (2, 10),\n",
       " (2, 11),\n",
       " (2, 12),\n",
       " (2, 13),\n",
       " (3, 4),\n",
       " (3, 5),\n",
       " (3, 6),\n",
       " (3, 7),\n",
       " (3, 8),\n",
       " (3, 9),\n",
       " (3, 10),\n",
       " (3, 11),\n",
       " (3, 12),\n",
       " (3, 13),\n",
       " (4, 5),\n",
       " (4, 6),\n",
       " (4, 7),\n",
       " (4, 8),\n",
       " (4, 9),\n",
       " (4, 10),\n",
       " (4, 11),\n",
       " (4, 12),\n",
       " (4, 13),\n",
       " (5, 6),\n",
       " (5, 7),\n",
       " (5, 8),\n",
       " (5, 9),\n",
       " (5, 10),\n",
       " (5, 11),\n",
       " (5, 12),\n",
       " (5, 13),\n",
       " (6, 7),\n",
       " (6, 8),\n",
       " (6, 9),\n",
       " (6, 10),\n",
       " (6, 11),\n",
       " (6, 12),\n",
       " (6, 13),\n",
       " (7, 8),\n",
       " (7, 9),\n",
       " (7, 10),\n",
       " (7, 11),\n",
       " (7, 12),\n",
       " (7, 13),\n",
       " (8, 9),\n",
       " (8, 10),\n",
       " (8, 11),\n",
       " (8, 12),\n",
       " (8, 13),\n",
       " (9, 10),\n",
       " (9, 11),\n",
       " (9, 12),\n",
       " (9, 13),\n",
       " (10, 11),\n",
       " (10, 12),\n",
       " (10, 13),\n",
       " (11, 12),\n",
       " (11, 13),\n",
       " (12, 13)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of users (clients)\n",
    "NUM_USERS = 14\n",
    "\n",
    "# Create combinations of users to hold out in each fold (2 users at a time)\n",
    "user_pairs = list(combinations(range(NUM_USERS), 2))\n",
    "\n",
    "user_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad4ec7-948b-4285-a5d9-1696527d4283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE WOULD REPLACE CROSS USER MAIN TO GENERATE ALL THE MEMBERSHIP INFERENCE MODELS\n",
    "\n",
    "if False:\n",
    "    # Initialize storage for results\n",
    "    membership_inference_results = {}\n",
    "    \n",
    "    # Loop over each user pair combination to leave them out\n",
    "    for pair_idx, (user1, user2) in enumerate(user_pairs):\n",
    "        print(f\"Training models leaving out users {user1} and {user2}\")\n",
    "        \n",
    "        # Get the list of users excluding the pair\n",
    "        remaining_users = [user for user in range(NUM_USERS) if user != user1 and user != user2]\n",
    "        \n",
    "        # Train a model on the remaining users\n",
    "        train_clients = [Client(i, copy.deepcopy(D_0), OPT_METHOD, cond0_training_and_labels_lst[i], DATA_STREAM,\n",
    "                                beta=BETA, scenario=SCENARIO, local_round_threshold=LOCAL_ROUND_THRESHOLD, lr=LR, \n",
    "                                num_kfolds=len(user_pairs), global_method=GLOBAL_METHOD, max_iter=MAX_ITER, num_steps=NUM_STEPS) \n",
    "                                for i in remaining_users]\n",
    "        \n",
    "        # Initialize clients for testing (the two held-out users)\n",
    "        test_clients = [Client(i, copy.deepcopy(D_0), OPT_METHOD, cond0_training_and_labels_lst[i], DATA_STREAM,\n",
    "                               beta=BETA, scenario=SCENARIO, local_round_threshold=LOCAL_ROUND_THRESHOLD, lr=LR, \n",
    "                               availability=False, val_set=True, num_kfolds=len(user_pairs), global_method=GLOBAL_METHOD, max_iter=MAX_ITER, num_steps=NUM_STEPS) \n",
    "                               for i in [user1, user2]]\n",
    "    \n",
    "        # Store testing datasets from held-out clients\n",
    "        testing_datasets_lst = [test_cli.get_testing_dataset() for test_cli in test_clients]\n",
    "        \n",
    "        # Pass the testing datasets to the train clients\n",
    "        for train_cli in train_clients:\n",
    "            train_cli.set_testset(testing_datasets_lst)\n",
    "        \n",
    "        # Combine all clients (train + test) for federated learning\n",
    "        full_client_lst = train_clients + test_clients\n",
    "    \n",
    "        # Initialize server\n",
    "        server_obj = Server(1, copy.deepcopy(D_0), opt_method=OPT_METHOD, global_method=GLOBAL_METHOD, all_clients=full_client_lst)\n",
    "        server_obj.global_rounds = GLOBAL_ROUNDS\n",
    "    \n",
    "        # Run federated learning rounds\n",
    "        for i in range(GLOBAL_ROUNDS):\n",
    "            server_obj.execute_FL_loop()\n",
    "    \n",
    "        # Save results after the training\n",
    "        server_obj.save_results_h5(f\"model_without_users_{user1}_and_{user2}.h5\")\n",
    "        \n",
    "        # Perform membership inference on the held-out users\n",
    "        # Test the model on user1 and user2 data and compare with other models\n",
    "        for test_cli in test_clients:\n",
    "            client_id = test_cli.id\n",
    "            performance_metrics = server_obj.evaluate(test_cli.get_testing_dataset())\n",
    "            \n",
    "            # Store results for analysis\n",
    "            membership_inference_results[f\"fold_{pair_idx}_user_{client_id}\"] = performance_metrics\n",
    "    \n",
    "    # Output the inference results\n",
    "    print(\"Membership Inference Attack Results:\", membership_inference_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aacb37-ec01-45f9-b0c1-24b22b41c583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac935e-6420-49a8-8079-bbbd49437fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee203f-9c88-44c1-8648-e698b749dad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603da46b-0192-402c-b10e-34e9e9dfc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate membership inference attack\n",
    "def membership_inference_attack(client_models, X_train, X_test, y_train, y_test):\n",
    "    shadow_model = train_shadow_model(client_models, X_train, y_train)\n",
    "    attack_model = train_attack_model(shadow_model, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Predict membership for some test data\n",
    "    y_pred = attack_model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Function to train shadow model\n",
    "def train_shadow_model(client_models, X_train, y_train):\n",
    "    # Simulate using the client models to mimic the behavior of the original model\n",
    "    shadow_model = ClientModel()\n",
    "    shadow_model.train(X_train, y_train)\n",
    "    return shadow_model\n",
    "\n",
    "# Function to train attack model (classifier predicting membership status)\n",
    "def train_attack_model(shadow_model, X_train, X_test, y_train, y_test):\n",
    "    attack_X_train, attack_X_test, attack_y_train, attack_y_test = create_attack_dataset(shadow_model, X_train, X_test)\n",
    "    attack_model = AttackModel()\n",
    "    attack_model.train(attack_X_train, attack_y_train)\n",
    "    return attack_model\n",
    "\n",
    "# Function to create attack dataset from shadow model outputs\n",
    "def create_attack_dataset(shadow_model, X_train, X_test):\n",
    "    train_confidences = shadow_model.predict_proba(X_train)\n",
    "    test_confidences = shadow_model.predict_proba(X_test)\n",
    "    \n",
    "    attack_X = np.concatenate([train_confidences, test_confidences])\n",
    "    attack_y = np.concatenate([np.ones(len(train_confidences)), np.zeros(len(test_confidences))])\n",
    "    \n",
    "    return train_test_split(attack_X, attack_y, test_size=0.5)\n",
    "\n",
    "# Example usage\n",
    "client_models = [ClientModel() for _ in range(14)]  # Placeholder for client models\n",
    "\n",
    "# Assuming you have X_train, y_train, X_test, y_test already loaded\n",
    "membership_inference_acc = membership_inference_attack(client_models, X_train, X_test, y_train, y_test)\n",
    "print(f\"Membership Inference Attack Accuracy: {membership_inference_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11e2c1-1717-44b0-ae12-2173a8cc806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client_id in range(14):\n",
    "    for model_id in range(100):\n",
    "        model = client_models[client_id][model_id]\n",
    "        \n",
    "        # Membership Inference\n",
    "        membership_inference_acc = membership_inference_attack(client_models, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Model Inversion\n",
    "        target_output = model.predict(some_input)  # Select some target output\n",
    "        inverted_input = model_inversion_attack(model, target_output, input_shape)\n",
    "        \n",
    "        print(f\"Client {client_id}, Model {model_id}, Membership Inference Acc: {membership_inference_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5c298-2e6a-4f87-8b1e-6952a1df5183",
   "metadata": {},
   "source": [
    "## Model Inversion Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064b89b-1d6b-4b79-a875-260ddd5fd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    # Function to perform model inversion attack\n",
    "    def model_inversion_attack(model, target_output, input_shape, num_iterations=1000, learning_rate=1e-2):\n",
    "        # Initialize input (with random noise)\n",
    "        inverted_input = torch.randn(input_shape, requires_grad=True)\n",
    "        optimizer = optim.Adam([inverted_input], lr=learning_rate)\n",
    "        \n",
    "        loss_fn = nn.MSELoss()\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inverted_input)\n",
    "            loss = loss_fn(output, target_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return inverted_input.detach()\n",
    "    \n",
    "    # Example usage\n",
    "    client_models = [ClientModel() for _ in range(14)]  # Placeholder for client models\n",
    "    target_model = client_models[0]  # Assume you are attacking the first client\n",
    "    target_output = torch.tensor([0.1, 0.9])  # Target output we want to reproduce\n",
    "    \n",
    "    # Perform inversion attack to get reconstructed input\n",
    "    inverted_input = model_inversion_attack(target_model, target_output, input_shape=(1, 28, 28))  # Example input shape (e.g. for images)\n",
    "    print(\"Reconstructed input:\", inverted_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00706f56-40c8-4464-ba18-3677844f1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Model Inversion Attack on a Linear Regression Model\n",
    "def model_inversion_attack(weights, target_output, input_shape, num_iterations=1000, learning_rate=0.01):\n",
    "    # Initialize a random input (e.g., random noise)\n",
    "    inverted_input = np.random.randn(*input_shape)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Compute the output from the current inverted input\n",
    "        output = np.dot(inverted_input, weights)\n",
    "        \n",
    "        # Compute the loss (Mean Squared Error between target and current output)\n",
    "        loss = np.mean((output - target_output) ** 2)\n",
    "        \n",
    "        # Compute gradient of the loss with respect to the input\n",
    "        grad = 2 * np.dot((output - target_output), weights.T) / len(target_output)\n",
    "        \n",
    "        # Update the inverted input by moving in the direction of the negative gradient\n",
    "        inverted_input -= learning_rate * grad\n",
    "        \n",
    "        if i % 100 == 0:  # Print loss every 100 iterations\n",
    "            print(f\"Iteration {i}, Loss: {loss}\")\n",
    "    \n",
    "    return inverted_input\n",
    "\n",
    "# Example usage:\n",
    "np.random.seed(42)\n",
    "\n",
    "# Assume a simple linear regression model (2 features, 1 target)\n",
    "weights = np.array([[2.0, -1.0]]).T  # A 2x1 weight matrix\n",
    "\n",
    "# Target output we want to reproduce\n",
    "target_output = np.array([0.5])\n",
    "\n",
    "# Perform the model inversion attack to find an input that reproduces the target output\n",
    "inverted_input = model_inversion_attack(weights, target_output, input_shape=(1, 2), num_iterations=1000, learning_rate=0.01)\n",
    "\n",
    "print(\"Reconstructed input:\", inverted_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18658d-480a-492e-bed9-89a61f365030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39996a74-f0c8-487e-8597-d3469545fd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf63e6-e122-4460-a2ef-8febf00e09e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c30ed9-c753-42a6-b226-6ff48db88b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1d788-66a5-4279-971f-9ef22e26840c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877ac05-455f-4250-874b-73706ea12e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc3702-d9e1-467a-800a-ad9b39f57dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd7077-f9be-468c-9c7e-7d1c8a107054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190ccb7-76a1-421e-8746-24c67e0a535c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17d429-a97e-4949-8a2b-5c9ca83c7c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
