{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89f9c39b-5e49-4097-b4ef-97fbd2832a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import h5py\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b1725c-7d47-4a1b-9b0d-0f3600660e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_num_kfolds = 7\n",
    "NUM_CLIENTS = 14\n",
    "local_round_threshold = 20\n",
    "num_usuable_updates = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eba46c-e6ba-43eb-807c-499ba06d5da5",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8514079-83c9-4608-9bbe-017da5954073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used right now. Probably also out of date\n",
    "\n",
    "# INTRA\n",
    "'09-19_22-56_NOFL'\n",
    "'09-19_23-04_FEDAVG' \n",
    "'09-19_23-05_PFAFO_GDLS'\n",
    "\n",
    "# CROSS\n",
    "'09-19_23-07_PFAFO_GDLS'\n",
    "'09-19_23-25_FEDAVG'\n",
    "'09-19_23-30_NOFL'\n",
    "\n",
    "# This is for the mutual info stuff really...\n",
    "\n",
    "cv_results_path = r\"C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\PythonVersion\\PythonSimsRevamp\\results\\FinalSims\"\n",
    "nofl_filename = \"FULLSCIPYMIN_NOFL_KFold0.h5\" \n",
    "fedavg_filename = \"GDLS_FEDAVG_KFold0.h5\" \n",
    "\n",
    "directory_str = \"09-19_23-25_FEDAVG\"\n",
    "h5_path = os.path.join(cv_results_path, directory_str, fedavg_filename)\n",
    "\n",
    "# This is also not used right now, I don't think?\n",
    "\n",
    "if False:\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        a_group_key = list(f.keys())#[0]\n",
    "        print(a_group_key)\n",
    "    \n",
    "        local_test_error_log = f['local_test_error_log'][()]\n",
    "        local_train_error_log = f['local_train_error_log'][()]\n",
    "        \n",
    "        client_local_model_log_keys = list(f['client_local_model_log'].keys())\n",
    "        #print(\"Keys in 'client_local_test_log':\", client_local_model_log_keys)\n",
    "        client_local_model_log_data = {}\n",
    "        for key in client_local_model_log_keys:\n",
    "            try:\n",
    "                client_local_model_log_data[key] = f['client_local_model_log'][key][:]\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "        client_local_test_log_keys = list(f['client_local_test_log'].keys())\n",
    "        # Iterate over the keys and extract the corresponding data\n",
    "        client_local_test_log_data = {}\n",
    "        for key in client_local_test_log_keys:\n",
    "            client_local_test_log_data[key] = f['client_local_test_log'][key][:]\n",
    "    \n",
    "        client_gradnorm_log_keys = list(f['gradient_norm_lists_by_client'].keys())\n",
    "        # Iterate over the keys and extract the corresponding data\n",
    "        client_gradnorm_log_data = {}\n",
    "        for key in client_gradnorm_log_keys:\n",
    "            client_gradnorm_log_data[key] = f['gradient_norm_lists_by_client'][key][:]\n",
    "    \n",
    "        if \"FEDAVG\" in directory_str:\n",
    "            global_dec_log = f['global_dec_log'][()]\n",
    "\n",
    "if False:\n",
    "    print(client_local_model_log_keys)\n",
    "    print()\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        print(client_local_model_log_data[client_local_model_log_keys[i]].shape)\n",
    "    print()\n",
    "    print(global_dec_log.shape)\n",
    "\n",
    "#from sklearn.neighbors import KernelDensity\n",
    "#from sklearn.metrics import mutual_info_score\n",
    "#import numpy as np\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "if False:\n",
    "    # Find the longest array based on shape[0]\n",
    "    max_length = max(client_local_model_log_data[key].shape[0] for key in client_local_model_log_keys)\n",
    "    # Generate the indices for equally spaced sampling from global_dec_log\n",
    "    indices = np.linspace(0, len(global_dec_log) - 1, max_length).astype(int)\n",
    "    # Sample global_dec_log to match the length of the longest client array\n",
    "    sampled_global_dec_log = global_dec_log[indices]\n",
    "\n",
    "    # So now I need to link each set of 20 consecutive models to each update. Eg 1-20-->F1, 21-40-->F2, ... 140+-->F7 (might not get to F7!)\n",
    "    ## So it should iterative through consecutive models\n",
    "    \n",
    "    # THIS PART DOESNT WORK: MI ISNT DEFINED / WORK WELL BETWEEN TRAINING DATA AND WEIGHTS, APPARENTLY...\n",
    "    ## Maybe I could just ignore that and flatten the model weights and it would be fine? Not sure...\n",
    "    cli_mi_local = {i: [] for i in range(NUM_CLIENTS)}\n",
    "    cli_mi_global = {i: [] for i in range(NUM_CLIENTS)}\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        cli_i_model_log = client_local_model_log_data[client_local_model_log_keys[i]]\n",
    "        if cli_i_model_log.shape[0]<2:\n",
    "            # Client didnt train (eg it was probably a testset client)\n",
    "            print(f\"Cli {i} skipped for being too short!\")\n",
    "        else:\n",
    "            for model_idx, model in enumerate(cli_i_model_log):\n",
    "                # Models and dataset may not be aligned\n",
    "                ## Eg some may be saved in order of 0, 10, 11, 12, 13, 1, ... whereas the other may be saved as 0, 1, 2, 3, ...\n",
    "    \n",
    "                if model_idx > local_round_threshold * num_usuable_updates:\n",
    "                    input_index = 7\n",
    "                else:\n",
    "                    input_index = model_idx // local_round_threshold\n",
    "                # Access the corresponding input for this chunk\n",
    "                X_train = client_F_dict[i][input_index]  # TODO: THIS DOES NOT SUPPORT 2 BATCH PFA RIGHT NOW!\n",
    "                #print(f\"Model idx: {model_idx}; Update (input) index: {input_index}'\")\n",
    "    \n",
    "                # COMPARE EACH LOCAL MODEL TO THE TRAINING DATA TO GENERATE A MUTUAL INFORMATION CURVE\n",
    "                # Step 1: Dimensionality reduction of training data using PCA\n",
    "                # Reduce X_train to 2 dimensions to match model weights shape\n",
    "                pca = PCA(n_components=2)\n",
    "                X_train_PCA = pca.fit_transform(X_train).flatten()\n",
    "                \n",
    "                # Step 2: Compute mutual information between reduced training data and local model weights\n",
    "                cli_mi_local[i].append(mutual_info_regression(X_train_PCA, model.flatten()))\n",
    "                \n",
    "                # COMPARE EACH GLOBAL MODEL (from the adjusted list) TO THE TRAINING DATA\n",
    "                #if model_idx%20==0:\n",
    "                cli_mi_global.append(mutual_info_regression(X_train_PCA, sampled_global_dec_log[model_idx].flatten()))\n",
    "\n",
    "if False:\n",
    "    # Helper function to compute entropy using KDE\n",
    "    def kde_entropy(X, bandwidth=1.0):\n",
    "        kde = KernelDensity(bandwidth=bandwidth)\n",
    "        kde.fit(X)\n",
    "        log_density = kde.score_samples(X)\n",
    "        return -np.mean(log_density)\n",
    "    \n",
    "    # Helper function to compute mutual information using KDE\n",
    "    def kde_mutual_information(X, Y, bandwidth=1.0):\n",
    "        # Joint entropy H(X,Y)\n",
    "        XY = np.hstack([X, Y])\n",
    "        joint_entropy = kde_entropy(XY, bandwidth)\n",
    "        \n",
    "        # Marginal entropies H(X) and H(Y)\n",
    "        entropy_X = kde_entropy(X, bandwidth)\n",
    "        entropy_Y = kde_entropy(Y, bandwidth)\n",
    "        \n",
    "        # MI = H(X) + H(Y) - H(X,Y)\n",
    "        mutual_info = entropy_X + entropy_Y - joint_entropy\n",
    "        return mutual_info\n",
    "    \n",
    "    # Dummy training data (64 examples, 1202 features)\n",
    "    X_train = np.random.rand(64, 1202)\n",
    "    \n",
    "    # Dummy local model weights (64 examples, 2 features)\n",
    "    client_local_model_weights = np.random.rand(64, 2)\n",
    "    \n",
    "    # Dummy global model weights (64 examples, 2 features)\n",
    "    global_model_weights = np.random.rand(64, 2)\n",
    "    \n",
    "    # Step 1: Reduce the training data dimensionality using PCA to match model weights (optional, depending on your scenario)\n",
    "    # Alternatively, you can use summary statistics, but let's use PCA here for KDE method\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_reduced = pca.fit_transform(X_train)\n",
    "    \n",
    "    # Step 2: Compute KDE-based mutual information between reduced training data and local model weights\n",
    "    mi_local_kde = kde_mutual_information(X_train_reduced, client_local_model_weights)\n",
    "    \n",
    "    # Step 3: Compute KDE-based mutual information between reduced training data and global model weights\n",
    "    mi_global_kde = kde_mutual_information(X_train_reduced, global_model_weights)\n",
    "    \n",
    "    # Output mutual information scores\n",
    "    print(\"KDE-based Mutual Information between X_train and local model weights:\", mi_local_kde)\n",
    "    print(\"KDE-based Mutual Information between X_train and global model weights:\", mi_global_kde)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5c298-2e6a-4f87-8b1e-6952a1df5183",
   "metadata": {},
   "source": [
    "## Model Inversion Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064b89b-1d6b-4b79-a875-260ddd5fd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    # Function to perform model inversion attack\n",
    "    def model_inversion_attack(model, target_output, input_shape, num_iterations=1000, learning_rate=1e-2):\n",
    "        # Initialize input (with random noise)\n",
    "        inverted_input = torch.randn(input_shape, requires_grad=True)\n",
    "        optimizer = optim.Adam([inverted_input], lr=learning_rate)\n",
    "        \n",
    "        loss_fn = nn.MSELoss()\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inverted_input)\n",
    "            loss = loss_fn(output, target_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return inverted_input.detach()\n",
    "    \n",
    "    # Example usage\n",
    "    client_models = [ClientModel() for _ in range(14)]  # Placeholder for client models\n",
    "    target_model = client_models[0]  # Assume you are attacking the first client\n",
    "    target_output = torch.tensor([0.1, 0.9])  # Target output we want to reproduce\n",
    "    \n",
    "    # Perform inversion attack to get reconstructed input\n",
    "    inverted_input = model_inversion_attack(target_model, target_output, input_shape=(1, 28, 28))  # Example input shape (e.g. for images)\n",
    "    print(\"Reconstructed input:\", inverted_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00706f56-40c8-4464-ba18-3677844f1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Model Inversion Attack on a Linear Regression Model\n",
    "def model_inversion_attack(weights, target_output, input_shape, num_iterations=1000, learning_rate=0.01):\n",
    "    # Initialize a random input (e.g., random noise)\n",
    "    inverted_input = np.random.randn(*input_shape)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Compute the output from the current inverted input\n",
    "        output = np.dot(inverted_input, weights)\n",
    "        \n",
    "        # Compute the loss (Mean Squared Error between target and current output)\n",
    "        loss = np.mean((output - target_output) ** 2)\n",
    "        \n",
    "        # Compute gradient of the loss with respect to the input\n",
    "        grad = 2 * np.dot((output - target_output), weights.T) / len(target_output)\n",
    "        \n",
    "        # Update the inverted input by moving in the direction of the negative gradient\n",
    "        inverted_input -= learning_rate * grad\n",
    "        \n",
    "        if i % 100 == 0:  # Print loss every 100 iterations\n",
    "            print(f\"Iteration {i}, Loss: {loss}\")\n",
    "    \n",
    "    return inverted_input\n",
    "\n",
    "# Example usage:\n",
    "np.random.seed(42)\n",
    "\n",
    "# Assume a simple linear regression model (2 features, 1 target)\n",
    "weights = np.array([[2.0, -1.0]]).T  # A 2x1 weight matrix\n",
    "\n",
    "# Target output we want to reproduce\n",
    "target_output = np.array([0.5])\n",
    "\n",
    "# Perform the model inversion attack to find an input that reproduces the target output\n",
    "inverted_input = model_inversion_attack(weights, target_output, input_shape=(1, 2), num_iterations=1000, learning_rate=0.01)\n",
    "\n",
    "print(\"Reconstructed input:\", inverted_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18658d-480a-492e-bed9-89a61f365030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39996a74-f0c8-487e-8597-d3469545fd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf63e6-e122-4460-a2ef-8febf00e09e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c30ed9-c753-42a6-b226-6ff48db88b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1d788-66a5-4279-971f-9ef22e26840c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877ac05-455f-4250-874b-73706ea12e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc3702-d9e1-467a-800a-ad9b39f57dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd7077-f9be-468c-9c7e-7d1c8a107054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190ccb7-76a1-421e-8746-24c67e0a535c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17d429-a97e-4949-8a2b-5c9ca83c7c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
