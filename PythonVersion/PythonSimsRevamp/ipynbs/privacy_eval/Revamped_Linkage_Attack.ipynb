{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17f193d",
   "metadata": {},
   "source": [
    "> __Purpose:__ \n",
    "\n",
    "- The x axis is really starting from update 10 (or 9?) in the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a61b3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import h5py\n",
    "import copy\n",
    "import statistics\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import re\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import *\n",
    "\n",
    "random.seed(a=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f2109c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_update = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb379c33-eece-4803-9e5d-c674321223e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVING = True\n",
    "\n",
    "#raise ValueError(\"MAKE SURE SAVING IS WHAT YOU WANT IT TO BE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2b58b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59f33a4f-ccfb-42df-a59f-8dbd6b20cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PLOTTED_CONDITIONS = 6\n",
    "NUM_PLOTTED_CONDITIONS_WITH_GLOBAL = 10\n",
    "NUM_CLIENTS = 14\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "results_path = r'C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\PythonVersion\\PythonSimsRevamp\\results'\n",
    "current_directory = r'\\FinalSimRes'\n",
    "base_path = results_path + current_directory\n",
    "\n",
    "'''\n",
    "# CROSS\n",
    "10-05_14-23_NOFL_Cross\n",
    "10-05_14-32_PFAFO_Cross5\n",
    "10-05_14-48_FEDAVG_Cross5\n",
    "10-05_15-08_FEDAVG_Cross1\n",
    "10-05_15-14_PFAFO_Cross1\n",
    "\n",
    "# INTRA\n",
    "10-05_13-44_NOFL_Intra\n",
    "10-05_13-46_FEDAVG_Intra5\n",
    "10-05_13-55_FEDAVG_Intra1\n",
    "10-05_14-10_PFAFO_Intra1\n",
    "10-05_14-13_PFAFO_Intra5\n",
    "'''\n",
    "\n",
    "# CROSS\n",
    "cpfa_model_dict = load_model_logs(base_path+r'\\10-05_14-32_PFAFO_Cross5', 'GDLS_PFAFO_KFold')\n",
    "cfa_model_dict = load_model_logs(base_path+r'\\10-05_14-48_FEDAVG_Cross5', 'GDLS_FEDAVG_KFold')\n",
    "cnofl_model_dict = load_model_logs(base_path+r'\\10-05_14-23_NOFL_Cross', 'FULLSCIPYMIN_NOFL_KFold')\n",
    "# INTRA\n",
    "ipfa_model_dict = load_model_logs(base_path+r'\\10-05_14-13_PFAFO_Intra5', 'GDLS_PFAFO_KFold')\n",
    "ifa_model_dict = load_model_logs(base_path+r'\\10-05_13-46_FEDAVG_Intra5', 'GDLS_FEDAVG_KFold')\n",
    "inofl_model_dict = load_model_logs(base_path+r'\\10-05_13-44_NOFL_Intra', 'FULLSCIPYMIN_NOFL_KFold')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850bb15",
   "metadata": {},
   "source": [
    "# Adversarial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d9d6712-6436-4246-a1d7-1cb744d22c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC() --> Was failling to converge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5955b098-cba4-4cdd-9b5a-7914b20890b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_linkage_attack(extractration_dict, num_kfolds=0, i_modulus=3, i_lt_or_eq='lt', \n",
    "                           my_models_list=[KNeighborsClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier()], \n",
    "                           make_df=True, min_n_samples=10, num_clients=14):\n",
    "\n",
    "    flattened_input_df, num_updates = create_linkage_attack_df(extractration_dict, num_clients=num_clients)\n",
    "\n",
    "    local_res_df = execute_local_linkage_attack(flattened_input_df, num_updates, num_kfolds=num_kfolds, i_modulus=i_modulus, i_lt_or_eq=i_lt_or_eq,\n",
    "                           my_models_list=my_models_list, make_df=make_df, min_n_samples=min_n_samples, num_clients=num_clients)\n",
    "\n",
    "    global_res_df = execute_global_linkage_attack(flattened_input_df, num_updates, i_modulus=i_modulus, i_lt_or_eq=i_lt_or_eq,\n",
    "                           my_models_list=my_models_list, make_df=make_df, min_n_samples=min_n_samples, num_clients=num_clients)\n",
    "\n",
    "    return local_res_df, global_res_df\n",
    "\n",
    "    \n",
    "def create_linkage_attack_df(extractration_dict, num_clients=14):\n",
    "\n",
    "    keys = extractration_dict.keys()\n",
    "    num_updates_lst = []\n",
    "    for key in keys:\n",
    "        if \"global\" in key:\n",
    "            continue\n",
    "        num_updates_lst.append(len(extractration_dict[key]))\n",
    "    mode_update = statistics.mode(num_updates_lst)\n",
    "    max_update = max(num_updates_lst)\n",
    "    # ^ NOW THAT GLOVAL IS INCLUDED, MAX UPDATE IS WAY OFF FOR LOCAL!\n",
    "    ## Could find the 2nd largest value, since all global runs will have the same length...\n",
    "    if max_update == mode_update:  # Eg this is a poor man's proxy for if this is the NOFL case\n",
    "        num_updates = mode_update\n",
    "    else:\n",
    "        num_updates = min(max_update, int(statistics.mean(num_updates_lst) + np.sqrt(statistics.stdev(num_updates_lst))))  # int() rounds down\n",
    "        # np.sqrt(stdev) bc sometimes stdev is massive (like 38), making num_updates just equal to the max...\n",
    "    print(f\"num updates = {num_updates}; max_update {max_update}; avg_num_updates {statistics.mean(num_updates_lst)}\")\n",
    "    \n",
    "    # Initialize a list of empty DataFrames for each user group\n",
    "    dec_flattened_list = [pd.DataFrame(columns=['Subject', 'Fold', 'Update Number', 'Flattened Dec']) for _ in range(num_updates)]\n",
    "    global_dec_flattened_list = [pd.DataFrame(columns=['Subject', 'Fold', 'Update Number', 'Flattened Dec']) for _ in range(max_update)]\n",
    "    \n",
    "    # Regular expression pattern to extract subject and fold\n",
    "    #pattern = r\"S(\\d+)_client_local_model_log_fold(\\d+)\"\n",
    "    #pattern = r\"(S\\d+)_client_local_model_log_fold(\\d+)\"\n",
    "    pattern = r\"((S\\d+)_client_local_model_log_fold(\\d+)|global_dec_log_fold(\\d+))\"\n",
    "    # Loop through keys and updates to populate the DataFrames\n",
    "    for key_idx, key in enumerate(keys):\n",
    "        key_len = len(extractration_dict[key])\n",
    "        match = re.search(pattern, key)  # Extract the subject and fold using regex\n",
    "        # Group0: Entire match (the entire local key? Not sure what happens if global is the match...)\n",
    "        # Group1: Local subject ID (str)\n",
    "        # Group3: Local fold\n",
    "        # Group4: Global fold\n",
    "        if match:\n",
    "            if match.group(2):\n",
    "                #print(f\"Group 1: {match.group(1)}\")\n",
    "                #print(f\"Group 2: {match.group(2)}\")\n",
    "                #print(f\"Group 3: {match.group(3)}\")\n",
    "                subject = match.group(1)  # e.g., 'S0', 'S1', 'S10'\n",
    "                fold = int(match.group(3))  # e.g., '0', '1', '2'\n",
    "                for update_number in range(num_updates): \n",
    "                    if update_number >= key_len:\n",
    "                        continue\n",
    "                    else:\n",
    "                        user_data = np.ravel(extractration_dict[key][update_number])\n",
    "                        dec_flattened_list[update_number].loc[len(dec_flattened_list[update_number])] = [subject, fold, update_number, user_data]\n",
    "            elif match.group(4):  # This means it's a 'global_dec_log_fold(\\d+)' key\n",
    "                #print(f\"Group 4: {match.group(4)}\")\n",
    "                fold = int(match.group(4))\n",
    "                for update_number in range(max_update): # Max_update will be the number of global rounds\n",
    "                    global_data = np.ravel(extractration_dict[key][update_number])\n",
    "                    for subj_ID_num in range(num_clients):\n",
    "                        global_dec_flattened_list[update_number].loc[len(global_dec_flattened_list[update_number])] = [\"S\"+str(subj_ID_num), fold, update_number, global_data]\n",
    "    \n",
    "    # Concat all the dfs into a single training input dataframe\n",
    "    dec_flattened = pd.concat(dec_flattened_list, ignore_index=True, axis=0)\n",
    "    flattened_input_df = dec_flattened.join(dec_flattened['Flattened Dec'].apply(pd.Series)).drop('Flattened Dec', axis=1)\n",
    "    return flattened_input_df, num_updates\n",
    "\n",
    "\n",
    "def execute_local_linkage_attack(flattened_input_df, num_updates, num_kfolds=0, i_modulus=3, i_lt_or_eq='eq', \n",
    "                           my_models_list=[KNeighborsClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier()], \n",
    "                           make_df=True, min_n_samples=10, num_clients=14):\n",
    "    my_metrics_columns=['Adversarial Model', 'Update Number', 'CV Acc', 'Test Acc']\n",
    "    full_test_res_df = pd.DataFrame(columns=my_metrics_columns)\n",
    "    \n",
    "    # Adjust stop index to prevent overflow\n",
    "    n_start_stop=(0, num_updates)\n",
    "    n_stop = n_start_stop[1] - 1\n",
    "    print(f\"num_updates {num_updates}; n_start_stop {n_start_stop}; new n_stop {n_stop}\")\n",
    "    \n",
    "    for i in range(n_start_stop[0], n_stop + 1):\n",
    "        #print(f\"Round i={i} of stop={n_start_stop[1]}\")\n",
    "    \n",
    "        if make_df:\n",
    "            #print(\"Making custom test set, NOT USING PASSED IN full_test_df\")\n",
    "    \n",
    "            if i_modulus==-1:\n",
    "                pass\n",
    "            else:\n",
    "                if i%i_modulus==0:\n",
    "                    print(f\"Round i={i} of stop={n_start_stop[1]}\")\n",
    "                    \n",
    "                    # Filter the dataframe to use data from updates <= i\n",
    "                    # TODO: DECIDE ON USING <, <=, OR BOTH!\n",
    "                    if i_lt_or_eq == 'lt':\n",
    "                        train_df = flattened_input_df[flattened_input_df['Update Number'] <= i]\n",
    "                    elif i_lt_or_eq == 'eq':\n",
    "                        train_df = flattened_input_df[flattened_input_df['Update Number'] == i]\n",
    "                    test_df = flattened_input_df[flattened_input_df['Update Number'] >= n_stop]  \n",
    "                    # Hold out the last update for testing\n",
    "                    ## Actually, I changed it to use ALL updates greater than the final train update (which is close to the mean...)\n",
    "                else:\n",
    "                    # SKIP THE ENTIRE LOOP\n",
    "                    #print(f\"{i} skipped for speed!\")\n",
    "                    continue\n",
    "        else:\n",
    "            raise ValueError('Need to set train_df somehow...: for now, only use make_df=True')\n",
    "        \n",
    "        X_test = test_df.drop(columns=['Subject', 'Fold', 'Update Number'])\n",
    "        y_test = test_df['Subject']\n",
    "    \n",
    "        # Explicitly use the 'Fold' column for cross-validation\n",
    "        for model in my_models_list:\n",
    "            if num_kfolds == 0:\n",
    "                #print(f\"Fold {fold_num}\")\n",
    "                train_fold = train_df#[train_df['Fold'] != fold_num]\n",
    "                #val_fold = train_df[train_df['Fold'] == fold_num]\n",
    "    \n",
    "                if train_fold.shape[0] < min_n_samples: \n",
    "                    print(f\"Not enough samples! Skipping this round\")\n",
    "                    continue\n",
    "                #elif val_fold.shape[0]==0:\n",
    "                #    # Clients are trained a different number of rounds, so the max may be much higher than the average\n",
    "                #    # Thus resulting in upper rounds not containing any instances of a specific fold (eg can't train the model the way it is expected here)\n",
    "                #    continue\n",
    "\n",
    "                X_train_fold = train_fold.drop(columns=['Subject', 'Fold', 'Update Number']).reset_index(drop=True)\n",
    "                y_train_fold = train_fold['Subject'].reset_index(drop=True)\n",
    "                #X_val_fold = val_fold.drop(columns=['Subject', 'Fold', 'Update Number'])\n",
    "                #y_val_fold = val_fold['Subject']\n",
    "    \n",
    "                # Fit model on the current training fold\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                # Evaluate on the validation fold\n",
    "                #score = model.score(X_val_fold, y_val_fold)\n",
    "                cv_avg_score = None\n",
    "            else:\n",
    "                cv_scores = []\n",
    "                for fold_num in range(num_kfolds):\n",
    "                    #print(f\"Fold {fold_num}\")\n",
    "                    train_fold = train_df[train_df['Fold'] != fold_num]\n",
    "                    val_fold = train_df[train_df['Fold'] == fold_num]\n",
    "        \n",
    "                    if train_fold.shape[0] < min_n_samples:\n",
    "                        # if model is KNN and num_train_samples < model.n_neighbors\n",
    "                        # Adjust n_neighbors if necessary\n",
    "                        #print(f\"Adjusting n_neighbors to {num_train_samples} since it's smaller than n_neighbors.\")\n",
    "                        #model.set_params(n_neighbors=num_train_samples) \n",
    "                        \n",
    "                        print(f\"Not enough samples! Skipping this round\")\n",
    "                        continue\n",
    "                    elif val_fold.shape[0]==0:\n",
    "                        # Clients are trained a different number of rounds, so the max may be much higher than the average\n",
    "                        # Thus resulting in upper rounds not containing any instances of a specific fold (eg can't train the model the way it is expected here)\n",
    "                        continue\n",
    "    \n",
    "                    X_train_fold = train_fold.drop(columns=['Subject', 'Fold', 'Update Number']).reset_index(drop=True)\n",
    "                    y_train_fold = train_fold['Subject'].reset_index(drop=True)\n",
    "        \n",
    "                    X_val_fold = val_fold.drop(columns=['Subject', 'Fold', 'Update Number'])\n",
    "                    y_val_fold = val_fold['Subject']\n",
    "        \n",
    "                    # Fit model on the current training fold\n",
    "                    model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "                    # Evaluate on the validation fold\n",
    "                    score = model.score(X_val_fold, y_val_fold)\n",
    "                    cv_scores.append(score)\n",
    "        \n",
    "                # After evaluating all folds, calculate the average cross-validation score\n",
    "                cv_avg_score = sum(cv_scores) / len(cv_scores)\n",
    "    \n",
    "            # Calculate the test accuracy on the hold-out test set\n",
    "            test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "            # Save the results to the full_test_res_df DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                'Adversarial Model': [type(model).__name__],\n",
    "                'Update Number': [i],\n",
    "                'CV Acc': [cv_avg_score],\n",
    "                'Test Acc': [test_acc]\n",
    "            })\n",
    "            full_test_res_df = pd.concat([full_test_res_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return full_test_res_df\n",
    "\n",
    "def execute_global_linkage_attack(flattened_input_df, num_updates, i_modulus=3, i_lt_or_eq='eq',\n",
    "                           my_models_list=[KNeighborsClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier()], \n",
    "                           make_df=True, min_n_samples=5, num_clients=14):\n",
    "    my_metrics_columns=['Adversarial Model', 'Update Number', 'Test Acc']\n",
    "    full_test_res_df = pd.DataFrame(columns=my_metrics_columns)\n",
    "    \n",
    "    # Adjust stop index to prevent overflow\n",
    "    n_start_stop=(0, num_updates)\n",
    "    n_stop = n_start_stop[1] - 1\n",
    "    print(f\"num_updates {num_updates}; n_start_stop {n_start_stop}; new n_stop {n_stop}\")\n",
    "    \n",
    "    for i in range(n_start_stop[0], n_stop + 1):\n",
    "        #print(f\"Round i={i} of stop={n_start_stop[1]}\")\n",
    "    \n",
    "        if make_df:\n",
    "            #print(\"Making custom test set, NOT USING PASSED IN full_test_df\")\n",
    "    \n",
    "            if i_modulus==-1:\n",
    "                pass\n",
    "            else:\n",
    "                if i%i_modulus==0:\n",
    "                    print(f\"Round i={i} of stop={n_start_stop[1]}\")\n",
    "                    \n",
    "                    # Filter the dataframe to use data from updates <= i\n",
    "                    # TODO: DECIDE ON USING <, <=, OR BOTH!\n",
    "                    if i_lt_or_eq == 'lt':\n",
    "                        train_df = flattened_input_df[flattened_input_df['Update Number'] <= i]\n",
    "                    elif i_lt_or_eq == 'eq':\n",
    "                        train_df = flattened_input_df[flattened_input_df['Update Number'] == i]\n",
    "                    test_df = flattened_input_df[flattened_input_df['Update Number'] >= n_stop]  \n",
    "                    # Hold out the last update for testing\n",
    "                    ## Actually, I changed it to use ALL updates greater than the final train update (which is close to the mean...)\n",
    "                else:\n",
    "                    # SKIP THE ENTIRE LOOP\n",
    "                    #print(f\"{i} skipped for speed!\")\n",
    "                    continue\n",
    "        else:\n",
    "            raise ValueError('Need to set train_df somehow...: for now, only use make_df=True')\n",
    "        \n",
    "        X_test = test_df.drop(columns=['Subject', 'Fold', 'Update Number'])\n",
    "        y_test = test_df['Subject']\n",
    "\n",
    "        # Explicitly use the 'Fold' column for cross-validation\n",
    "        for model in my_models_list:\n",
    "            if train_df.shape[0] < min_n_samples:\n",
    "                print(f\"Not enough samples! Skipping this round\")\n",
    "                continue\n",
    "\n",
    "            X_train = train_df.drop(columns=['Subject', 'Fold', 'Update Number']).reset_index(drop=True)\n",
    "            y_train = train_df['Subject'].reset_index(drop=True)\n",
    "\n",
    "            # Fit model on the current training fold\n",
    "            model.fit(X_train, y_train)\n",
    "            # Calculate the test accuracy on the hold-out test set\n",
    "            test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "            # Save the results to the full_test_res_df DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                'Adversarial Model': [type(model).__name__],\n",
    "                'Update Number': [i],\n",
    "                'Test Acc': [test_acc]\n",
    "            })\n",
    "            full_test_res_df = pd.concat([full_test_res_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return full_test_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43255b2a-ea29-43ca-a8be-14b03af13227",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_nofl = 2\n",
    "i_fl = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "242a020a-7c19-47c6-825b-88ac9dd49fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num updates = 96; max_update 127; avg_num_updates 90.28571428571429\n",
      "num_updates 96; n_start_stop (0, 96); new n_stop 95\n",
      "Round i=0 of stop=96\n",
      "Round i=10 of stop=96\n",
      "Round i=20 of stop=96\n",
      "Round i=30 of stop=96\n",
      "Round i=40 of stop=96\n",
      "Round i=50 of stop=96\n",
      "Round i=60 of stop=96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SAVING:\n\u001b[1;32m----> 2\u001b[0m     cpfa_local_res_df, cpfa_global_res_df \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_linkage_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpfa_model_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_modulus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_fl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     cfa_local_res_df, cfa_global_res_df \u001b[38;5;241m=\u001b[39m execute_linkage_attack(cfa_model_dict, i_modulus\u001b[38;5;241m=\u001b[39mi_fl)\n\u001b[0;32m      4\u001b[0m     cnofl_local_res_df, cnofl_global_res_df \u001b[38;5;241m=\u001b[39m execute_linkage_attack(cnofl_model_dict, i_modulus\u001b[38;5;241m=\u001b[39mi_nofl)\n",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m, in \u001b[0;36mexecute_linkage_attack\u001b[1;34m(extractration_dict, num_kfolds, i_modulus, i_lt_or_eq, my_models_list, make_df, min_n_samples, num_clients)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_linkage_attack\u001b[39m(extractration_dict, num_kfolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, i_modulus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, i_lt_or_eq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      2\u001b[0m                            my_models_list\u001b[38;5;241m=\u001b[39m[KNeighborsClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier()], \n\u001b[0;32m      3\u001b[0m                            make_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m):\n\u001b[0;32m      5\u001b[0m     flattened_input_df, num_updates \u001b[38;5;241m=\u001b[39m create_linkage_attack_df(extractration_dict, num_clients\u001b[38;5;241m=\u001b[39mnum_clients)\n\u001b[1;32m----> 7\u001b[0m     local_res_df \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_local_linkage_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_input_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_kfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_modulus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_modulus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_lt_or_eq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_lt_or_eq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmy_models_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_models_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_n_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_n_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_clients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     global_res_df \u001b[38;5;241m=\u001b[39m execute_global_linkage_attack(flattened_input_df, num_updates, i_modulus\u001b[38;5;241m=\u001b[39mi_modulus, i_lt_or_eq\u001b[38;5;241m=\u001b[39mi_lt_or_eq,\n\u001b[0;32m     11\u001b[0m                            my_models_list\u001b[38;5;241m=\u001b[39mmy_models_list, make_df\u001b[38;5;241m=\u001b[39mmake_df, min_n_samples\u001b[38;5;241m=\u001b[39mmin_n_samples, num_clients\u001b[38;5;241m=\u001b[39mnum_clients)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_res_df, global_res_df\n",
      "Cell \u001b[1;32mIn[21], line 138\u001b[0m, in \u001b[0;36mexecute_local_linkage_attack\u001b[1;34m(flattened_input_df, num_updates, num_kfolds, i_modulus, i_lt_or_eq, my_models_list, make_df, min_n_samples, num_clients)\u001b[0m\n\u001b[0;32m    133\u001b[0m y_train_fold \u001b[38;5;241m=\u001b[39m train_fold[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m#X_val_fold = val_fold.drop(columns=['Subject', 'Fold', 'Update Number'])\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m#y_val_fold = val_fold['Subject']\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Fit model on the current training fold\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Evaluate on the validation fold\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m#score = model.score(X_val_fold, y_val_fold)\u001b[39;00m\n\u001b[0;32m    141\u001b[0m cv_avg_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    612\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    254\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    256\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 257\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    262\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    270\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if SAVING:\n",
    "    cpfa_local_res_df, cpfa_global_res_df = execute_linkage_attack(cpfa_model_dict, i_modulus=i_fl)\n",
    "    cfa_local_res_df, cfa_global_res_df = execute_linkage_attack(cfa_model_dict, i_modulus=i_fl)\n",
    "    cnofl_local_res_df, cnofl_global_res_df = execute_linkage_attack(cnofl_model_dict, i_modulus=i_nofl)\n",
    "\n",
    "    cpfa_local_res_df.to_pickle(\"cpfa_local_res_df.pkl\")\n",
    "    cpfa_global_res_df.to_pickle(\"cpfa_global_res_df.pkl\")\n",
    "    cfa_local_res_df.to_pickle(\"cfa_local_res_df.pkl\")\n",
    "    cfa_global_res_df.to_pickle(\"cfa_global_res_df.pkl\")\n",
    "    cnofl_local_res_df.to_pickle(\"cnofl_local_res_df.pkl\")\n",
    "    cnofl_global_res_df.to_pickle(\"cnofl_global_res_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a71a5-ee02-41f2-821b-f18cceb6c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVING:\n",
    "    ipfa_local_res_df, ipfa_global_res_df = execute_linkage_attack(ipfa_model_dict, i_modulus=i_fl)\n",
    "    ifa_local_res_df, ifa_global_res_df = execute_linkage_attack(ifa_model_dict, i_modulus=i_fl)\n",
    "    inofl_local_res_df, inofl_global_res_df = execute_linkage_attack(inofl_model_dict, i_modulus=i_nofl)\n",
    "    \n",
    "    ipfa_local_res_df.to_pickle(\"ipfa_local_res_df.pkl\")\n",
    "    ipfa_global_res_df.to_pickle(\"ipfa_global_res_df.pkl\")\n",
    "    ifa_local_res_df.to_pickle(\"ifa_local_res_df.pkl\")\n",
    "    ifa_global_res_df.to_pickle(\"ifa_global_res_df.pkl\")\n",
    "    inofl_local_res_df.to_pickle(\"inofl_local_res_df.pkl\")\n",
    "    inofl_global_res_df.to_pickle(\"inofl_global_res_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69649574-8c4d-40e5-b99f-47a306fa5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# List of local and global dataframes\n",
    "local_dfs = [ipfa_local_res_df, ifa_local_res_df, inofl_local_res_df, cpfa_local_res_df, cfa_local_res_df, cnofl_local_res_df]\n",
    "global_dfs = [ipfa_global_res_df, ifa_global_res_df, inofl_global_res_df, cpfa_global_res_df, cfa_global_res_df, cnofl_global_res_df]\n",
    "\n",
    "# Combine all dataframes and filter by 'KNeighborsClassifier'\n",
    "filtered_dfs = []\n",
    "for local_df, global_df in zip(local_dfs, global_dfs):\n",
    "    # Filter for KNeighborsClassifier in each dataframe\n",
    "    local_filtered = local_df[local_df['Adversarial Model'] == 'KNeighborsClassifier']\n",
    "    global_filtered = global_df[global_df['Adversarial Model'] == 'KNeighborsClassifier']\n",
    "    \n",
    "    # Append both filtered dataframes to a combined list\n",
    "    filtered_dfs.append(local_filtered)\n",
    "    filtered_dfs.append(global_filtered)\n",
    "\n",
    "# Concatenate all filtered dataframes\n",
    "combined_df = pd.concat(filtered_dfs)\n",
    "\n",
    "### 1. LINE PLOT\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Define unique colors for trials (6 trials, local and global)\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "line_styles = ['-', '--']  # Solid for local, dashed for global\n",
    "\n",
    "# Plot the local and global models with different line styles\n",
    "for idx, df in enumerate(local_dfs):\n",
    "    local_filtered = df[df['Adversarial Model'] == 'KNeighborsClassifier']\n",
    "    global_filtered = global_dfs[idx][global_dfs[idx]['Adversarial Model'] == 'KNeighborsClassifier']\n",
    "    \n",
    "    plt.plot(local_filtered['Update Number'], local_filtered['Test Acc'], color=colors[idx], linestyle=line_styles[0], label=f'Trial {idx+1} Local')\n",
    "    plt.plot(global_filtered['Update Number'], global_filtered['Test Acc'], color=colors[idx], linestyle=line_styles[1], label=f'Trial {idx+1} Global')\n",
    "\n",
    "plt.xlabel('Update Number')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('KNN Adversarial Model Accuracy Progression')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "### 2. HEATMAP\n",
    "\n",
    "# To create a heatmap, we need a pivoted dataframe where rows represent trials (local and global) and columns are Update Number\n",
    "\n",
    "# Add a column to represent the trial name\n",
    "for idx, (local_df, global_df) in enumerate(zip(local_dfs, global_dfs)):\n",
    "    local_dfs[idx]['Trial'] = f'Trial {idx+1} Local'\n",
    "    global_dfs[idx]['Trial'] = f'Trial {idx+1} Global'\n",
    "\n",
    "# Combine all dataframes again with the new 'Trial' column\n",
    "heatmap_df = pd.concat(local_dfs + global_dfs)\n",
    "\n",
    "# Filter for KNeighborsClassifier and pivot the dataframe\n",
    "heatmap_df = heatmap_df[heatmap_df['Adversarial Model'] == 'KNeighborsClassifier']\n",
    "heatmap_pivot = heatmap_df.pivot(index='Trial', columns='Update Number', values='Test Acc')\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_pivot, annot=True, cmap='coolwarm', cbar_kws={'label': 'Test Accuracy (%)'})\n",
    "plt.title('KNN Adversarial Model Accuracy Heatmap')\n",
    "plt.xlabel('Update Number')\n",
    "plt.ylabel('Trial')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37af10b-9e54-446a-a5ed-6993f2ba68e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9012b-8973-418c-94eb-ed07edf22c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
