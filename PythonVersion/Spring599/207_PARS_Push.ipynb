{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59151bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import copy\n",
    "\n",
    "from experiment_params import *\n",
    "from cost_funcs import *\n",
    "from fl_sim_classes import *\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\kdmen\\Desktop\\Research\\personalization-privacy-risk\\Data'\n",
    "cond0_filename = r'\\cond0_dict_list.p'\n",
    "all_decs_init_filename = r'\\all_decs_init.p'\n",
    "nofl_decs_filename = r'\\nofl_decs.p'\n",
    "id2color = {0:'lightcoral', 1:'maroon', 2:'chocolate', 3:'darkorange', 4:'gold', 5:'olive', 6:'olivedrab', \n",
    "            7:'lawngreen', 8:'aquamarine', 9:'deepskyblue', 10:'steelblue', 11:'violet', 12:'darkorchid', 13:'deeppink'}\n",
    "implemented_client_training_methods = ['EtaGradStep', 'EtaScipyMinStep', 'FullScipyMinStep']\n",
    "implement_these_methods_next = ['APFL', 'AFL', 'PersA_FL_MAML', 'PersA_FL_ME', 'PFA']\n",
    "num_participants = 14\n",
    "\n",
    "# For exclusion when plotting later on\n",
    "bad_nodes = [1,3,13]\n",
    "\n",
    "with open(path+cond0_filename, 'rb') as fp:\n",
    "    cond0_training_and_labels_lst = pickle.load(fp)\n",
    "\n",
    "D_0_7 = np.random.rand(2,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac5acc",
   "metadata": {},
   "source": [
    "# PARS-Push: Personalized, Robust Stochastic Gradient-Push\n",
    "- We consider a static, directed, and strongly connected network G = {[n], E} with no self-loops, and E ⊆ [n] × [n], where (i, j) ∈ E if there is an edge from node i to j. --> Eg this is not federated learning, appears to be fully distributed\n",
    "- The core of our method relies on the running sum technique developed in [20]–[22] which is equivalent to the Push-Sum algorithm [25] over a sequence of virtual time-varying augmented graphs. \n",
    "- PARS-Push has two key aspects: (i) robust asynchronous aggregation over a virtual, augmented graph, and (ii) stochastic gradient descent with respect to unbiased stochastic gradients of (4).\n",
    "- In-neighbors are the adjacent nodes that are directed IN to the target agent i, and out-neighbors are just the reverse (the nodes that the target agent points to).\n",
    "- How to adapt distributed layout to FL case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df50539",
   "metadata": {},
   "source": [
    "1. First, agent i select u+1 independent data batches curly_v wrt p_i, and after performing u steps of stochastic gradient descent starting from zi (Lines 6-11)\n",
    "\n",
    "1. Agent i computes an unbiased stochastic gradient of (4), and then updates xi in Line 12\n",
    "\n",
    "1. Second, node i updatesits parameters xi, yi, φx i , and φy i according to Line 14-15, \n",
    "\n",
    "1. Then sends the running sum parameters to its out-neighbors N_i^+ (Line 16)\n",
    "\n",
    "1. Finally, agent i processes the received messages from its in-neighbors N − i which leads to selecting the most recent updates (Lines 17-22)\n",
    "\n",
    "1. Consequently, agent i updates xi, yi, and zi in Lines 23-25 by combining newly received messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif self.global_method=='APFL': \n",
    "    t = self.current_global_round  # Should this be global or local? Global based on how they wrote it...\n",
    "    # F.T@F is not symmetric, so use eig not eigh\n",
    "    # eig returns (UNORDERED) eigvals, eigvecs\n",
    "    if self.use_real_hess:\n",
    "        # May be better to check to see if the update is in the update_transition_log...\n",
    "        # Should also just be able to use self.current_update%self.local_round_threshold or something\n",
    "        if self.prev_update == self.current_update:  # When is this condition ever true......\n",
    "            # Note that this changes if you want to do SGD instead of GD\n",
    "            eigvals = self.prev_eigvals\n",
    "        else:\n",
    "            print(f\"Client{self.ID}: Recalculating the Hessian for new update {self.current_update}!\")\n",
    "            eigvals, _ = np.linalg.eig(hessian_cost_l2(self.F, self.alphaD))\n",
    "            self.prev_eigvals = eigvals\n",
    "            # Coping mechanism\n",
    "            self.prev_update = self.current_update\n",
    "    mu = np.amin(eigvals)  # Mu is the minimum eigvalue\n",
    "    if mu.imag < self.tol and mu.real < self.tol:\n",
    "        raise ValueError(\"mu is ~0, thus implying func is not mu-SC\")\n",
    "    elif mu.imag < self.tol:\n",
    "        mu = mu.real\n",
    "    elif mu.real < self.tol:\n",
    "        print(\"Setting to imaginary only\")  # This is an issue if this runs\n",
    "        mu = mu.imag\n",
    "    L = np.amax(eigvals)  # L is the maximum eigvalue\n",
    "    if L.imag < self.tol and L.real < self.tol:\n",
    "        raise ValueError(\"L is 0, thus implying func is not L-smooth\")\n",
    "    elif mu.imag < self.tol:\n",
    "        L = L.real\n",
    "    elif L.real < self.tol:\n",
    "        print(\"Setting to imaginary only\")  # This is an issue if this runs\n",
    "        L = L.imag\n",
    "    if self.verbose: \n",
    "        # Find a better way to print this out without spamming the console... eg log file...\n",
    "        print(f\"Client{self.ID}: L: {L}, mu: {mu}\")\n",
    "    kappa = L/mu\n",
    "    a = np.max([128*kappa, self.tau])\n",
    "    eta_t = 16 / (mu*(t+a))\n",
    "    if self.input_eta:\n",
    "        if self.safe_lr_factor!=False:\n",
    "            raise ValueError(\"Cannot input eta AND use safe learning rate (they overwrite each other)\")\n",
    "        eta_t = self.eta\n",
    "    elif self.safe_lr_factor!=False:\n",
    "        print(\"Forcing eta_t to be based on the input safe lr factor\")\n",
    "        # This is only subtly different from just inputting eta... a little more dynamic ig\n",
    "        eta_t = 1/(self.safe_lr_factor*L)\n",
    "    elif eta_t >= 1/(2*L):\n",
    "        # Note that we only check when automatically setting\n",
    "        # ie if you manually input it will do whatever you tell it to do\n",
    "        raise ValueError(\"Learning rate is too large according to constaints on GD\")\n",
    "    if self.verbose:\n",
    "        print(f\"Client{self.ID}: eta_t: {eta_t}\")\n",
    "    self.p.append((t+a)**2)\n",
    "    if self.track_lr_comps:\n",
    "        self.L_log.append(L)\n",
    "        self.mu_log.append(mu)\n",
    "        self.eta_t_log.append(eta_t)\n",
    "\n",
    "    if self.adaptive:\n",
    "        self.adap_alpha.append(self.adap_alpha[-1] - eta_t*np.inner(np.reshape((self.w-self.global_w), (self.PCA_comps*2)), np.reshape(gradient_cost_l2(self.F, self.mixed_w, self.H, self.Vmixed, self.learning_batch, self.alphaF, self.alphaD, Ne=self.PCA_comps), (2*self.PCA_comps))))\n",
    "        # This is theoretically the same but I'm not sure what grad_alpha means\n",
    "        #self.sus_adap_alpha.append() ... didn't write yet\n",
    "\n",
    "    # GRADIENT DESCENT BASED MODEL UPDATE\n",
    "    # NOTE: eta_t IS DIFFERENT FROM CLIENT'S ETA (WHICH IS NOT USED)\n",
    "    # Why do I flatten the grads...\n",
    "\n",
    "    global_gradient = np.reshape(gradient_cost_l2(self.F, self.global_w, self.H, self.Vglobal, self.learning_batch, self.alphaF, self.alphaD, Ne=self.PCA_comps), (2, self.PCA_comps))\n",
    "    local_gradient = np.reshape(gradient_cost_l2(self.F, self.mixed_w, self.H, self.Vmixed, self.learning_batch, self.alphaF, self.alphaD, Ne=self.PCA_comps), (2, self.PCA_comps))\n",
    "    # Gradient clipping\n",
    "    if self.gradient_clipping:\n",
    "        if np.linalg.norm(global_gradient) > self.clipping_threshold:\n",
    "            global_gradient = self.clipping_threshold*global_gradient/np.linalg.norm(global_gradient)\n",
    "        if np.linalg.norm(local_gradient) > self.clipping_threshold:\n",
    "            local_gradient = self.clipping_threshold*local_gradient/np.linalg.norm(local_gradient)\n",
    "    if self.track_gradient:\n",
    "        self.gradient_log.append(np.linalg.norm(gradient_cost_l2(self.F, self.w, self.H, self.V, self.learning_batch, self.alphaF, self.alphaD, Ne=self.PCA_comps)))\n",
    "        self.pers_gradient_log.append(np.linalg.norm(local_gradient))\n",
    "        # ^ Local gradient is evaluated wrt mixed inputs (eg w and V)\n",
    "        self.global_gradient_log.append(np.linalg.norm(global_gradient))\n",
    "\n",
    "    ########################################\n",
    "    # Or should I normalize the dec here?  I'll also turn this on since idc about computational speed rn\n",
    "    if self.normalize_dec:\n",
    "        self.global_w /= np.amax(self.global_w)\n",
    "        self.w /= np.amax(self.w)\n",
    "        self.mixed_w /= np.amax(self.mixed_w)\n",
    "    ########################################\n",
    "\n",
    "    # PSEUDOCODE: my_client.global_w -= my_client.eta * grad(f_i(my_client.global_w; my_client.smallChi))\n",
    "    self.global_w -= eta_t * global_gradient\n",
    "    # PSEUDOCODE: my_client.local_w -= my_client.eta * grad_v(f_i(my_client.v_bar; my_client.smallChi))\n",
    "    self.w -= eta_t * local_gradient\n",
    "    self.mixed_w = self.adap_alpha[-1]*self.w - (1 - self.adap_alpha[-1])*self.global_w\n",
    "    ########################################\n",
    "    # Or should I normalize the dec here?  I'll also turn this on since idc about computational speed rn\n",
    "    if self.normalize_dec:\n",
    "        self.global_w /= np.amax(self.global_w)\n",
    "        self.w /= np.amax(self.w)\n",
    "        self.mixed_w /= np.amax(self.mixed_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2433c2",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Inputs\n",
    "#x_i  # exists in R^d; i is each node\n",
    "#theta_t  # exists in R for all t existing in Z0^+\n",
    "# Init\n",
    "y_i = 1\n",
    "z_i = x_i\n",
    "kappa_i = -1\n",
    "phi_i_x = 0\n",
    "phi_i_y = 0\n",
    "kappa_ij = -1  # For all (j, i) existing in Edges\n",
    "rho_ij_x = -1  # For all (j, i) existing in Edges\n",
    "rho_ij_y = -1  # For all (j, i) existing in Edges\n",
    "for t in range(1000):\n",
    "    for my_client in all_clients:\n",
    "        if my_client.available:\n",
    "            # Line 5\n",
    "            my_client.n = np.sum(theta)  # From my_client.kappa+1 to t\n",
    "            # Line 6-11\n",
    "            w_i = z_i\n",
    "            for r in range(u-1):\n",
    "                # Sample a batch D^t_{i, r} with size b from p_i\n",
    "                w_i^(r+1) = w_i^r - alpha*nabla_tilde(w_i^r, D^t_{i, r})\n",
    "            # Sample a batch D^t_{i, u} with size b from p_i\n",
    "            # Is that an introduction or just now shown ...\n",
    "            # Line 12\n",
    "            # Crazy eqn\n",
    "            x_i = x_i - n_i[...]\n",
    "            kappa_i = t\n",
    "            # Line 14-15\n",
    "            x_i = x_i/(d^+_i + 1)\n",
    "            y_i = y_i/(d^+_i + 1)\n",
    "            phi^x_i += x_i\n",
    "            phu^y_i += y_i\n",
    "            # Line 16\n",
    "            blah\n",
    "            # Lines 17-22\n",
    "            blah\n",
    "            for (phi^x_i, phi^y_i, kappa_i) in R_i:\n",
    "                if kappa_j > kappa_{ij}:\n",
    "                    blahx\n",
    "                    blahy\n",
    "            # Line 23\n",
    "            blahx\n",
    "            blahy\n",
    "            blahx\n",
    "            blahy\n",
    "#return z_i \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1be568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9a627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
