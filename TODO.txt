FL:
TO DO
1. Implement FL (FedAvg) for our dataset (probably will not perform well since it is homogenuous global model)
  - Use composition instead of inheritance? Optimal OOP approach
2. Figure out what to do about running things in parallel vs just sequentially
3. Need to fix decoders to match with trials...
4. Simulating / retro-active FL: just run all nodes on 1 CPU? Threading? Might not really matter since network communication is its own thing (not a real ML question for us)

CONSIDERATIONS
1. ADVERSARIAL ROBUSTNESS
 - Seems like we ought to add a way to track how much each client has been contributing, to see if one client is just spamming data, drastically moving the global model
 - Generally, can an adversary contribute and hurt the performance on everyone else (intuitively yes), but do we have a way to track this? Amount of change from current model?  Statistical variance? Make separate globel models for different clusters?
 - Likewise probably need to do something about eavesdropping --> eg HE/SMC/encryption

MORE MISC
1. It probably would be good to have a way to generalize/transfer for models when channels don't re-align between trials.  Since we are not doing calibration.  If it could know the user / similar decs, it could maybe "re-arrange" (or do something like PCA where there's no direct channel correspondence anymore) the channels so performance goes back to what it was before.
 - Look at adding PCA to linear regression framework and still getting something useful? Of course would not get a dec that's (2,64), but I don't see why would need anything bigger than (2,1).  The 64 channels must be getting combined somewhere somehow in the code
 2. In a similar vein: Multi-task learning: Would be interesting to see if we could train a model and have it work beyond a single EMG task... not sure if all EMG tasks are the same from the models perspective tho if it's just converting it to vel/pos anyways